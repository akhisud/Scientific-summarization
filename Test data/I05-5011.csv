This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Automatic paraphrase discovery important challenging task.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	first stage identifies keyword phrase joins phrases keyword sets.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	second stage links sets involve pairs individual NEs.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases grouped.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	One difficulties Natural Language Processing fact many ways express thing event.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	lot research lexical relations, along creation resources WordNet.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	order create IE system new domain, one spend long time create knowledge.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	2.1 Overview.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	explaining method detail, present brief overview subsection.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	First, large corpus, extract NE instance pairs.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	pair also record context, i.e. phrase two NEs (Step1).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	use simple TF/IDF method measure topicality words.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	shall see, linked sets paraphrases.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	overview illustrated Figure 1.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	sentences corpus tagged transformation-based chunker NE tagger.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Figure 2 shows examples extracted NE pair instances contexts.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	used TF/ITF metric identify keywords.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Figure 3 Figure 1.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Overview method 2.2 Step Step Algorithm.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	section, explain algorithm step step examples.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	size, examples (Figures 2 4) appear end paper.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	shows keywords scores.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	(If TF/IDF score word threshold, phrase discarded.)	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	gather phrases keyword.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Figure 4 shows phrase sets based keywords CC-domain.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Cluster phrases based Links set phrases share keyword.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	However, phrases express meanings even though share keyword.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	step, try link sets, put single cluster.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	clue NE instance pairs.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	pair NE instances used different phrases, phrases likely paraphrases.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Here, “EG” represents “Eastern Group Plc”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	“H” represents “Hanson Plc”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	So, set threshold least two examples required build link.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	examples shown Figure 5.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Notice CC-domain special case.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	links solve problem.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	figure 4, reverse relations indicated `*’ next frequency.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	sets phrases share keyword links sets.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	3.1 Corpora.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	sentences analyzed chunker NE tag- ger.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	3.2 Results.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	subsection, report results experiment, terms number words, phrases clusters.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	report evaluation results next subsection.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Step 1.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	frequency Company – Company domain ranks 11th 35,567 examples.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair keywords found NE category pair.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	natural larger data domain, keywords found.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	total, 2,000 NE category pairs, 5,184 keywords found.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	phrase contain keywords, phrase discarded.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	total, across domains, kept 13,976 phrases keywords.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	CC-domain, 32 sets phrases contain 2 phrases.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	concentrate sets.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Among 32 sets, found following pairs sets two links.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	set represented keyword number parentheses indicates number shared NE pair instances.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	describe evaluation clusters next subsection.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	3.3 Evaluation Results.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	evaluated results based two metrics.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	One accuracy within set phrases share keyword; accuracy links.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	easy make clear definition “paraphrase”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Although precise criterion, cases evaluated relatively clear-cut.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	set, phrases bracketed frequencies considered paraphrases set.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	accuracy calculated ratio number paraphrases total number phrases set.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	results, along total number phrases, shown Table 1.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	larger sets accurate small sets.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	make several observations cause errors.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	errors include NE tagging errors errors due phrase includes NEs.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Also cases one two NEs belong phrase outside relation.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	example, sentence “Mr.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	return issues discussion section.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	links “CC-domain shown Step 4 subsection 3.2.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	company buys another company, paying event occur, two phrases indicate event.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	similar explanation applies link “stake” set.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	checked whether discovered links listed WordNet.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	result suggests benefit using automatic discovery method.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Evaluation results links	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	work reported closely related [Ha- segawa et al. 04].	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	First, describe method compare method.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	first collect NE instance pairs contexts, like method.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	However, next step clearly different.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	cluster NE instance pairs based words contexts using bag- of-words method.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	threshold, NE instance pairs could used hence variety phrases also limited.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	number NE instance pairs used experiment less half method.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	kinds efforts discover paraphrase automatically corpora.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	availability comparable corpora limited, significant limitation approach.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	repeated several times collect list author / book title pairs expressions.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	However, methods need initial seeds, relation entities known advance.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	limitation obstacle making technology “open domain”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	However, desirable separate them.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	problem arises keywords consist one word.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	One possibility use n-grams based mutual information.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	frequent multi-word sequence domain, could use keyword candidate.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	explained results section, “strength” “add” desirable keywords CC-domain.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Also, “agree” CC-domain desirable keyword.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	relatively frequent word domain, used different extraction scenarios.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	checked similar verbs major domains, one.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Chunking enough find relationships.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	remains future work.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Limitations several limitations methods.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	phrases expressions length less 5 chunks, appear two NEs.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Also, method using keywords rules phrases don’t contain popular words domain.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	claiming method almighty.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Applications discovered paraphrases multiple applications.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	One obvious application information extraction.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	proposed unsupervised method discover paraphrases large untagged corpus.	1
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	accuracies link 73% 86% two evaluated domains.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	results promising several avenues improving results.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	paper necessarily reflect position U.S. Government.	0
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.  (Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	1
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Automatic paraphrase discovery important challenging task.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	first stage identifies keyword phrase joins phrases keyword sets.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	second stage links sets involve pairs individual NEs.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases grouped.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	One difficulties Natural Language Processing fact many ways express thing event.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	lot research lexical relations, along creation resources WordNet.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	order create IE system new domain, one spend long time create knowledge.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	2.1 Overview.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	explaining method detail, present brief overview subsection.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	First, large corpus, extract NE instance pairs.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	pair also record context, i.e. phrase two NEs (Step1).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	use simple TF/IDF method measure topicality words.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	shall see, linked sets paraphrases.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	overview illustrated Figure 1.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	sentences corpus tagged transformation-based chunker NE tagger.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Figure 2 shows examples extracted NE pair instances contexts.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	used TF/ITF metric identify keywords.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Figure 3 Figure 1.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Overview method 2.2 Step Step Algorithm.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	section, explain algorithm step step examples.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	size, examples (Figures 2 4) appear end paper.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	shows keywords scores.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	(If TF/IDF score word threshold, phrase discarded.)	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	gather phrases keyword.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Figure 4 shows phrase sets based keywords CC-domain.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Cluster phrases based Links set phrases share keyword.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	However, phrases express meanings even though share keyword.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	step, try link sets, put single cluster.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	clue NE instance pairs.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	pair NE instances used different phrases, phrases likely paraphrases.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Here, “EG” represents “Eastern Group Plc”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	“H” represents “Hanson Plc”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	So, set threshold least two examples required build link.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	examples shown Figure 5.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Notice CC-domain special case.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	links solve problem.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	figure 4, reverse relations indicated `*’ next frequency.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	sets phrases share keyword links sets.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	3.1 Corpora.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	sentences analyzed chunker NE tag- ger.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	3.2 Results.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	subsection, report results experiment, terms number words, phrases clusters.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	report evaluation results next subsection.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Step 1.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	frequency Company – Company domain ranks 11th 35,567 examples.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair keywords found NE category pair.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	natural larger data domain, keywords found.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	total, 2,000 NE category pairs, 5,184 keywords found.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	phrase contain keywords, phrase discarded.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	total, across domains, kept 13,976 phrases keywords.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	CC-domain, 32 sets phrases contain 2 phrases.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	concentrate sets.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Among 32 sets, found following pairs sets two links.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	set represented keyword number parentheses indicates number shared NE pair instances.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	describe evaluation clusters next subsection.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	3.3 Evaluation Results.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	evaluated results based two metrics.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	One accuracy within set phrases share keyword; accuracy links.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	easy make clear definition “paraphrase”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Although precise criterion, cases evaluated relatively clear-cut.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	set, phrases bracketed frequencies considered paraphrases set.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	accuracy calculated ratio number paraphrases total number phrases set.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	results, along total number phrases, shown Table 1.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	larger sets accurate small sets.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	make several observations cause errors.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	errors include NE tagging errors errors due phrase includes NEs.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Also cases one two NEs belong phrase outside relation.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	example, sentence “Mr.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	return issues discussion section.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	links “CC-domain shown Step 4 subsection 3.2.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	company buys another company, paying event occur, two phrases indicate event.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	similar explanation applies link “stake” set.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	checked whether discovered links listed WordNet.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	result suggests benefit using automatic discovery method.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Evaluation results links	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	work reported closely related [Ha- segawa et al. 04].	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	First, describe method compare method.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	first collect NE instance pairs contexts, like method.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	However, next step clearly different.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	cluster NE instance pairs based words contexts using bag- of-words method.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	threshold, NE instance pairs could used hence variety phrases also limited.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	number NE instance pairs used experiment less half method.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	kinds efforts discover paraphrase automatically corpora.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	availability comparable corpora limited, significant limitation approach.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	repeated several times collect list author / book title pairs expressions.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	However, methods need initial seeds, relation entities known advance.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	limitation obstacle making technology “open domain”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	However, desirable separate them.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	problem arises keywords consist one word.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	One possibility use n-grams based mutual information.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	frequent multi-word sequence domain, could use keyword candidate.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	explained results section, “strength” “add” desirable keywords CC-domain.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Also, “agree” CC-domain desirable keyword.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	relatively frequent word domain, used different extraction scenarios.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	checked similar verbs major domains, one.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Chunking enough find relationships.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	remains future work.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Limitations several limitations methods.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	phrases expressions length less 5 chunks, appear two NEs.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Also, method using keywords rules phrases don’t contain popular words domain.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	claiming method almighty.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Applications discovered paraphrases multiple applications.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	One obvious application information extraction.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	proposed unsupervised method discover paraphrases large untagged corpus.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	accuracies link 73% 86% two evaluated domains.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	results promising several avenues improving results.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	paper necessarily reflect position U.S. Government.	0
The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Automatic paraphrase discovery important challenging task.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	first stage identifies keyword phrase joins phrases keyword sets.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	second stage links sets involve pairs individual NEs.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	total 13,976 phrases grouped.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	One difficulties Natural Language Processing fact many ways express thing event.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	lot research lexical relations, along creation resources WordNet.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	order create IE system new domain, one spend long time create knowledge.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	2.1 Overview.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	explaining method detail, present brief overview subsection.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	First, large corpus, extract NE instance pairs.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	pair also record context, i.e. phrase two NEs (Step1).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	use simple TF/IDF method measure topicality words.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	shall see, linked sets paraphrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	overview illustrated Figure 1.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	sentences corpus tagged transformation-based chunker NE tagger.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Figure 2 shows examples extracted NE pair instances contexts.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Step 2.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	used TF/ITF metric identify keywords.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Figure 3 Figure 1.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Overview method 2.2 Step Step Algorithm.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	section, explain algorithm step step examples.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	size, examples (Figures 2 4) appear end paper.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	shows keywords scores.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Step 3.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	(If TF/IDF score word threshold, phrase discarded.)	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	gather phrases keyword.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Figure 4 shows phrase sets based keywords CC-domain.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Step 4.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Cluster phrases based Links set phrases share keyword.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	However, phrases express meanings even though share keyword.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	step, try link sets, put single cluster.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	clue NE instance pairs.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	pair NE instances used different phrases, phrases likely paraphrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Here, “EG” represents “Eastern Group Plc”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	“H” represents “Hanson Plc”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	So, set threshold least two examples required build link.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	examples shown Figure 5.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Notice CC-domain special case.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	links solve problem.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	figure 4, reverse relations indicated `*’ next frequency.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	sets phrases share keyword links sets.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	3.1 Corpora.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	sentences analyzed chunker NE tag- ger.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	3.2 Results.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	subsection, report results experiment, terms number words, phrases clusters.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	report evaluation results next subsection.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Step 1.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	frequency Company – Company domain ranks 11th 35,567 examples.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Step 2.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Find keywords NE pair keywords found NE category pair.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	natural larger data domain, keywords found.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	total, 2,000 NE category pairs, 5,184 keywords found.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Step 3.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	phrase contain keywords, phrase discarded.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	total, across domains, kept 13,976 phrases keywords.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Step 4.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	CC-domain, 32 sets phrases contain 2 phrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	concentrate sets.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Among 32 sets, found following pairs sets two links.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	set represented keyword number parentheses indicates number shared NE pair instances.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	describe evaluation clusters next subsection.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	3.3 Evaluation Results.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	evaluated results based two metrics.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	One accuracy within set phrases share keyword; accuracy links.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	easy make clear definition “paraphrase”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Although precise criterion, cases evaluated relatively clear-cut.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	set, phrases bracketed frequencies considered paraphrases set.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	accuracy calculated ratio number paraphrases total number phrases set.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	results, along total number phrases, shown Table 1.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	larger sets accurate small sets.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	make several observations cause errors.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	errors include NE tagging errors errors due phrase includes NEs.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Also cases one two NEs belong phrase outside relation.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	example, sentence “Mr.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	return issues discussion section.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	links “CC-domain shown Step 4 subsection 3.2.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	company buys another company, paying event occur, two phrases indicate event.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	similar explanation applies link “stake” set.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	checked whether discovered links listed WordNet.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	result suggests benefit using automatic discovery method.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Evaluation results links	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	work reported closely related [Ha- segawa et al. 04].	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	First, describe method compare method.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	first collect NE instance pairs contexts, like method.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	However, next step clearly different.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	cluster NE instance pairs based words contexts using bag- of-words method.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	threshold, NE instance pairs could used hence variety phrases also limited.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	number NE instance pairs used experiment less half method.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	kinds efforts discover paraphrase automatically corpora.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	availability comparable corpora limited, significant limitation approach.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	repeated several times collect list author / book title pairs expressions.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	However, methods need initial seeds, relation entities known advance.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	limitation obstacle making technology “open domain”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	However, desirable separate them.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	problem arises keywords consist one word.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	One possibility use n-grams based mutual information.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	frequent multi-word sequence domain, could use keyword candidate.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	explained results section, “strength” “add” desirable keywords CC-domain.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Also, “agree” CC-domain desirable keyword.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	relatively frequent word domain, used different extraction scenarios.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	checked similar verbs major domains, one.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Smith estimates Lotus make profit quarter…”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Chunking enough find relationships.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	remains future work.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Limitations several limitations methods.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	phrases expressions length less 5 chunks, appear two NEs.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Also, method using keywords rules phrases don’t contain popular words domain.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	claiming method almighty.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Applications discovered paraphrases multiple applications.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	One obvious application information extraction.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	proposed unsupervised method discover paraphrases large untagged corpus.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	accuracies link 73% 86% two evaluated domains.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	results promising several avenues improving results.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	paper necessarily reflect position U.S. Government.	0
In the same way, most NLP systems like information retrieval (Sekine, 2005) or question answering (Duclaye et al., 2003), based on pattern recognition, can be improved by a paraphrase generator.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Automatic paraphrase discovery important challenging task.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	first stage identifies keyword phrase joins phrases keyword sets.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	second stage links sets involve pairs individual NEs.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	total 13,976 phrases grouped.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	One difficulties Natural Language Processing fact many ways express thing event.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	lot research lexical relations, along creation resources WordNet.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	order create IE system new domain, one spend long time create knowledge.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	paper, propose unsupervised method discover paraphrases large untagged corpus.	1
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	2.1 Overview.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	explaining method detail, present brief overview subsection.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	First, large corpus, extract NE instance pairs.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	pair also record context, i.e. phrase two NEs (Step1).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	use simple TF/IDF method measure topicality words.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	shall see, linked sets paraphrases.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	overview illustrated Figure 1.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	sentences corpus tagged transformation-based chunker NE tagger.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Figure 2 shows examples extracted NE pair instances contexts.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Step 2.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	used TF/ITF metric identify keywords.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Figure 3 Figure 1.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Overview method 2.2 Step Step Algorithm.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	section, explain algorithm step step examples.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	size, examples (Figures 2 4) appear end paper.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	shows keywords scores.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Step 3.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	(If TF/IDF score word threshold, phrase discarded.)	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	gather phrases keyword.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Figure 4 shows phrase sets based keywords CC-domain.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Step 4.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Cluster phrases based Links set phrases share keyword.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	However, phrases express meanings even though share keyword.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	step, try link sets, put single cluster.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	clue NE instance pairs.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	pair NE instances used different phrases, phrases likely paraphrases.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Here, “EG” represents “Eastern Group Plc”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	“H” represents “Hanson Plc”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	So, set threshold least two examples required build link.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	examples shown Figure 5.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Notice CC-domain special case.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	links solve problem.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	figure 4, reverse relations indicated `*’ next frequency.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	sets phrases share keyword links sets.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	3.1 Corpora.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	sentences analyzed chunker NE tag- ger.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	3.2 Results.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	subsection, report results experiment, terms number words, phrases clusters.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	report evaluation results next subsection.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Step 1.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	frequency Company – Company domain ranks 11th 35,567 examples.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Step 2.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Find keywords NE pair keywords found NE category pair.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	natural larger data domain, keywords found.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	total, 2,000 NE category pairs, 5,184 keywords found.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Step 3.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	phrase contain keywords, phrase discarded.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	total, across domains, kept 13,976 phrases keywords.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Step 4.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	CC-domain, 32 sets phrases contain 2 phrases.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	concentrate sets.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Among 32 sets, found following pairs sets two links.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	set represented keyword number parentheses indicates number shared NE pair instances.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	describe evaluation clusters next subsection.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	3.3 Evaluation Results.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	evaluated results based two metrics.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	One accuracy within set phrases share keyword; accuracy links.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	easy make clear definition “paraphrase”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Although precise criterion, cases evaluated relatively clear-cut.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	set, phrases bracketed frequencies considered paraphrases set.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	accuracy calculated ratio number paraphrases total number phrases set.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	results, along total number phrases, shown Table 1.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	larger sets accurate small sets.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	make several observations cause errors.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	errors include NE tagging errors errors due phrase includes NEs.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Also cases one two NEs belong phrase outside relation.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	example, sentence “Mr.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	return issues discussion section.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	links “CC-domain shown Step 4 subsection 3.2.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	company buys another company, paying event occur, two phrases indicate event.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	similar explanation applies link “stake” set.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	checked whether discovered links listed WordNet.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	result suggests benefit using automatic discovery method.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Evaluation results links	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	work reported closely related [Ha- segawa et al. 04].	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	First, describe method compare method.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	first collect NE instance pairs contexts, like method.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	However, next step clearly different.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	cluster NE instance pairs based words contexts using bag- of-words method.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	threshold, NE instance pairs could used hence variety phrases also limited.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	number NE instance pairs used experiment less half method.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	kinds efforts discover paraphrase automatically corpora.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	availability comparable corpora limited, significant limitation approach.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	repeated several times collect list author / book title pairs expressions.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	However, methods need initial seeds, relation entities known advance.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	limitation obstacle making technology “open domain”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	However, desirable separate them.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	problem arises keywords consist one word.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	One possibility use n-grams based mutual information.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	frequent multi-word sequence domain, could use keyword candidate.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	explained results section, “strength” “add” desirable keywords CC-domain.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Also, “agree” CC-domain desirable keyword.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	relatively frequent word domain, used different extraction scenarios.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	checked similar verbs major domains, one.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Smith estimates Lotus make profit quarter…”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Chunking enough find relationships.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	remains future work.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Limitations several limitations methods.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	phrases expressions length less 5 chunks, appear two NEs.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Also, method using keywords rules phrases don’t contain popular words domain.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	claiming method almighty.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Applications discovered paraphrases multiple applications.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	One obvious application information extraction.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	proposed unsupervised method discover paraphrases large untagged corpus.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	accuracies link 73% 86% two evaluated domains.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	results promising several avenues improving results.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	paper necessarily reflect position U.S. Government.	0
Data-driven paraphrase discovery methods (Lin and Pantel, 2001; Pasca and Dienes, 2005; Wu and Zhou, 2003; Sekine, 2005) extends the idea of distributional similarity to phrases.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Automatic paraphrase discovery important challenging task.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	first stage identifies keyword phrase joins phrases keyword sets.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	second stage links sets involve pairs individual NEs.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	total 13,976 phrases grouped.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	One difficulties Natural Language Processing fact many ways express thing event.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	lot research lexical relations, along creation resources WordNet.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	order create IE system new domain, one spend long time create knowledge.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	2.1 Overview.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	explaining method detail, present brief overview subsection.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	First, large corpus, extract NE instance pairs.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	pair also record context, i.e. phrase two NEs (Step1).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	use simple TF/IDF method measure topicality words.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	shall see, linked sets paraphrases.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	overview illustrated Figure 1.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	sentences corpus tagged transformation-based chunker NE tagger.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Figure 2 shows examples extracted NE pair instances contexts.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Step 2.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	used TF/ITF metric identify keywords.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Figure 3 Figure 1.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Overview method 2.2 Step Step Algorithm.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	section, explain algorithm step step examples.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	size, examples (Figures 2 4) appear end paper.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	shows keywords scores.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Step 3.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	(If TF/IDF score word threshold, phrase discarded.)	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	gather phrases keyword.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Figure 4 shows phrase sets based keywords CC-domain.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Step 4.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Cluster phrases based Links set phrases share keyword.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	However, phrases express meanings even though share keyword.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	step, try link sets, put single cluster.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	clue NE instance pairs.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	pair NE instances used different phrases, phrases likely paraphrases.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Here, “EG” represents “Eastern Group Plc”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	“H” represents “Hanson Plc”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	So, set threshold least two examples required build link.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	examples shown Figure 5.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Notice CC-domain special case.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	links solve problem.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	figure 4, reverse relations indicated `*’ next frequency.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	sets phrases share keyword links sets.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	3.1 Corpora.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	sentences analyzed chunker NE tag- ger.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	3.2 Results.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	subsection, report results experiment, terms number words, phrases clusters.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	report evaluation results next subsection.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Step 1.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	frequency Company – Company domain ranks 11th 35,567 examples.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Step 2.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Find keywords NE pair keywords found NE category pair.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	natural larger data domain, keywords found.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	total, 2,000 NE category pairs, 5,184 keywords found.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Step 3.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	phrase contain keywords, phrase discarded.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	total, across domains, kept 13,976 phrases keywords.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Step 4.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	CC-domain, 32 sets phrases contain 2 phrases.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	concentrate sets.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Among 32 sets, found following pairs sets two links.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	set represented keyword number parentheses indicates number shared NE pair instances.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	describe evaluation clusters next subsection.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	3.3 Evaluation Results.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	evaluated results based two metrics.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	One accuracy within set phrases share keyword; accuracy links.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	easy make clear definition “paraphrase”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Although precise criterion, cases evaluated relatively clear-cut.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	set, phrases bracketed frequencies considered paraphrases set.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	accuracy calculated ratio number paraphrases total number phrases set.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	results, along total number phrases, shown Table 1.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	larger sets accurate small sets.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	make several observations cause errors.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	errors include NE tagging errors errors due phrase includes NEs.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Also cases one two NEs belong phrase outside relation.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	example, sentence “Mr.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	return issues discussion section.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	links “CC-domain shown Step 4 subsection 3.2.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	company buys another company, paying event occur, two phrases indicate event.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	similar explanation applies link “stake” set.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	checked whether discovered links listed WordNet.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	result suggests benefit using automatic discovery method.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Evaluation results links	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	work reported closely related [Ha- segawa et al. 04].	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	First, describe method compare method.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	first collect NE instance pairs contexts, like method.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	However, next step clearly different.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	cluster NE instance pairs based words contexts using bag- of-words method.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	threshold, NE instance pairs could used hence variety phrases also limited.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	number NE instance pairs used experiment less half method.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	kinds efforts discover paraphrase automatically corpora.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	availability comparable corpora limited, significant limitation approach.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	repeated several times collect list author / book title pairs expressions.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	However, methods need initial seeds, relation entities known advance.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	limitation obstacle making technology “open domain”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	However, desirable separate them.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	problem arises keywords consist one word.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	One possibility use n-grams based mutual information.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	frequent multi-word sequence domain, could use keyword candidate.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	explained results section, “strength” “add” desirable keywords CC-domain.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Also, “agree” CC-domain desirable keyword.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	relatively frequent word domain, used different extraction scenarios.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	checked similar verbs major domains, one.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Smith estimates Lotus make profit quarter…”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Chunking enough find relationships.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	remains future work.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Limitations several limitations methods.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	phrases expressions length less 5 chunks, appear two NEs.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Also, method using keywords rules phrases don’t contain popular words domain.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	claiming method almighty.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Applications discovered paraphrases multiple applications.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	One obvious application information extraction.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	proposed unsupervised method discover paraphrases large untagged corpus.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	accuracies link 73% 86% two evaluated domains.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	results promising several avenues improving results.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	paper necessarily reflect position U.S. Government.	0
A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Automatic paraphrase discovery important challenging task.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	first stage identifies keyword phrase joins phrases keyword sets.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	second stage links sets involve pairs individual NEs.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	total 13,976 phrases grouped.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	One difficulties Natural Language Processing fact many ways express thing event.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	lot research lexical relations, along creation resources WordNet.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	order create IE system new domain, one spend long time create knowledge.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	2.1 Overview.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	explaining method detail, present brief overview subsection.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	First, large corpus, extract NE instance pairs.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	pair also record context, i.e. phrase two NEs (Step1).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	use simple TF/IDF method measure topicality words.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	shall see, linked sets paraphrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	overview illustrated Figure 1.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	sentences corpus tagged transformation-based chunker NE tagger.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Figure 2 shows examples extracted NE pair instances contexts.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Step 2.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	used TF/ITF metric identify keywords.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Figure 3 Figure 1.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Overview method 2.2 Step Step Algorithm.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	section, explain algorithm step step examples.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	size, examples (Figures 2 4) appear end paper.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	shows keywords scores.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Step 3.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	(If TF/IDF score word threshold, phrase discarded.)	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	gather phrases keyword.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Figure 4 shows phrase sets based keywords CC-domain.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Step 4.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Cluster phrases based Links set phrases share keyword.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	However, phrases express meanings even though share keyword.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	step, try link sets, put single cluster.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	clue NE instance pairs.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	pair NE instances used different phrases, phrases likely paraphrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Here, “EG” represents “Eastern Group Plc”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	“H” represents “Hanson Plc”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	So, set threshold least two examples required build link.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	examples shown Figure 5.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Notice CC-domain special case.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	links solve problem.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	figure 4, reverse relations indicated `*’ next frequency.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	sets phrases share keyword links sets.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	3.1 Corpora.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	sentences analyzed chunker NE tag- ger.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	3.2 Results.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	subsection, report results experiment, terms number words, phrases clusters.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	report evaluation results next subsection.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Step 1.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	frequency Company – Company domain ranks 11th 35,567 examples.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Step 2.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Find keywords NE pair keywords found NE category pair.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	natural larger data domain, keywords found.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	total, 2,000 NE category pairs, 5,184 keywords found.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Step 3.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	phrase contain keywords, phrase discarded.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	total, across domains, kept 13,976 phrases keywords.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Step 4.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	CC-domain, 32 sets phrases contain 2 phrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	concentrate sets.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Among 32 sets, found following pairs sets two links.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	set represented keyword number parentheses indicates number shared NE pair instances.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	describe evaluation clusters next subsection.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	3.3 Evaluation Results.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	evaluated results based two metrics.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	One accuracy within set phrases share keyword; accuracy links.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	easy make clear definition “paraphrase”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Although precise criterion, cases evaluated relatively clear-cut.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	set, phrases bracketed frequencies considered paraphrases set.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	accuracy calculated ratio number paraphrases total number phrases set.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	results, along total number phrases, shown Table 1.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	larger sets accurate small sets.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	make several observations cause errors.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	errors include NE tagging errors errors due phrase includes NEs.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Also cases one two NEs belong phrase outside relation.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	example, sentence “Mr.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	return issues discussion section.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	links “CC-domain shown Step 4 subsection 3.2.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	company buys another company, paying event occur, two phrases indicate event.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	similar explanation applies link “stake” set.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	checked whether discovered links listed WordNet.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	result suggests benefit using automatic discovery method.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Evaluation results links	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	work reported closely related [Ha- segawa et al. 04].	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	First, describe method compare method.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	first collect NE instance pairs contexts, like method.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	However, next step clearly different.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	cluster NE instance pairs based words contexts using bag- of-words method.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	threshold, NE instance pairs could used hence variety phrases also limited.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	number NE instance pairs used experiment less half method.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	kinds efforts discover paraphrase automatically corpora.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	availability comparable corpora limited, significant limitation approach.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	repeated several times collect list author / book title pairs expressions.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	However, methods need initial seeds, relation entities known advance.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	limitation obstacle making technology “open domain”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	However, desirable separate them.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	problem arises keywords consist one word.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	One possibility use n-grams based mutual information.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	frequent multi-word sequence domain, could use keyword candidate.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	explained results section, “strength” “add” desirable keywords CC-domain.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Also, “agree” CC-domain desirable keyword.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	relatively frequent word domain, used different extraction scenarios.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	checked similar verbs major domains, one.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Smith estimates Lotus make profit quarter…”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Chunking enough find relationships.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	remains future work.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Limitations several limitations methods.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	phrases expressions length less 5 chunks, appear two NEs.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Also, method using keywords rules phrases don’t contain popular words domain.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	claiming method almighty.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Applications discovered paraphrases multiple applications.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	One obvious application information extraction.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	proposed unsupervised method discover paraphrases large untagged corpus.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	accuracies link 73% 86% two evaluated domains.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	results promising several avenues improving results.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	paper necessarily reflect position U.S. Government.	0
To avoid the drawback, several linguistic clues, such as fine-grained classification of named entities and coordinated sentences, have been utilized (Sekine, 2005; Torisawa, 2006).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Automatic paraphrase discovery important challenging task.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	first stage identifies keyword phrase joins phrases keyword sets.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	second stage links sets involve pairs individual NEs.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	total 13,976 phrases grouped.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	One difficulties Natural Language Processing fact many ways express thing event.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	lot research lexical relations, along creation resources WordNet.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	order create IE system new domain, one spend long time create knowledge.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	2.1 Overview.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	explaining method detail, present brief overview subsection.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	First, large corpus, extract NE instance pairs.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	pair also record context, i.e. phrase two NEs (Step1).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	use simple TF/IDF method measure topicality words.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	shall see, linked sets paraphrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	overview illustrated Figure 1.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	sentences corpus tagged transformation-based chunker NE tagger.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Figure 2 shows examples extracted NE pair instances contexts.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Step 2.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	used TF/ITF metric identify keywords.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Figure 3 Figure 1.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Overview method 2.2 Step Step Algorithm.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	section, explain algorithm step step examples.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	size, examples (Figures 2 4) appear end paper.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	shows keywords scores.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Step 3.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	(If TF/IDF score word threshold, phrase discarded.)	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	gather phrases keyword.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Figure 4 shows phrase sets based keywords CC-domain.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Step 4.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Cluster phrases based Links set phrases share keyword.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	However, phrases express meanings even though share keyword.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	step, try link sets, put single cluster.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	clue NE instance pairs.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	pair NE instances used different phrases, phrases likely paraphrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Here, “EG” represents “Eastern Group Plc”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	“H” represents “Hanson Plc”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	So, set threshold least two examples required build link.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	examples shown Figure 5.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Notice CC-domain special case.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	links solve problem.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	figure 4, reverse relations indicated `*’ next frequency.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	sets phrases share keyword links sets.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	3.1 Corpora.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	sentences analyzed chunker NE tag- ger.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	3.2 Results.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	subsection, report results experiment, terms number words, phrases clusters.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	report evaluation results next subsection.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Step 1.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	frequency Company – Company domain ranks 11th 35,567 examples.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Step 2.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Find keywords NE pair keywords found NE category pair.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	natural larger data domain, keywords found.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	total, 2,000 NE category pairs, 5,184 keywords found.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Step 3.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	phrase contain keywords, phrase discarded.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	total, across domains, kept 13,976 phrases keywords.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Step 4.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	CC-domain, 32 sets phrases contain 2 phrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	concentrate sets.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Among 32 sets, found following pairs sets two links.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	set represented keyword number parentheses indicates number shared NE pair instances.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	describe evaluation clusters next subsection.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	3.3 Evaluation Results.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	evaluated results based two metrics.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	One accuracy within set phrases share keyword; accuracy links.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	easy make clear definition “paraphrase”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Although precise criterion, cases evaluated relatively clear-cut.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	set, phrases bracketed frequencies considered paraphrases set.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	accuracy calculated ratio number paraphrases total number phrases set.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	results, along total number phrases, shown Table 1.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	larger sets accurate small sets.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	make several observations cause errors.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	errors include NE tagging errors errors due phrase includes NEs.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Also cases one two NEs belong phrase outside relation.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	example, sentence “Mr.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	return issues discussion section.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	links “CC-domain shown Step 4 subsection 3.2.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	company buys another company, paying event occur, two phrases indicate event.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	similar explanation applies link “stake” set.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	checked whether discovered links listed WordNet.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	result suggests benefit using automatic discovery method.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Evaluation results links	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	work reported closely related [Ha- segawa et al. 04].	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	First, describe method compare method.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	first collect NE instance pairs contexts, like method.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	However, next step clearly different.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	cluster NE instance pairs based words contexts using bag- of-words method.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	threshold, NE instance pairs could used hence variety phrases also limited.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	number NE instance pairs used experiment less half method.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	kinds efforts discover paraphrase automatically corpora.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	availability comparable corpora limited, significant limitation approach.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	repeated several times collect list author / book title pairs expressions.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	However, methods need initial seeds, relation entities known advance.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	limitation obstacle making technology “open domain”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	However, desirable separate them.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	problem arises keywords consist one word.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	One possibility use n-grams based mutual information.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	frequent multi-word sequence domain, could use keyword candidate.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	explained results section, “strength” “add” desirable keywords CC-domain.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Also, “agree” CC-domain desirable keyword.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	relatively frequent word domain, used different extraction scenarios.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	checked similar verbs major domains, one.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Smith estimates Lotus make profit quarter…”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Chunking enough find relationships.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	remains future work.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Limitations several limitations methods.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	phrases expressions length less 5 chunks, appear two NEs.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Also, method using keywords rules phrases don’t contain popular words domain.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	claiming method almighty.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Applications discovered paraphrases multiple applications.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	One obvious application information extraction.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	proposed unsupervised method discover paraphrases large untagged corpus.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	accuracies link 73% 86% two evaluated domains.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	results promising several avenues improving results.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	paper necessarily reflect position U.S. Government.	0
As a later reﬁnement, Sekine (2005) makes a similar attempt at using distributional similarity over named entity pairs in order to produce a list of fully lexicalized phrasal paraphrases for speciﬁc concepts represented by keywords.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	1
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Automatic paraphrase discovery important challenging task.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	first stage identifies keyword phrase joins phrases keyword sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	second stage links sets involve pairs individual NEs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	total 13,976 phrases grouped.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	One difficulties Natural Language Processing fact many ways express thing event.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	lot research lexical relations, along creation resources WordNet.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	order create IE system new domain, one spend long time create knowledge.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	2.1 Overview.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	explaining method detail, present brief overview subsection.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	First, large corpus, extract NE instance pairs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	pair also record context, i.e. phrase two NEs (Step1).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	use simple TF/IDF method measure topicality words.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	shall see, linked sets paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	overview illustrated Figure 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	sentences corpus tagged transformation-based chunker NE tagger.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Figure 2 shows examples extracted NE pair instances contexts.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Step 2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	used TF/ITF metric identify keywords.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Figure 3 Figure 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Overview method 2.2 Step Step Algorithm.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	section, explain algorithm step step examples.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	size, examples (Figures 2 4) appear end paper.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	shows keywords scores.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Step 3.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	(If TF/IDF score word threshold, phrase discarded.)	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	gather phrases keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Figure 4 shows phrase sets based keywords CC-domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Step 4.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Cluster phrases based Links set phrases share keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	However, phrases express meanings even though share keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	step, try link sets, put single cluster.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	clue NE instance pairs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	pair NE instances used different phrases, phrases likely paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Here, “EG” represents “Eastern Group Plc”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	“H” represents “Hanson Plc”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	So, set threshold least two examples required build link.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	examples shown Figure 5.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Notice CC-domain special case.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	links solve problem.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	figure 4, reverse relations indicated `*’ next frequency.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	sets phrases share keyword links sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	3.1 Corpora.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	sentences analyzed chunker NE tag- ger.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	3.2 Results.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	subsection, report results experiment, terms number words, phrases clusters.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	report evaluation results next subsection.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Step 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	frequency Company – Company domain ranks 11th 35,567 examples.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Step 2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Find keywords NE pair keywords found NE category pair.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	natural larger data domain, keywords found.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	total, 2,000 NE category pairs, 5,184 keywords found.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Step 3.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	phrase contain keywords, phrase discarded.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	total, across domains, kept 13,976 phrases keywords.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Step 4.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	CC-domain, 32 sets phrases contain 2 phrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	concentrate sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Among 32 sets, found following pairs sets two links.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	set represented keyword number parentheses indicates number shared NE pair instances.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	describe evaluation clusters next subsection.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	3.3 Evaluation Results.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	evaluated results based two metrics.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	One accuracy within set phrases share keyword; accuracy links.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	easy make clear definition “paraphrase”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Although precise criterion, cases evaluated relatively clear-cut.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	set, phrases bracketed frequencies considered paraphrases set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	accuracy calculated ratio number paraphrases total number phrases set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	results, along total number phrases, shown Table 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	larger sets accurate small sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	make several observations cause errors.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	errors include NE tagging errors errors due phrase includes NEs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Also cases one two NEs belong phrase outside relation.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	example, sentence “Mr.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	return issues discussion section.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	links “CC-domain shown Step 4 subsection 3.2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	company buys another company, paying event occur, two phrases indicate event.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	similar explanation applies link “stake” set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	checked whether discovered links listed WordNet.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	result suggests benefit using automatic discovery method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Evaluation results links	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	work reported closely related [Ha- segawa et al. 04].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	First, describe method compare method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	first collect NE instance pairs contexts, like method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	However, next step clearly different.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	cluster NE instance pairs based words contexts using bag- of-words method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	threshold, NE instance pairs could used hence variety phrases also limited.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	number NE instance pairs used experiment less half method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	kinds efforts discover paraphrase automatically corpora.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	availability comparable corpora limited, significant limitation approach.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	repeated several times collect list author / book title pairs expressions.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	However, methods need initial seeds, relation entities known advance.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	limitation obstacle making technology “open domain”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	However, desirable separate them.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	problem arises keywords consist one word.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	One possibility use n-grams based mutual information.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	frequent multi-word sequence domain, could use keyword candidate.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	explained results section, “strength” “add” desirable keywords CC-domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Also, “agree” CC-domain desirable keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	relatively frequent word domain, used different extraction scenarios.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	checked similar verbs major domains, one.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Smith estimates Lotus make profit quarter…”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Chunking enough find relationships.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	remains future work.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Limitations several limitations methods.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	phrases expressions length less 5 chunks, appear two NEs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Also, method using keywords rules phrases don’t contain popular words domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	claiming method almighty.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Applications discovered paraphrases multiple applications.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	One obvious application information extraction.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	proposed unsupervised method discover paraphrases large untagged corpus.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	accuracies link 73% 86% two evaluated domains.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	results promising several avenues improving results.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	paper necessarily reflect position U.S. Government.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates (Lin and Pantel 2001; Sekine 2005; Szpektor and Dagan 2008; Yates and Etzioni 2009; Schoenmackers et al. 2010).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Automatic paraphrase discovery important challenging task.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	first stage identifies keyword phrase joins phrases keyword sets.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	second stage links sets involve pairs individual NEs.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	total 13,976 phrases grouped.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	One difficulties Natural Language Processing fact many ways express thing event.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	lot research lexical relations, along creation resources WordNet.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	order create IE system new domain, one spend long time create knowledge.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	2.1 Overview.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	explaining method detail, present brief overview subsection.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	First, large corpus, extract NE instance pairs.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	pair also record context, i.e. phrase two NEs (Step1).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	use simple TF/IDF method measure topicality words.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	shall see, linked sets paraphrases.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	overview illustrated Figure 1.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	sentences corpus tagged transformation-based chunker NE tagger.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Figure 2 shows examples extracted NE pair instances contexts.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Step 2.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	used TF/ITF metric identify keywords.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Figure 3 Figure 1.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Overview method 2.2 Step Step Algorithm.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	section, explain algorithm step step examples.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	size, examples (Figures 2 4) appear end paper.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	shows keywords scores.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Step 3.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	(If TF/IDF score word threshold, phrase discarded.)	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	gather phrases keyword.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Figure 4 shows phrase sets based keywords CC-domain.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Step 4.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Cluster phrases based Links set phrases share keyword.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	However, phrases express meanings even though share keyword.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	step, try link sets, put single cluster.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	clue NE instance pairs.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	pair NE instances used different phrases, phrases likely paraphrases.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Here, “EG” represents “Eastern Group Plc”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	“H” represents “Hanson Plc”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	So, set threshold least two examples required build link.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	examples shown Figure 5.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Notice CC-domain special case.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	links solve problem.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	figure 4, reverse relations indicated `*’ next frequency.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	sets phrases share keyword links sets.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	3.1 Corpora.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	sentences analyzed chunker NE tag- ger.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	3.2 Results.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	subsection, report results experiment, terms number words, phrases clusters.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	report evaluation results next subsection.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Step 1.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	frequency Company – Company domain ranks 11th 35,567 examples.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Step 2.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Find keywords NE pair keywords found NE category pair.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	natural larger data domain, keywords found.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	total, 2,000 NE category pairs, 5,184 keywords found.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Step 3.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	phrase contain keywords, phrase discarded.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	total, across domains, kept 13,976 phrases keywords.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Step 4.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	CC-domain, 32 sets phrases contain 2 phrases.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	concentrate sets.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Among 32 sets, found following pairs sets two links.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	set represented keyword number parentheses indicates number shared NE pair instances.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	describe evaluation clusters next subsection.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	3.3 Evaluation Results.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	evaluated results based two metrics.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	One accuracy within set phrases share keyword; accuracy links.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	easy make clear definition “paraphrase”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Although precise criterion, cases evaluated relatively clear-cut.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	set, phrases bracketed frequencies considered paraphrases set.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	accuracy calculated ratio number paraphrases total number phrases set.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	results, along total number phrases, shown Table 1.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	larger sets accurate small sets.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	make several observations cause errors.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	errors include NE tagging errors errors due phrase includes NEs.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Also cases one two NEs belong phrase outside relation.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	example, sentence “Mr.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	return issues discussion section.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	links “CC-domain shown Step 4 subsection 3.2.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	company buys another company, paying event occur, two phrases indicate event.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	similar explanation applies link “stake” set.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	checked whether discovered links listed WordNet.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	result suggests benefit using automatic discovery method.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Evaluation results links	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	work reported closely related [Ha- segawa et al. 04].	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	First, describe method compare method.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	first collect NE instance pairs contexts, like method.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	However, next step clearly different.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	cluster NE instance pairs based words contexts using bag- of-words method.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	threshold, NE instance pairs could used hence variety phrases also limited.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	number NE instance pairs used experiment less half method.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	kinds efforts discover paraphrase automatically corpora.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	availability comparable corpora limited, significant limitation approach.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	repeated several times collect list author / book title pairs expressions.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	However, methods need initial seeds, relation entities known advance.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	limitation obstacle making technology “open domain”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	However, desirable separate them.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	problem arises keywords consist one word.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	One possibility use n-grams based mutual information.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	frequent multi-word sequence domain, could use keyword candidate.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	explained results section, “strength” “add” desirable keywords CC-domain.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Also, “agree” CC-domain desirable keyword.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	relatively frequent word domain, used different extraction scenarios.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	checked similar verbs major domains, one.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Smith estimates Lotus make profit quarter…”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Chunking enough find relationships.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	remains future work.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Limitations several limitations methods.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	phrases expressions length less 5 chunks, appear two NEs.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Also, method using keywords rules phrases don’t contain popular words domain.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	claiming method almighty.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Applications discovered paraphrases multiple applications.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	One obvious application information extraction.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	proposed unsupervised method discover paraphrases large untagged corpus.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	accuracies link 73% 86% two evaluated domains.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	results promising several avenues improving results.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	paper necessarily reflect position U.S. Government.	0
We also proposed a method to find paraphrases in the context of two Named Entity instances in a large un-annotated corpus (Sekine 05).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Automatic paraphrase discovery important challenging task.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	first stage identifies keyword phrase joins phrases keyword sets.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	second stage links sets involve pairs individual NEs.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	total 13,976 phrases grouped.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	One difficulties Natural Language Processing fact many ways express thing event.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	lot research lexical relations, along creation resources WordNet.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	order create IE system new domain, one spend long time create knowledge.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	2.1 Overview.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	explaining method detail, present brief overview subsection.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	First, large corpus, extract NE instance pairs.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	pair also record context, i.e. phrase two NEs (Step1).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	use simple TF/IDF method measure topicality words.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	shall see, linked sets paraphrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	overview illustrated Figure 1.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	1
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	sentences corpus tagged transformation-based chunker NE tagger.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Figure 2 shows examples extracted NE pair instances contexts.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Step 2.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	used TF/ITF metric identify keywords.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Figure 3 Figure 1.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Overview method 2.2 Step Step Algorithm.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	section, explain algorithm step step examples.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	size, examples (Figures 2 4) appear end paper.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	shows keywords scores.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Step 3.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	(If TF/IDF score word threshold, phrase discarded.)	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	gather phrases keyword.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Figure 4 shows phrase sets based keywords CC-domain.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Step 4.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Cluster phrases based Links set phrases share keyword.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	However, phrases express meanings even though share keyword.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	step, try link sets, put single cluster.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	clue NE instance pairs.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	pair NE instances used different phrases, phrases likely paraphrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Here, “EG” represents “Eastern Group Plc”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	“H” represents “Hanson Plc”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	So, set threshold least two examples required build link.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	examples shown Figure 5.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Notice CC-domain special case.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	links solve problem.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	figure 4, reverse relations indicated `*’ next frequency.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	sets phrases share keyword links sets.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	3.1 Corpora.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	sentences analyzed chunker NE tag- ger.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	3.2 Results.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	subsection, report results experiment, terms number words, phrases clusters.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	report evaluation results next subsection.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Step 1.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	frequency Company – Company domain ranks 11th 35,567 examples.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Step 2.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Find keywords NE pair keywords found NE category pair.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	natural larger data domain, keywords found.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	total, 2,000 NE category pairs, 5,184 keywords found.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Step 3.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	phrase contain keywords, phrase discarded.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	total, across domains, kept 13,976 phrases keywords.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Step 4.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	CC-domain, 32 sets phrases contain 2 phrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	concentrate sets.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Among 32 sets, found following pairs sets two links.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	set represented keyword number parentheses indicates number shared NE pair instances.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	describe evaluation clusters next subsection.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	3.3 Evaluation Results.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	evaluated results based two metrics.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	One accuracy within set phrases share keyword; accuracy links.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	easy make clear definition “paraphrase”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Although precise criterion, cases evaluated relatively clear-cut.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	set, phrases bracketed frequencies considered paraphrases set.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	accuracy calculated ratio number paraphrases total number phrases set.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	results, along total number phrases, shown Table 1.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	larger sets accurate small sets.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	make several observations cause errors.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	errors include NE tagging errors errors due phrase includes NEs.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also cases one two NEs belong phrase outside relation.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	example, sentence “Mr.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	return issues discussion section.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	links “CC-domain shown Step 4 subsection 3.2.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	company buys another company, paying event occur, two phrases indicate event.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	similar explanation applies link “stake” set.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	checked whether discovered links listed WordNet.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	result suggests benefit using automatic discovery method.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Evaluation results links	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	work reported closely related [Ha- segawa et al. 04].	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	First, describe method compare method.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	first collect NE instance pairs contexts, like method.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	However, next step clearly different.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	cluster NE instance pairs based words contexts using bag- of-words method.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	threshold, NE instance pairs could used hence variety phrases also limited.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	number NE instance pairs used experiment less half method.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	kinds efforts discover paraphrase automatically corpora.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	availability comparable corpora limited, significant limitation approach.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	repeated several times collect list author / book title pairs expressions.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	However, methods need initial seeds, relation entities known advance.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	limitation obstacle making technology “open domain”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	However, desirable separate them.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	problem arises keywords consist one word.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	One possibility use n-grams based mutual information.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	frequent multi-word sequence domain, could use keyword candidate.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	explained results section, “strength” “add” desirable keywords CC-domain.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also, “agree” CC-domain desirable keyword.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	relatively frequent word domain, used different extraction scenarios.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	checked similar verbs major domains, one.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Smith estimates Lotus make profit quarter…”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Chunking enough find relationships.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	remains future work.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Limitations several limitations methods.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	phrases expressions length less 5 chunks, appear two NEs.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also, method using keywords rules phrases don’t contain popular words domain.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	claiming method almighty.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Applications discovered paraphrases multiple applications.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	One obvious application information extraction.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	proposed unsupervised method discover paraphrases large untagged corpus.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	accuracies link 73% 86% two evaluated domains.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	results promising several avenues improving results.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	paper necessarily reflect position U.S. Government.	0
Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most donâ€™t.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Automatic paraphrase discovery important challenging task.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	first stage identifies keyword phrase joins phrases keyword sets.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	second stage links sets involve pairs individual NEs.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases grouped.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	One difficulties Natural Language Processing fact many ways express thing event.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	lot research lexical relations, along creation resources WordNet.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	order create IE system new domain, one spend long time create knowledge.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	2.1 Overview.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	explaining method detail, present brief overview subsection.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	First, large corpus, extract NE instance pairs.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	pair also record context, i.e. phrase two NEs (Step1).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	use simple TF/IDF method measure topicality words.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	shall see, linked sets paraphrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	overview illustrated Figure 1.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	1
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	sentences corpus tagged transformation-based chunker NE tagger.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Figure 2 shows examples extracted NE pair instances contexts.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	used TF/ITF metric identify keywords.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Figure 3 Figure 1.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Overview method 2.2 Step Step Algorithm.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	section, explain algorithm step step examples.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	size, examples (Figures 2 4) appear end paper.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	shows keywords scores.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	(If TF/IDF score word threshold, phrase discarded.)	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	gather phrases keyword.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Figure 4 shows phrase sets based keywords CC-domain.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Cluster phrases based Links set phrases share keyword.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	However, phrases express meanings even though share keyword.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	step, try link sets, put single cluster.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	clue NE instance pairs.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	pair NE instances used different phrases, phrases likely paraphrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Here, “EG” represents “Eastern Group Plc”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	“H” represents “Hanson Plc”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	So, set threshold least two examples required build link.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	examples shown Figure 5.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Notice CC-domain special case.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	links solve problem.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	figure 4, reverse relations indicated `*’ next frequency.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	sets phrases share keyword links sets.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	3.1 Corpora.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	sentences analyzed chunker NE tag- ger.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	3.2 Results.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	subsection, report results experiment, terms number words, phrases clusters.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	report evaluation results next subsection.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Step 1.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	frequency Company – Company domain ranks 11th 35,567 examples.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair keywords found NE category pair.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	natural larger data domain, keywords found.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	total, 2,000 NE category pairs, 5,184 keywords found.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	phrase contain keywords, phrase discarded.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	total, across domains, kept 13,976 phrases keywords.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	CC-domain, 32 sets phrases contain 2 phrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	concentrate sets.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Among 32 sets, found following pairs sets two links.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	set represented keyword number parentheses indicates number shared NE pair instances.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	describe evaluation clusters next subsection.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	3.3 Evaluation Results.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	evaluated results based two metrics.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	One accuracy within set phrases share keyword; accuracy links.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	easy make clear definition “paraphrase”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Although precise criterion, cases evaluated relatively clear-cut.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	set, phrases bracketed frequencies considered paraphrases set.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	accuracy calculated ratio number paraphrases total number phrases set.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	results, along total number phrases, shown Table 1.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	larger sets accurate small sets.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	make several observations cause errors.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	errors include NE tagging errors errors due phrase includes NEs.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Also cases one two NEs belong phrase outside relation.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	example, sentence “Mr.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	return issues discussion section.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	links “CC-domain shown Step 4 subsection 3.2.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	company buys another company, paying event occur, two phrases indicate event.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	similar explanation applies link “stake” set.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	checked whether discovered links listed WordNet.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	result suggests benefit using automatic discovery method.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Evaluation results links	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	work reported closely related [Ha- segawa et al. 04].	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	First, describe method compare method.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	first collect NE instance pairs contexts, like method.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	However, next step clearly different.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	cluster NE instance pairs based words contexts using bag- of-words method.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	threshold, NE instance pairs could used hence variety phrases also limited.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	number NE instance pairs used experiment less half method.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	kinds efforts discover paraphrase automatically corpora.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	availability comparable corpora limited, significant limitation approach.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	repeated several times collect list author / book title pairs expressions.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	However, methods need initial seeds, relation entities known advance.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	limitation obstacle making technology “open domain”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	However, desirable separate them.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	problem arises keywords consist one word.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	One possibility use n-grams based mutual information.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	frequent multi-word sequence domain, could use keyword candidate.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	explained results section, “strength” “add” desirable keywords CC-domain.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Also, “agree” CC-domain desirable keyword.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	relatively frequent word domain, used different extraction scenarios.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	checked similar verbs major domains, one.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Chunking enough find relationships.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	remains future work.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Limitations several limitations methods.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	phrases expressions length less 5 chunks, appear two NEs.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Also, method using keywords rules phrases don’t contain popular words domain.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	claiming method almighty.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Applications discovered paraphrases multiple applications.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	One obvious application information extraction.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	proposed unsupervised method discover paraphrases large untagged corpus.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	accuracies link 73% 86% two evaluated domains.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	results promising several avenues improving results.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	paper necessarily reflect position U.S. Government.	0
Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Automatic paraphrase discovery important challenging task.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	first stage identifies keyword phrase joins phrases keyword sets.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	second stage links sets involve pairs individual NEs.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases grouped.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	One difficulties Natural Language Processing fact many ways express thing event.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	lot research lexical relations, along creation resources WordNet.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	order create IE system new domain, one spend long time create knowledge.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	2.1 Overview.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	explaining method detail, present brief overview subsection.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	First, large corpus, extract NE instance pairs.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	pair also record context, i.e. phrase two NEs (Step1).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	use simple TF/IDF method measure topicality words.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	shall see, linked sets paraphrases.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	overview illustrated Figure 1.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	sentences corpus tagged transformation-based chunker NE tagger.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Figure 2 shows examples extracted NE pair instances contexts.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	used TF/ITF metric identify keywords.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Figure 3 Figure 1.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Overview method 2.2 Step Step Algorithm.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	section, explain algorithm step step examples.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	size, examples (Figures 2 4) appear end paper.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	shows keywords scores.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	(If TF/IDF score word threshold, phrase discarded.)	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	gather phrases keyword.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Figure 4 shows phrase sets based keywords CC-domain.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Cluster phrases based Links set phrases share keyword.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	However, phrases express meanings even though share keyword.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	step, try link sets, put single cluster.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	clue NE instance pairs.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	pair NE instances used different phrases, phrases likely paraphrases.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Here, “EG” represents “Eastern Group Plc”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	“H” represents “Hanson Plc”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	So, set threshold least two examples required build link.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	examples shown Figure 5.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Notice CC-domain special case.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	links solve problem.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	figure 4, reverse relations indicated `*’ next frequency.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	sets phrases share keyword links sets.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	3.1 Corpora.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	sentences analyzed chunker NE tag- ger.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	3.2 Results.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	subsection, report results experiment, terms number words, phrases clusters.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	report evaluation results next subsection.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Step 1.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	frequency Company – Company domain ranks 11th 35,567 examples.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair keywords found NE category pair.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	natural larger data domain, keywords found.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	total, 2,000 NE category pairs, 5,184 keywords found.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	phrase contain keywords, phrase discarded.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	total, across domains, kept 13,976 phrases keywords.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	CC-domain, 32 sets phrases contain 2 phrases.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	concentrate sets.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Among 32 sets, found following pairs sets two links.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	set represented keyword number parentheses indicates number shared NE pair instances.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	describe evaluation clusters next subsection.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	3.3 Evaluation Results.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	evaluated results based two metrics.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	One accuracy within set phrases share keyword; accuracy links.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	easy make clear definition “paraphrase”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Although precise criterion, cases evaluated relatively clear-cut.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	set, phrases bracketed frequencies considered paraphrases set.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	accuracy calculated ratio number paraphrases total number phrases set.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	results, along total number phrases, shown Table 1.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	larger sets accurate small sets.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	make several observations cause errors.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	errors include NE tagging errors errors due phrase includes NEs.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Also cases one two NEs belong phrase outside relation.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	example, sentence “Mr.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	return issues discussion section.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	links “CC-domain shown Step 4 subsection 3.2.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	company buys another company, paying event occur, two phrases indicate event.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	similar explanation applies link “stake” set.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	checked whether discovered links listed WordNet.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	result suggests benefit using automatic discovery method.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Evaluation results links	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	work reported closely related [Ha- segawa et al. 04].	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	First, describe method compare method.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	first collect NE instance pairs contexts, like method.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	However, next step clearly different.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	cluster NE instance pairs based words contexts using bag- of-words method.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	threshold, NE instance pairs could used hence variety phrases also limited.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	number NE instance pairs used experiment less half method.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	kinds efforts discover paraphrase automatically corpora.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	availability comparable corpora limited, significant limitation approach.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	repeated several times collect list author / book title pairs expressions.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	However, methods need initial seeds, relation entities known advance.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	limitation obstacle making technology “open domain”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	However, desirable separate them.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	problem arises keywords consist one word.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	One possibility use n-grams based mutual information.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	frequent multi-word sequence domain, could use keyword candidate.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	explained results section, “strength” “add” desirable keywords CC-domain.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Also, “agree” CC-domain desirable keyword.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	relatively frequent word domain, used different extraction scenarios.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	checked similar verbs major domains, one.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Chunking enough find relationships.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	remains future work.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Limitations several limitations methods.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	phrases expressions length less 5 chunks, appear two NEs.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Also, method using keywords rules phrases don’t contain popular words domain.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	claiming method almighty.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Applications discovered paraphrases multiple applications.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	One obvious application information extraction.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	proposed unsupervised method discover paraphrases large untagged corpus.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	accuracies link 73% 86% two evaluated domains.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	results promising several avenues improving results.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	paper necessarily reflect position U.S. Government.	0
Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	1
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Automatic paraphrase discovery important challenging task.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	first stage identifies keyword phrase joins phrases keyword sets.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	second stage links sets involve pairs individual NEs.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases grouped.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	One difficulties Natural Language Processing fact many ways express thing event.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	lot research lexical relations, along creation resources WordNet.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	order create IE system new domain, one spend long time create knowledge.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	2.1 Overview.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	explaining method detail, present brief overview subsection.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	First, large corpus, extract NE instance pairs.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	pair also record context, i.e. phrase two NEs (Step1).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	use simple TF/IDF method measure topicality words.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	shall see, linked sets paraphrases.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	overview illustrated Figure 1.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	sentences corpus tagged transformation-based chunker NE tagger.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Figure 2 shows examples extracted NE pair instances contexts.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	used TF/ITF metric identify keywords.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Figure 3 Figure 1.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Overview method 2.2 Step Step Algorithm.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	section, explain algorithm step step examples.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	size, examples (Figures 2 4) appear end paper.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	shows keywords scores.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	(If TF/IDF score word threshold, phrase discarded.)	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	gather phrases keyword.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Figure 4 shows phrase sets based keywords CC-domain.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Cluster phrases based Links set phrases share keyword.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	However, phrases express meanings even though share keyword.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	step, try link sets, put single cluster.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	clue NE instance pairs.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	pair NE instances used different phrases, phrases likely paraphrases.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Here, “EG” represents “Eastern Group Plc”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	“H” represents “Hanson Plc”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	So, set threshold least two examples required build link.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	examples shown Figure 5.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Notice CC-domain special case.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	links solve problem.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	figure 4, reverse relations indicated `*’ next frequency.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	sets phrases share keyword links sets.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	3.1 Corpora.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	sentences analyzed chunker NE tag- ger.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	3.2 Results.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	subsection, report results experiment, terms number words, phrases clusters.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	report evaluation results next subsection.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Step 1.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	frequency Company – Company domain ranks 11th 35,567 examples.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Step 2.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Find keywords NE pair keywords found NE category pair.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	natural larger data domain, keywords found.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	total, 2,000 NE category pairs, 5,184 keywords found.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Step 3.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	phrase contain keywords, phrase discarded.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	total, across domains, kept 13,976 phrases keywords.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Step 4.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	CC-domain, 32 sets phrases contain 2 phrases.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	concentrate sets.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Among 32 sets, found following pairs sets two links.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	set represented keyword number parentheses indicates number shared NE pair instances.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	describe evaluation clusters next subsection.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	3.3 Evaluation Results.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	evaluated results based two metrics.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	One accuracy within set phrases share keyword; accuracy links.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	easy make clear definition “paraphrase”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Although precise criterion, cases evaluated relatively clear-cut.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	set, phrases bracketed frequencies considered paraphrases set.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	accuracy calculated ratio number paraphrases total number phrases set.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	results, along total number phrases, shown Table 1.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	larger sets accurate small sets.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	make several observations cause errors.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	errors include NE tagging errors errors due phrase includes NEs.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Also cases one two NEs belong phrase outside relation.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	example, sentence “Mr.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	return issues discussion section.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	links “CC-domain shown Step 4 subsection 3.2.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	company buys another company, paying event occur, two phrases indicate event.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	similar explanation applies link “stake” set.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	checked whether discovered links listed WordNet.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	result suggests benefit using automatic discovery method.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Evaluation results links	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	work reported closely related [Ha- segawa et al. 04].	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	First, describe method compare method.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	first collect NE instance pairs contexts, like method.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	However, next step clearly different.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	cluster NE instance pairs based words contexts using bag- of-words method.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	threshold, NE instance pairs could used hence variety phrases also limited.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	number NE instance pairs used experiment less half method.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	kinds efforts discover paraphrase automatically corpora.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	availability comparable corpora limited, significant limitation approach.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	repeated several times collect list author / book title pairs expressions.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	However, methods need initial seeds, relation entities known advance.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	limitation obstacle making technology “open domain”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	However, desirable separate them.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	problem arises keywords consist one word.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	One possibility use n-grams based mutual information.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	frequent multi-word sequence domain, could use keyword candidate.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	explained results section, “strength” “add” desirable keywords CC-domain.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Also, “agree” CC-domain desirable keyword.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	relatively frequent word domain, used different extraction scenarios.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	checked similar verbs major domains, one.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Smith estimates Lotus make profit quarter…”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Chunking enough find relationships.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	remains future work.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Limitations several limitations methods.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	phrases expressions length less 5 chunks, appear two NEs.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Also, method using keywords rules phrases don’t contain popular words domain.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	claiming method almighty.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Applications discovered paraphrases multiple applications.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	One obvious application information extraction.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	proposed unsupervised method discover paraphrases large untagged corpus.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	accuracies link 73% 86% two evaluated domains.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	results promising several avenues improving results.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	paper necessarily reflect position U.S. Government.	0
Recently, several algorithms were proposed for automatically learning entailment rules and paraphrases (viewed as bidirectional entailment rules) (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Szpektor et al., 2004; Sekine, 2005).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Automatic paraphrase discovery important challenging task.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	first stage identifies keyword phrase joins phrases keyword sets.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	second stage links sets involve pairs individual NEs.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	total 13,976 phrases grouped.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	One difficulties Natural Language Processing fact many ways express thing event.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	lot research lexical relations, along creation resources WordNet.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	order create IE system new domain, one spend long time create knowledge.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	2.1 Overview.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	explaining method detail, present brief overview subsection.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	First, large corpus, extract NE instance pairs.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	pair also record context, i.e. phrase two NEs (Step1).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	use simple TF/IDF method measure topicality words.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	shall see, linked sets paraphrases.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	overview illustrated Figure 1.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	sentences corpus tagged transformation-based chunker NE tagger.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Figure 2 shows examples extracted NE pair instances contexts.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Step 2.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	used TF/ITF metric identify keywords.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Figure 3 Figure 1.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Overview method 2.2 Step Step Algorithm.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	section, explain algorithm step step examples.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	size, examples (Figures 2 4) appear end paper.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	shows keywords scores.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Step 3.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	(If TF/IDF score word threshold, phrase discarded.)	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	gather phrases keyword.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Figure 4 shows phrase sets based keywords CC-domain.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Step 4.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Cluster phrases based Links set phrases share keyword.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	However, phrases express meanings even though share keyword.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	step, try link sets, put single cluster.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	clue NE instance pairs.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	pair NE instances used different phrases, phrases likely paraphrases.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Here, “EG” represents “Eastern Group Plc”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	“H” represents “Hanson Plc”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	So, set threshold least two examples required build link.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	examples shown Figure 5.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Notice CC-domain special case.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	links solve problem.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	figure 4, reverse relations indicated `*’ next frequency.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	sets phrases share keyword links sets.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	3.1 Corpora.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	sentences analyzed chunker NE tag- ger.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	3.2 Results.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	subsection, report results experiment, terms number words, phrases clusters.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	report evaluation results next subsection.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Step 1.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	frequency Company – Company domain ranks 11th 35,567 examples.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Step 2.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Find keywords NE pair keywords found NE category pair.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	natural larger data domain, keywords found.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	total, 2,000 NE category pairs, 5,184 keywords found.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Step 3.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	phrase contain keywords, phrase discarded.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	total, across domains, kept 13,976 phrases keywords.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Step 4.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	CC-domain, 32 sets phrases contain 2 phrases.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	concentrate sets.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Among 32 sets, found following pairs sets two links.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	set represented keyword number parentheses indicates number shared NE pair instances.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	describe evaluation clusters next subsection.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	3.3 Evaluation Results.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	evaluated results based two metrics.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	One accuracy within set phrases share keyword; accuracy links.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	easy make clear definition “paraphrase”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Although precise criterion, cases evaluated relatively clear-cut.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	set, phrases bracketed frequencies considered paraphrases set.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	accuracy calculated ratio number paraphrases total number phrases set.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	results, along total number phrases, shown Table 1.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	larger sets accurate small sets.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	make several observations cause errors.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	errors include NE tagging errors errors due phrase includes NEs.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Also cases one two NEs belong phrase outside relation.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	example, sentence “Mr.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	return issues discussion section.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	links “CC-domain shown Step 4 subsection 3.2.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	company buys another company, paying event occur, two phrases indicate event.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	similar explanation applies link “stake” set.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	checked whether discovered links listed WordNet.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	result suggests benefit using automatic discovery method.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Evaluation results links	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	work reported closely related [Ha- segawa et al. 04].	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	First, describe method compare method.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	first collect NE instance pairs contexts, like method.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	However, next step clearly different.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	cluster NE instance pairs based words contexts using bag- of-words method.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	threshold, NE instance pairs could used hence variety phrases also limited.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	number NE instance pairs used experiment less half method.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	kinds efforts discover paraphrase automatically corpora.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	availability comparable corpora limited, significant limitation approach.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	repeated several times collect list author / book title pairs expressions.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	However, methods need initial seeds, relation entities known advance.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	limitation obstacle making technology “open domain”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	However, desirable separate them.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	problem arises keywords consist one word.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	One possibility use n-grams based mutual information.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	frequent multi-word sequence domain, could use keyword candidate.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	explained results section, “strength” “add” desirable keywords CC-domain.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Also, “agree” CC-domain desirable keyword.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	relatively frequent word domain, used different extraction scenarios.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	checked similar verbs major domains, one.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Smith estimates Lotus make profit quarter…”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Chunking enough find relationships.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	remains future work.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Limitations several limitations methods.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	phrases expressions length less 5 chunks, appear two NEs.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Also, method using keywords rules phrases don’t contain popular words domain.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	claiming method almighty.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Applications discovered paraphrases multiple applications.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	One obvious application information extraction.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	proposed unsupervised method discover paraphrases large untagged corpus.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	accuracies link 73% 86% two evaluated domains.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	results promising several avenues improving results.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	paper necessarily reflect position U.S. Government.	0
Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	1
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Automatic paraphrase discovery important challenging task.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	first stage identifies keyword phrase joins phrases keyword sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	second stage links sets involve pairs individual NEs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	total 13,976 phrases grouped.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	One difficulties Natural Language Processing fact many ways express thing event.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	lot research lexical relations, along creation resources WordNet.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	order create IE system new domain, one spend long time create knowledge.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	2.1 Overview.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	explaining method detail, present brief overview subsection.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	First, large corpus, extract NE instance pairs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	pair also record context, i.e. phrase two NEs (Step1).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	use simple TF/IDF method measure topicality words.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	shall see, linked sets paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	overview illustrated Figure 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	sentences corpus tagged transformation-based chunker NE tagger.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Figure 2 shows examples extracted NE pair instances contexts.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Step 2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	used TF/ITF metric identify keywords.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Figure 3 Figure 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Overview method 2.2 Step Step Algorithm.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	section, explain algorithm step step examples.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	size, examples (Figures 2 4) appear end paper.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	shows keywords scores.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Step 3.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	(If TF/IDF score word threshold, phrase discarded.)	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	gather phrases keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Figure 4 shows phrase sets based keywords CC-domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Step 4.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Cluster phrases based Links set phrases share keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	However, phrases express meanings even though share keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	step, try link sets, put single cluster.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	clue NE instance pairs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	pair NE instances used different phrases, phrases likely paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Here, “EG” represents “Eastern Group Plc”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	“H” represents “Hanson Plc”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	So, set threshold least two examples required build link.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	examples shown Figure 5.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Notice CC-domain special case.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	links solve problem.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	figure 4, reverse relations indicated `*’ next frequency.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	sets phrases share keyword links sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	3.1 Corpora.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	sentences analyzed chunker NE tag- ger.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	3.2 Results.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	subsection, report results experiment, terms number words, phrases clusters.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	report evaluation results next subsection.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Step 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	frequency Company – Company domain ranks 11th 35,567 examples.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Step 2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Find keywords NE pair keywords found NE category pair.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	natural larger data domain, keywords found.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	total, 2,000 NE category pairs, 5,184 keywords found.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Step 3.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	phrase contain keywords, phrase discarded.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	total, across domains, kept 13,976 phrases keywords.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Step 4.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	CC-domain, 32 sets phrases contain 2 phrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	concentrate sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Among 32 sets, found following pairs sets two links.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	set represented keyword number parentheses indicates number shared NE pair instances.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	describe evaluation clusters next subsection.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	3.3 Evaluation Results.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	evaluated results based two metrics.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	One accuracy within set phrases share keyword; accuracy links.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	easy make clear definition “paraphrase”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Although precise criterion, cases evaluated relatively clear-cut.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	set, phrases bracketed frequencies considered paraphrases set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	accuracy calculated ratio number paraphrases total number phrases set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	results, along total number phrases, shown Table 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	larger sets accurate small sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	make several observations cause errors.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	errors include NE tagging errors errors due phrase includes NEs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Also cases one two NEs belong phrase outside relation.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	example, sentence “Mr.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	return issues discussion section.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	links “CC-domain shown Step 4 subsection 3.2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	company buys another company, paying event occur, two phrases indicate event.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	similar explanation applies link “stake” set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	checked whether discovered links listed WordNet.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	result suggests benefit using automatic discovery method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Evaluation results links	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	work reported closely related [Ha- segawa et al. 04].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	First, describe method compare method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	first collect NE instance pairs contexts, like method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	However, next step clearly different.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	cluster NE instance pairs based words contexts using bag- of-words method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	threshold, NE instance pairs could used hence variety phrases also limited.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	number NE instance pairs used experiment less half method.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	kinds efforts discover paraphrase automatically corpora.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	availability comparable corpora limited, significant limitation approach.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	repeated several times collect list author / book title pairs expressions.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	However, methods need initial seeds, relation entities known advance.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	limitation obstacle making technology “open domain”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	However, desirable separate them.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	problem arises keywords consist one word.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	One possibility use n-grams based mutual information.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	frequent multi-word sequence domain, could use keyword candidate.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	explained results section, “strength” “add” desirable keywords CC-domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Also, “agree” CC-domain desirable keyword.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	relatively frequent word domain, used different extraction scenarios.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	checked similar verbs major domains, one.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Smith estimates Lotus make profit quarter…”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Chunking enough find relationships.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	remains future work.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Limitations several limitations methods.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	phrases expressions length less 5 chunks, appear two NEs.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Also, method using keywords rules phrases don’t contain popular words domain.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	claiming method almighty.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Applications discovered paraphrases multiple applications.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	One obvious application information extraction.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	proposed unsupervised method discover paraphrases large untagged corpus.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	accuracies link 73% 86% two evaluated domains.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	results promising several avenues improving results.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	paper necessarily reflect position U.S. Government.	0
This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g.  (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	1
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Automatic paraphrase discovery important challenging task.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	first stage identifies keyword phrase joins phrases keyword sets.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	second stage links sets involve pairs individual NEs.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	total 13,976 phrases grouped.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	One difficulties Natural Language Processing fact many ways express thing event.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	lot research lexical relations, along creation resources WordNet.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	order create IE system new domain, one spend long time create knowledge.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	2.1 Overview.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	explaining method detail, present brief overview subsection.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	First, large corpus, extract NE instance pairs.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	pair also record context, i.e. phrase two NEs (Step1).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	use simple TF/IDF method measure topicality words.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	shall see, linked sets paraphrases.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	overview illustrated Figure 1.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	sentences corpus tagged transformation-based chunker NE tagger.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Figure 2 shows examples extracted NE pair instances contexts.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Step 2.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	used TF/ITF metric identify keywords.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Figure 3 Figure 1.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Overview method 2.2 Step Step Algorithm.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	section, explain algorithm step step examples.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	size, examples (Figures 2 4) appear end paper.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	shows keywords scores.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Step 3.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	(If TF/IDF score word threshold, phrase discarded.)	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	gather phrases keyword.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Figure 4 shows phrase sets based keywords CC-domain.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Step 4.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Cluster phrases based Links set phrases share keyword.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	However, phrases express meanings even though share keyword.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	step, try link sets, put single cluster.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	clue NE instance pairs.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	pair NE instances used different phrases, phrases likely paraphrases.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Here, “EG” represents “Eastern Group Plc”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	“H” represents “Hanson Plc”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	So, set threshold least two examples required build link.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	examples shown Figure 5.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Notice CC-domain special case.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	links solve problem.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	figure 4, reverse relations indicated `*’ next frequency.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	sets phrases share keyword links sets.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	3.1 Corpora.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	sentences analyzed chunker NE tag- ger.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	3.2 Results.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	subsection, report results experiment, terms number words, phrases clusters.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	report evaluation results next subsection.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Step 1.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	frequency Company – Company domain ranks 11th 35,567 examples.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Step 2.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Find keywords NE pair keywords found NE category pair.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	natural larger data domain, keywords found.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	total, 2,000 NE category pairs, 5,184 keywords found.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Step 3.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	phrase contain keywords, phrase discarded.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	total, across domains, kept 13,976 phrases keywords.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Step 4.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	CC-domain, 32 sets phrases contain 2 phrases.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	concentrate sets.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Among 32 sets, found following pairs sets two links.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	set represented keyword number parentheses indicates number shared NE pair instances.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	describe evaluation clusters next subsection.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	3.3 Evaluation Results.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	evaluated results based two metrics.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	One accuracy within set phrases share keyword; accuracy links.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	easy make clear definition “paraphrase”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Although precise criterion, cases evaluated relatively clear-cut.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	set, phrases bracketed frequencies considered paraphrases set.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	accuracy calculated ratio number paraphrases total number phrases set.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	results, along total number phrases, shown Table 1.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	larger sets accurate small sets.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	make several observations cause errors.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	errors include NE tagging errors errors due phrase includes NEs.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Also cases one two NEs belong phrase outside relation.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	example, sentence “Mr.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	return issues discussion section.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	links “CC-domain shown Step 4 subsection 3.2.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	company buys another company, paying event occur, two phrases indicate event.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	similar explanation applies link “stake” set.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	checked whether discovered links listed WordNet.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	result suggests benefit using automatic discovery method.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Evaluation results links	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	work reported closely related [Ha- segawa et al. 04].	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	First, describe method compare method.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	first collect NE instance pairs contexts, like method.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	However, next step clearly different.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	cluster NE instance pairs based words contexts using bag- of-words method.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	threshold, NE instance pairs could used hence variety phrases also limited.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	number NE instance pairs used experiment less half method.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	kinds efforts discover paraphrase automatically corpora.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	availability comparable corpora limited, significant limitation approach.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	repeated several times collect list author / book title pairs expressions.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	However, methods need initial seeds, relation entities known advance.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	limitation obstacle making technology “open domain”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	However, desirable separate them.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	problem arises keywords consist one word.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	One possibility use n-grams based mutual information.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	frequent multi-word sequence domain, could use keyword candidate.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	explained results section, “strength” “add” desirable keywords CC-domain.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Also, “agree” CC-domain desirable keyword.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	relatively frequent word domain, used different extraction scenarios.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	checked similar verbs major domains, one.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Smith estimates Lotus make profit quarter…”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Chunking enough find relationships.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	remains future work.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Limitations several limitations methods.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	phrases expressions length less 5 chunks, appear two NEs.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Also, method using keywords rules phrases don’t contain popular words domain.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	claiming method almighty.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Applications discovered paraphrases multiple applications.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	One obvious application information extraction.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	proposed unsupervised method discover paraphrases large untagged corpus.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	accuracies link 73% 86% two evaluated domains.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	results promising several avenues improving results.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	paper necessarily reflect position U.S. Government.	0
Thus, acquisition of such knowledge received considerable attention in the last decade (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2009; Schoenmackers et al., 2010).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Automatic paraphrase discovery important challenging task.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	first stage identifies keyword phrase joins phrases keyword sets.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	second stage links sets involve pairs individual NEs.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	total 13,976 phrases grouped.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	One difficulties Natural Language Processing fact many ways express thing event.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	lot research lexical relations, along creation resources WordNet.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	order create IE system new domain, one spend long time create knowledge.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	2.1 Overview.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	explaining method detail, present brief overview subsection.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	First, large corpus, extract NE instance pairs.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	pair also record context, i.e. phrase two NEs (Step1).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	use simple TF/IDF method measure topicality words.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	shall see, linked sets paraphrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	overview illustrated Figure 1.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	sentences corpus tagged transformation-based chunker NE tagger.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Figure 2 shows examples extracted NE pair instances contexts.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Step 2.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	used TF/ITF metric identify keywords.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Figure 3 Figure 1.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Overview method 2.2 Step Step Algorithm.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	section, explain algorithm step step examples.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	size, examples (Figures 2 4) appear end paper.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	shows keywords scores.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Step 3.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	(If TF/IDF score word threshold, phrase discarded.)	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	gather phrases keyword.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Figure 4 shows phrase sets based keywords CC-domain.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Step 4.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Cluster phrases based Links set phrases share keyword.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	However, phrases express meanings even though share keyword.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	step, try link sets, put single cluster.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	clue NE instance pairs.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	pair NE instances used different phrases, phrases likely paraphrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Here, “EG” represents “Eastern Group Plc”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	“H” represents “Hanson Plc”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	So, set threshold least two examples required build link.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	examples shown Figure 5.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Notice CC-domain special case.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	links solve problem.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	figure 4, reverse relations indicated `*’ next frequency.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	sets phrases share keyword links sets.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	3.1 Corpora.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	sentences analyzed chunker NE tag- ger.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	3.2 Results.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	subsection, report results experiment, terms number words, phrases clusters.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	report evaluation results next subsection.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Step 1.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	frequency Company – Company domain ranks 11th 35,567 examples.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Step 2.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Find keywords NE pair keywords found NE category pair.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	natural larger data domain, keywords found.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	total, 2,000 NE category pairs, 5,184 keywords found.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Step 3.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	phrase contain keywords, phrase discarded.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	total, across domains, kept 13,976 phrases keywords.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Step 4.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	CC-domain, 32 sets phrases contain 2 phrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	concentrate sets.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Among 32 sets, found following pairs sets two links.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	set represented keyword number parentheses indicates number shared NE pair instances.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	describe evaluation clusters next subsection.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	3.3 Evaluation Results.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	evaluated results based two metrics.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	One accuracy within set phrases share keyword; accuracy links.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	easy make clear definition “paraphrase”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Although precise criterion, cases evaluated relatively clear-cut.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	set, phrases bracketed frequencies considered paraphrases set.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	accuracy calculated ratio number paraphrases total number phrases set.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	results, along total number phrases, shown Table 1.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	larger sets accurate small sets.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	make several observations cause errors.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	errors include NE tagging errors errors due phrase includes NEs.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Also cases one two NEs belong phrase outside relation.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	example, sentence “Mr.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	return issues discussion section.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	links “CC-domain shown Step 4 subsection 3.2.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	company buys another company, paying event occur, two phrases indicate event.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	similar explanation applies link “stake” set.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	checked whether discovered links listed WordNet.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	result suggests benefit using automatic discovery method.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Evaluation results links	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	work reported closely related [Ha- segawa et al. 04].	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	First, describe method compare method.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	first collect NE instance pairs contexts, like method.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	However, next step clearly different.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	cluster NE instance pairs based words contexts using bag- of-words method.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	threshold, NE instance pairs could used hence variety phrases also limited.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	number NE instance pairs used experiment less half method.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	kinds efforts discover paraphrase automatically corpora.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	availability comparable corpora limited, significant limitation approach.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	repeated several times collect list author / book title pairs expressions.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	However, methods need initial seeds, relation entities known advance.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	limitation obstacle making technology “open domain”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	However, desirable separate them.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	problem arises keywords consist one word.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	One possibility use n-grams based mutual information.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	frequent multi-word sequence domain, could use keyword candidate.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	explained results section, “strength” “add” desirable keywords CC-domain.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Also, “agree” CC-domain desirable keyword.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	relatively frequent word domain, used different extraction scenarios.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	checked similar verbs major domains, one.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Smith estimates Lotus make profit quarter…”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Chunking enough find relationships.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	remains future work.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Limitations several limitations methods.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	phrases expressions length less 5 chunks, appear two NEs.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Also, method using keywords rules phrases don’t contain popular words domain.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	claiming method almighty.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	1
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Applications discovered paraphrases multiple applications.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	One obvious application information extraction.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	proposed unsupervised method discover paraphrases large untagged corpus.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	accuracies link 73% 86% two evaluated domains.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	results promising several avenues improving results.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	paper necessarily reflect position U.S. Government.	0
We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Automatic paraphrase discovery important challenging task.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	first stage identifies keyword phrase joins phrases keyword sets.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	second stage links sets involve pairs individual NEs.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	total 13,976 phrases grouped.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	One difficulties Natural Language Processing fact many ways express thing event.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	lot research lexical relations, along creation resources WordNet.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	order create IE system new domain, one spend long time create knowledge.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	2.1 Overview.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	explaining method detail, present brief overview subsection.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	First, large corpus, extract NE instance pairs.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	pair also record context, i.e. phrase two NEs (Step1).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	use simple TF/IDF method measure topicality words.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	shall see, linked sets paraphrases.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	overview illustrated Figure 1.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	sentences corpus tagged transformation-based chunker NE tagger.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Figure 2 shows examples extracted NE pair instances contexts.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Step 2.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	used TF/ITF metric identify keywords.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Figure 3 Figure 1.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Overview method 2.2 Step Step Algorithm.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	section, explain algorithm step step examples.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	size, examples (Figures 2 4) appear end paper.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	shows keywords scores.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Step 3.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	(If TF/IDF score word threshold, phrase discarded.)	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	gather phrases keyword.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Figure 4 shows phrase sets based keywords CC-domain.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Step 4.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Cluster phrases based Links set phrases share keyword.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	However, phrases express meanings even though share keyword.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	step, try link sets, put single cluster.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	clue NE instance pairs.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	pair NE instances used different phrases, phrases likely paraphrases.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Here, “EG” represents “Eastern Group Plc”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	“H” represents “Hanson Plc”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	So, set threshold least two examples required build link.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	examples shown Figure 5.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Notice CC-domain special case.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	links solve problem.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	figure 4, reverse relations indicated `*’ next frequency.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	sets phrases share keyword links sets.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	3.1 Corpora.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	sentences analyzed chunker NE tag- ger.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	3.2 Results.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	subsection, report results experiment, terms number words, phrases clusters.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	report evaluation results next subsection.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Step 1.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	frequency Company – Company domain ranks 11th 35,567 examples.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Step 2.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Find keywords NE pair keywords found NE category pair.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	natural larger data domain, keywords found.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	total, 2,000 NE category pairs, 5,184 keywords found.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Step 3.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	phrase contain keywords, phrase discarded.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	total, across domains, kept 13,976 phrases keywords.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Step 4.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	CC-domain, 32 sets phrases contain 2 phrases.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	concentrate sets.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Among 32 sets, found following pairs sets two links.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	set represented keyword number parentheses indicates number shared NE pair instances.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	describe evaluation clusters next subsection.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	3.3 Evaluation Results.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	evaluated results based two metrics.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	One accuracy within set phrases share keyword; accuracy links.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	easy make clear definition “paraphrase”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Although precise criterion, cases evaluated relatively clear-cut.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	set, phrases bracketed frequencies considered paraphrases set.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	accuracy calculated ratio number paraphrases total number phrases set.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	results, along total number phrases, shown Table 1.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	larger sets accurate small sets.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	make several observations cause errors.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	errors include NE tagging errors errors due phrase includes NEs.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Also cases one two NEs belong phrase outside relation.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	example, sentence “Mr.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	return issues discussion section.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	links “CC-domain shown Step 4 subsection 3.2.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	company buys another company, paying event occur, two phrases indicate event.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	similar explanation applies link “stake” set.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	checked whether discovered links listed WordNet.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	result suggests benefit using automatic discovery method.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Evaluation results links	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	work reported closely related [Ha- segawa et al. 04].	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	First, describe method compare method.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	first collect NE instance pairs contexts, like method.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	However, next step clearly different.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	cluster NE instance pairs based words contexts using bag- of-words method.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	threshold, NE instance pairs could used hence variety phrases also limited.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	number NE instance pairs used experiment less half method.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	kinds efforts discover paraphrase automatically corpora.	1
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	availability comparable corpora limited, significant limitation approach.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	repeated several times collect list author / book title pairs expressions.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	However, methods need initial seeds, relation entities known advance.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	limitation obstacle making technology “open domain”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	However, desirable separate them.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	problem arises keywords consist one word.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	One possibility use n-grams based mutual information.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	frequent multi-word sequence domain, could use keyword candidate.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	explained results section, “strength” “add” desirable keywords CC-domain.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Also, “agree” CC-domain desirable keyword.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	relatively frequent word domain, used different extraction scenarios.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	checked similar verbs major domains, one.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Smith estimates Lotus make profit quarter…”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Chunking enough find relationships.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	remains future work.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Limitations several limitations methods.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	phrases expressions length less 5 chunks, appear two NEs.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Also, method using keywords rules phrases don’t contain popular words domain.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	claiming method almighty.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Applications discovered paraphrases multiple applications.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	One obvious application information extraction.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	proposed unsupervised method discover paraphrases large untagged corpus.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	accuracies link 73% 86% two evaluated domains.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	results promising several avenues improving results.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	paper necessarily reflect position U.S. Government.	0
Consequently, sub stantial effort has been made to learn such rules (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008; Schoenmackers et al., 2010).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Automatic paraphrase discovery important challenging task.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	first stage identifies keyword phrase joins phrases keyword sets.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	second stage links sets involve pairs individual NEs.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	total 13,976 phrases grouped.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	One difficulties Natural Language Processing fact many ways express thing event.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	lot research lexical relations, along creation resources WordNet.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	order create IE system new domain, one spend long time create knowledge.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	2.1 Overview.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	explaining method detail, present brief overview subsection.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	First, large corpus, extract NE instance pairs.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	pair also record context, i.e. phrase two NEs (Step1).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	use simple TF/IDF method measure topicality words.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	shall see, linked sets paraphrases.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	overview illustrated Figure 1.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	sentences corpus tagged transformation-based chunker NE tagger.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Figure 2 shows examples extracted NE pair instances contexts.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Step 2.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	used TF/ITF metric identify keywords.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Figure 3 Figure 1.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Overview method 2.2 Step Step Algorithm.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	section, explain algorithm step step examples.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	size, examples (Figures 2 4) appear end paper.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	shows keywords scores.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Step 3.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	(If TF/IDF score word threshold, phrase discarded.)	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	gather phrases keyword.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Figure 4 shows phrase sets based keywords CC-domain.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Step 4.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Cluster phrases based Links set phrases share keyword.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	However, phrases express meanings even though share keyword.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	step, try link sets, put single cluster.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	clue NE instance pairs.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	pair NE instances used different phrases, phrases likely paraphrases.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Here, “EG” represents “Eastern Group Plc”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	“H” represents “Hanson Plc”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	So, set threshold least two examples required build link.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	examples shown Figure 5.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Notice CC-domain special case.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	links solve problem.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	figure 4, reverse relations indicated `*’ next frequency.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	sets phrases share keyword links sets.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	3.1 Corpora.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	sentences analyzed chunker NE tag- ger.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	3.2 Results.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	subsection, report results experiment, terms number words, phrases clusters.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	report evaluation results next subsection.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Step 1.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	frequency Company – Company domain ranks 11th 35,567 examples.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Step 2.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Find keywords NE pair keywords found NE category pair.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	natural larger data domain, keywords found.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	total, 2,000 NE category pairs, 5,184 keywords found.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Step 3.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	phrase contain keywords, phrase discarded.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	total, across domains, kept 13,976 phrases keywords.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Step 4.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	CC-domain, 32 sets phrases contain 2 phrases.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	concentrate sets.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Among 32 sets, found following pairs sets two links.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	set represented keyword number parentheses indicates number shared NE pair instances.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	describe evaluation clusters next subsection.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	3.3 Evaluation Results.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	evaluated results based two metrics.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	One accuracy within set phrases share keyword; accuracy links.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	easy make clear definition “paraphrase”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Although precise criterion, cases evaluated relatively clear-cut.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	set, phrases bracketed frequencies considered paraphrases set.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	accuracy calculated ratio number paraphrases total number phrases set.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	results, along total number phrases, shown Table 1.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	larger sets accurate small sets.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	make several observations cause errors.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	errors include NE tagging errors errors due phrase includes NEs.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also cases one two NEs belong phrase outside relation.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	example, sentence “Mr.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	return issues discussion section.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	links “CC-domain shown Step 4 subsection 3.2.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	company buys another company, paying event occur, two phrases indicate event.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	similar explanation applies link “stake” set.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	checked whether discovered links listed WordNet.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	result suggests benefit using automatic discovery method.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Evaluation results links	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	work reported closely related [Ha- segawa et al. 04].	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	First, describe method compare method.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	first collect NE instance pairs contexts, like method.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	However, next step clearly different.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	cluster NE instance pairs based words contexts using bag- of-words method.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	threshold, NE instance pairs could used hence variety phrases also limited.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	number NE instance pairs used experiment less half method.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	kinds efforts discover paraphrase automatically corpora.	1
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	availability comparable corpora limited, significant limitation approach.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	repeated several times collect list author / book title pairs expressions.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	However, methods need initial seeds, relation entities known advance.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	limitation obstacle making technology “open domain”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	However, desirable separate them.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	problem arises keywords consist one word.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	One possibility use n-grams based mutual information.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	frequent multi-word sequence domain, could use keyword candidate.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	explained results section, “strength” “add” desirable keywords CC-domain.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also, “agree” CC-domain desirable keyword.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	relatively frequent word domain, used different extraction scenarios.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	checked similar verbs major domains, one.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Smith estimates Lotus make profit quarter…”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Chunking enough find relationships.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	remains future work.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Limitations several limitations methods.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	phrases expressions length less 5 chunks, appear two NEs.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also, method using keywords rules phrases don’t contain popular words domain.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	claiming method almighty.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Applications discovered paraphrases multiple applications.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	One obvious application information extraction.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	proposed unsupervised method discover paraphrases large untagged corpus.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	accuracies link 73% 86% two evaluated domains.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	results promising several avenues improving results.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	paper necessarily reflect position U.S. Government.	0
The significance of inference rules has led to substantial effort into developing algorithms that automatically learn inference rules (Lin and Pantel, 2001; Sekine, 2005; Schoenmackers et al., 2010), and generate knowledge resources for inference systems.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Automatic paraphrase discovery important challenging task.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	first stage identifies keyword phrase joins phrases keyword sets.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	second stage links sets involve pairs individual NEs.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	total 13,976 phrases grouped.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	One difficulties Natural Language Processing fact many ways express thing event.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	lot research lexical relations, along creation resources WordNet.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	1
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	order create IE system new domain, one spend long time create knowledge.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	2.1 Overview.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	explaining method detail, present brief overview subsection.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	First, large corpus, extract NE instance pairs.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	pair also record context, i.e. phrase two NEs (Step1).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	use simple TF/IDF method measure topicality words.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	shall see, linked sets paraphrases.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	overview illustrated Figure 1.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	sentences corpus tagged transformation-based chunker NE tagger.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Figure 2 shows examples extracted NE pair instances contexts.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Step 2.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	used TF/ITF metric identify keywords.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Figure 3 Figure 1.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Overview method 2.2 Step Step Algorithm.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	section, explain algorithm step step examples.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	size, examples (Figures 2 4) appear end paper.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	shows keywords scores.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Step 3.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	(If TF/IDF score word threshold, phrase discarded.)	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	gather phrases keyword.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Figure 4 shows phrase sets based keywords CC-domain.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Step 4.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Cluster phrases based Links set phrases share keyword.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	However, phrases express meanings even though share keyword.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	step, try link sets, put single cluster.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	clue NE instance pairs.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	pair NE instances used different phrases, phrases likely paraphrases.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Here, “EG” represents “Eastern Group Plc”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	“H” represents “Hanson Plc”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	So, set threshold least two examples required build link.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	examples shown Figure 5.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Notice CC-domain special case.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	links solve problem.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	figure 4, reverse relations indicated `*’ next frequency.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	sets phrases share keyword links sets.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	3.1 Corpora.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	sentences analyzed chunker NE tag- ger.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	3.2 Results.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	subsection, report results experiment, terms number words, phrases clusters.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	report evaluation results next subsection.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Step 1.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	frequency Company – Company domain ranks 11th 35,567 examples.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Step 2.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Find keywords NE pair keywords found NE category pair.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	natural larger data domain, keywords found.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	total, 2,000 NE category pairs, 5,184 keywords found.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Step 3.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	phrase contain keywords, phrase discarded.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	total, across domains, kept 13,976 phrases keywords.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Step 4.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	CC-domain, 32 sets phrases contain 2 phrases.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	concentrate sets.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Among 32 sets, found following pairs sets two links.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	set represented keyword number parentheses indicates number shared NE pair instances.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	describe evaluation clusters next subsection.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	3.3 Evaluation Results.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	evaluated results based two metrics.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	One accuracy within set phrases share keyword; accuracy links.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	easy make clear definition “paraphrase”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Although precise criterion, cases evaluated relatively clear-cut.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	set, phrases bracketed frequencies considered paraphrases set.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	accuracy calculated ratio number paraphrases total number phrases set.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	results, along total number phrases, shown Table 1.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	larger sets accurate small sets.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	make several observations cause errors.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	errors include NE tagging errors errors due phrase includes NEs.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Also cases one two NEs belong phrase outside relation.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	example, sentence “Mr.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	return issues discussion section.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	links “CC-domain shown Step 4 subsection 3.2.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	company buys another company, paying event occur, two phrases indicate event.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	similar explanation applies link “stake” set.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	checked whether discovered links listed WordNet.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	result suggests benefit using automatic discovery method.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Evaluation results links	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	work reported closely related [Ha- segawa et al. 04].	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	First, describe method compare method.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	first collect NE instance pairs contexts, like method.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	However, next step clearly different.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	cluster NE instance pairs based words contexts using bag- of-words method.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	threshold, NE instance pairs could used hence variety phrases also limited.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	number NE instance pairs used experiment less half method.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	kinds efforts discover paraphrase automatically corpora.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	availability comparable corpora limited, significant limitation approach.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	repeated several times collect list author / book title pairs expressions.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	However, methods need initial seeds, relation entities known advance.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	limitation obstacle making technology “open domain”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	However, desirable separate them.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	problem arises keywords consist one word.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	One possibility use n-grams based mutual information.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	frequent multi-word sequence domain, could use keyword candidate.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	explained results section, “strength” “add” desirable keywords CC-domain.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Also, “agree” CC-domain desirable keyword.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	relatively frequent word domain, used different extraction scenarios.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	checked similar verbs major domains, one.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Smith estimates Lotus make profit quarter…”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Chunking enough find relationships.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	remains future work.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Limitations several limitations methods.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	phrases expressions length less 5 chunks, appear two NEs.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Also, method using keywords rules phrases don’t contain popular words domain.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	claiming method almighty.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Applications discovered paraphrases multiple applications.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	One obvious application information extraction.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	proposed unsupervised method discover paraphrases large untagged corpus.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	accuracies link 73% 86% two evaluated domains.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	results promising several avenues improving results.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	paper necessarily reflect position U.S. Government.	0
Some attempts were made to let annotators judge rule correctness directly, that is by asking them to judge the correctness of a given rule (Shinyama et al., 2002; Sekine, 2005).	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Automatic paraphrase discovery important challenging task.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	first stage identifies keyword phrase joins phrases keyword sets.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	second stage links sets involve pairs individual NEs.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	total 13,976 phrases grouped.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	One difficulties Natural Language Processing fact many ways express thing event.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	lot research lexical relations, along creation resources WordNet.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	order create IE system new domain, one spend long time create knowledge.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	2.1 Overview.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	explaining method detail, present brief overview subsection.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	First, large corpus, extract NE instance pairs.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	pair also record context, i.e. phrase two NEs (Step1).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	use simple TF/IDF method measure topicality words.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	shall see, linked sets paraphrases.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	overview illustrated Figure 1.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	sentences corpus tagged transformation-based chunker NE tagger.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Figure 2 shows examples extracted NE pair instances contexts.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Step 2.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	used TF/ITF metric identify keywords.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Figure 3 Figure 1.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Overview method 2.2 Step Step Algorithm.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	section, explain algorithm step step examples.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	size, examples (Figures 2 4) appear end paper.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	shows keywords scores.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Step 3.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	(If TF/IDF score word threshold, phrase discarded.)	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	gather phrases keyword.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Figure 4 shows phrase sets based keywords CC-domain.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Step 4.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Cluster phrases based Links set phrases share keyword.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	However, phrases express meanings even though share keyword.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	step, try link sets, put single cluster.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	clue NE instance pairs.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	pair NE instances used different phrases, phrases likely paraphrases.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Here, “EG” represents “Eastern Group Plc”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	“H” represents “Hanson Plc”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	So, set threshold least two examples required build link.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	examples shown Figure 5.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Notice CC-domain special case.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	links solve problem.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	figure 4, reverse relations indicated `*’ next frequency.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	sets phrases share keyword links sets.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	3.1 Corpora.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	sentences analyzed chunker NE tag- ger.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	3.2 Results.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	subsection, report results experiment, terms number words, phrases clusters.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	report evaluation results next subsection.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Step 1.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	frequency Company – Company domain ranks 11th 35,567 examples.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Step 2.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Find keywords NE pair keywords found NE category pair.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	natural larger data domain, keywords found.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	total, 2,000 NE category pairs, 5,184 keywords found.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Step 3.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	phrase contain keywords, phrase discarded.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	total, across domains, kept 13,976 phrases keywords.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Step 4.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	CC-domain, 32 sets phrases contain 2 phrases.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	concentrate sets.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Among 32 sets, found following pairs sets two links.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	set represented keyword number parentheses indicates number shared NE pair instances.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	describe evaluation clusters next subsection.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	3.3 Evaluation Results.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	evaluated results based two metrics.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	One accuracy within set phrases share keyword; accuracy links.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	easy make clear definition “paraphrase”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Although precise criterion, cases evaluated relatively clear-cut.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	set, phrases bracketed frequencies considered paraphrases set.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	accuracy calculated ratio number paraphrases total number phrases set.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	results, along total number phrases, shown Table 1.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	larger sets accurate small sets.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	make several observations cause errors.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	errors include NE tagging errors errors due phrase includes NEs.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Also cases one two NEs belong phrase outside relation.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	example, sentence “Mr.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	return issues discussion section.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	links “CC-domain shown Step 4 subsection 3.2.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	company buys another company, paying event occur, two phrases indicate event.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	similar explanation applies link “stake” set.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	checked whether discovered links listed WordNet.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	result suggests benefit using automatic discovery method.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Evaluation results links	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	work reported closely related [Ha- segawa et al. 04].	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	First, describe method compare method.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	first collect NE instance pairs contexts, like method.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	However, next step clearly different.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	cluster NE instance pairs based words contexts using bag- of-words method.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	threshold, NE instance pairs could used hence variety phrases also limited.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	number NE instance pairs used experiment less half method.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	kinds efforts discover paraphrase automatically corpora.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	availability comparable corpora limited, significant limitation approach.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	repeated several times collect list author / book title pairs expressions.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	However, methods need initial seeds, relation entities known advance.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	limitation obstacle making technology “open domain”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	However, desirable separate them.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	problem arises keywords consist one word.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	One possibility use n-grams based mutual information.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	frequent multi-word sequence domain, could use keyword candidate.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	explained results section, “strength” “add” desirable keywords CC-domain.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Also, “agree” CC-domain desirable keyword.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	relatively frequent word domain, used different extraction scenarios.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	checked similar verbs major domains, one.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Smith estimates Lotus make profit quarter…”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Chunking enough find relationships.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	remains future work.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Limitations several limitations methods.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	phrases expressions length less 5 chunks, appear two NEs.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Also, method using keywords rules phrases don’t contain popular words domain.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	claiming method almighty.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Applications discovered paraphrases multiple applications.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	One obvious application information extraction.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	proposed unsupervised method discover paraphrases large untagged corpus.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	accuracies link 73% 86% two evaluated domains.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	results promising several avenues improving results.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	paper necessarily reflect position U.S. Government.	0
Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Automatic paraphrase discovery important challenging task.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	first stage identifies keyword phrase joins phrases keyword sets.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	second stage links sets involve pairs individual NEs.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	total 13,976 phrases grouped.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	One difficulties Natural Language Processing fact many ways express thing event.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	lot research lexical relations, along creation resources WordNet.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	order create IE system new domain, one spend long time create knowledge.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	2.1 Overview.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	explaining method detail, present brief overview subsection.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	First, large corpus, extract NE instance pairs.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	pair also record context, i.e. phrase two NEs (Step1).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	use simple TF/IDF method measure topicality words.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	shall see, linked sets paraphrases.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	overview illustrated Figure 1.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	sentences corpus tagged transformation-based chunker NE tagger.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Figure 2 shows examples extracted NE pair instances contexts.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Step 2.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	used TF/ITF metric identify keywords.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Figure 3 Figure 1.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Overview method 2.2 Step Step Algorithm.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	section, explain algorithm step step examples.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	size, examples (Figures 2 4) appear end paper.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	shows keywords scores.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Step 3.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	(If TF/IDF score word threshold, phrase discarded.)	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	gather phrases keyword.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Figure 4 shows phrase sets based keywords CC-domain.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Step 4.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Cluster phrases based Links set phrases share keyword.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	However, phrases express meanings even though share keyword.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	step, try link sets, put single cluster.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	clue NE instance pairs.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	pair NE instances used different phrases, phrases likely paraphrases.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Here, “EG” represents “Eastern Group Plc”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	“H” represents “Hanson Plc”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	So, set threshold least two examples required build link.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	examples shown Figure 5.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Notice CC-domain special case.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	links solve problem.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	figure 4, reverse relations indicated `*’ next frequency.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	sets phrases share keyword links sets.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	3.1 Corpora.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	sentences analyzed chunker NE tag- ger.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	3.2 Results.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	subsection, report results experiment, terms number words, phrases clusters.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	report evaluation results next subsection.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Step 1.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	frequency Company – Company domain ranks 11th 35,567 examples.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Step 2.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Find keywords NE pair keywords found NE category pair.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	natural larger data domain, keywords found.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	total, 2,000 NE category pairs, 5,184 keywords found.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Step 3.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	phrase contain keywords, phrase discarded.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	total, across domains, kept 13,976 phrases keywords.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Step 4.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	CC-domain, 32 sets phrases contain 2 phrases.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	concentrate sets.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Among 32 sets, found following pairs sets two links.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	set represented keyword number parentheses indicates number shared NE pair instances.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	describe evaluation clusters next subsection.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	3.3 Evaluation Results.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	evaluated results based two metrics.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	One accuracy within set phrases share keyword; accuracy links.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	easy make clear definition “paraphrase”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Although precise criterion, cases evaluated relatively clear-cut.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	set, phrases bracketed frequencies considered paraphrases set.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	accuracy calculated ratio number paraphrases total number phrases set.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	results, along total number phrases, shown Table 1.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	larger sets accurate small sets.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	make several observations cause errors.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	errors include NE tagging errors errors due phrase includes NEs.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also cases one two NEs belong phrase outside relation.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	example, sentence “Mr.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	return issues discussion section.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	links “CC-domain shown Step 4 subsection 3.2.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	company buys another company, paying event occur, two phrases indicate event.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	similar explanation applies link “stake” set.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	checked whether discovered links listed WordNet.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	result suggests benefit using automatic discovery method.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Evaluation results links	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	work reported closely related [Ha- segawa et al. 04].	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	First, describe method compare method.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	first collect NE instance pairs contexts, like method.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	However, next step clearly different.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	cluster NE instance pairs based words contexts using bag- of-words method.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	threshold, NE instance pairs could used hence variety phrases also limited.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	number NE instance pairs used experiment less half method.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	kinds efforts discover paraphrase automatically corpora.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	availability comparable corpora limited, significant limitation approach.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	repeated several times collect list author / book title pairs expressions.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	However, methods need initial seeds, relation entities known advance.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	limitation obstacle making technology “open domain”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	However, desirable separate them.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	problem arises keywords consist one word.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	One possibility use n-grams based mutual information.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	frequent multi-word sequence domain, could use keyword candidate.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	explained results section, “strength” “add” desirable keywords CC-domain.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also, “agree” CC-domain desirable keyword.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	relatively frequent word domain, used different extraction scenarios.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	checked similar verbs major domains, one.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Smith estimates Lotus make profit quarter…”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Chunking enough find relationships.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	remains future work.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Limitations several limitations methods.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	phrases expressions length less 5 chunks, appear two NEs.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also, method using keywords rules phrases don’t contain popular words domain.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	claiming method almighty.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Applications discovered paraphrases multiple applications.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	One obvious application information extraction.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	proposed unsupervised method discover paraphrases large untagged corpus.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	accuracies link 73% 86% two evaluated domains.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	results promising several avenues improving results.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	paper necessarily reflect position U.S. Government.	0
Some existing paraphrase and entailment acquisition algorithms add constraints to the learned rules (e.g.  (Sekine, 2005), (CallisonBurch, 2008)), but most do not.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Automatic Paraphrase Discovery based Context Keywords NE Pairs	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Automatic paraphrase discovery important challenging task.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	propose unsupervised method discover paraphrases large untagged corpus, without requiring seed phrase cue.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	focus phrases connect two Named Entities (NEs), proceed two stages.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	first stage identifies keyword phrase joins phrases keyword sets.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	second stage links sets involve pairs individual NEs.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	total 13,976 phrases grouped.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	accuracy sets representing paraphrase ranged 73% 99%, depending NE categories set sizes; accuracy links two evaluated domains 73% 86%.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	One difficulties Natural Language Processing fact many ways express thing event.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	expression word short phrase (like “corporation” “company”), called “synonym”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	lot research lexical relations, along creation resources WordNet.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	expression longer complicated (like “A buys B” “A’s purchase B”), called “paraphrase”, i.e. set phrases express thing event.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Recently, topic getting attention, evident Paraphrase Workshops 2003 2004, driven needs various NLP applications.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	example, Information Retrieval (IR), match user’s query expressions desired documents, Question Answering (QA), find answer user’s question even formulation answer document different question.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Also, Information Extraction (IE), system tries extract elements events (e.g. date company names corporate merger event), several event instances different news articles aligned even expressed differently.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	realize importance paraphrase; however, major obstacle construction paraphrase knowledge.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	example, easily imagine number paraphrases “A buys B” enormous possible create comprehensive inventory hand.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Also, don’t know many paraphrase sets necessary cover even everyday things events.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	now, IE researchers creating paraphrase knowledge (or IE patterns) hand specific tasks.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	So, limitation IE performed predefined task, like “corporate mergers” “management succession”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	order create IE system new domain, one spend long time create knowledge.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	So, costly make IE technology “open- domain” “on-demand” like IR QA.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	paper, propose unsupervised method discover paraphrases large untagged corpus.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kinds cue.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	2.1 Overview.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	explaining method detail, present brief overview subsection.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	First, large corpus, extract NE instance pairs.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Here, NE instance pair pair NEs separated 4 syntactic chunks; example, “IBM plans acquire Lotus”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	pair also record context, i.e. phrase two NEs (Step1).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Next, pair NE categories, collect contexts find keywords topical NE category pair.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	use simple TF/IDF method measure topicality words.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Hereafter, pair NE categories called domain; e.g. “Company – Company” domain, call CC- domain (Step 2).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	domain, phrases contain keyword gathered build set phrases (Step 3).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Finally, find links sets phrases, based NE instance pair data (for example, different phrases link “IBM” “Lotus”) (Step 4).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	shall see, linked sets paraphrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	overview illustrated Figure 1.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Corpus Step 1 NE pair instances Step 2 Step 1.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Extract NE instance pairs contexts First, extract NE pair instances context corpus.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	sentences corpus tagged transformation-based chunker NE tagger.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	NE tagger rule-based system 140 NE categories [Sekine et al. 2004].	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	140 NE categories designed extending MUC’s 7 NE categories finer sub-categories (such Company, Institute, Political Party Organization; Country, Province, City Location) adding new types NE categories (Position Title, Product, Event, Natural Object).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	NE pair instances co-occur separated 4 chunks collected along information NE types phrase NEs (the ‘context’).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Figure 2 shows examples extracted NE pair instances contexts.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	data sorted based frequency context (“a unit of” appeared 314 times corpus) NE pair instances appearing context shown frequency (e.g. “NBC” “General Electric Co.” appeared 10 times context “a unit of”).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Step 2.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Find keywords NE pair look contexts domain, noticed one important words indicate relation NEs (for example, word “unit” phrase “a unit of”).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	figure important word (e.g. keyword), believe capture meaning phrase keyword.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	used TF/ITF metric identify keywords.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	keywords Step 3 Sets phrases based keywords Step 4 Links sets phrases contexts collected given domain gathered bag TF/ITF scores calculated words except stopwords bag.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Here, term frequency (TF) frequency word bag inverse term frequency (ITF) inverse log frequency entire corpus.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Figure 3 Figure 1.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Overview method 2.2 Step Step Algorithm.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	section, explain algorithm step step examples.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	size, examples (Figures 2 4) appear end paper.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	shows keywords scores.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Step 3.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Gather phrases using keywords Next, select keyword phrase – top-ranked word based TF/IDF metric.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	(If TF/IDF score word threshold, phrase discarded.)	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	gather phrases keyword.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Figure 4 shows phrase sets based keywords CC-domain.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Step 4.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Cluster phrases based Links set phrases share keyword.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	However, phrases express meanings even though share keyword.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	example, Figure 3, see phrases “buy”, “acquire” “purchase” sets mostly paraphrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	step, try link sets, put single cluster.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	clue NE instance pairs.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	pair NE instances used different phrases, phrases likely paraphrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	example, two NEs “Eastern Group Plc” “Hanson Plc” following contexts.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Here, “EG” represents “Eastern Group Plc”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	“H” represents “Hanson Plc”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	x EG, agreed bought H x EG, owned H x H acquire EG x H’s agreement buy EG Three phrases actually paraphrases, sometime could noise; second phrase above.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	So, set threshold least two examples required build link.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	examples shown Figure 5.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Notice CC-domain special case.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	two NE categories same, can’t differentiate phrases different orders par ticipants – whether buying company to-be-bought company comes first.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	links solve problem.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	seen example, first two phrases different order NE names last two, determine last two phrases represent reversed relation.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	figure 4, reverse relations indicated `*’ next frequency.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	sets phrases share keyword links sets.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	3.1 Corpora.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	experiments, used four newswire corpora, Los Angeles Times/Washington Post, New York Times, Reuters Wall Street Journal, published 1995.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	contain 200M words (25M, 110M, 40M 19M words, respectively).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	sentences analyzed chunker NE tag- ger.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	procedure using tagged sentences discover paraphrases takes one hour 2GHz Pentium 4 PC 1GB memory.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	3.2 Results.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	subsection, report results experiment, terms number words, phrases clusters.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	report evaluation results next subsection.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Step 1.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Extract NE pair instances contexts four years newspaper corpus, extracted 1.9 million pairs NE instances.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	frequent NE category pairs “Person - Person (209,236), followed “Country - Coun- try” (95,123) “Person - Country” (75,509).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	frequency Company – Company domain ranks 11th 35,567 examples.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	lower frequency examples include noise, set threshold NE category pair appear least 5 times considered NE instance pair appear least twice considered.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	limits number NE category pairs 2,000 number NE pair instances 0.63 million.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Step 2.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Find keywords NE pair keywords found NE category pair.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	example, CC-domain, 96 keywords found TF/ITF scores threshold; shown Figure 3.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	natural larger data domain, keywords found.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	“Person – Person” domain, 618 keywords found, “Country – Country” domain, 303 keywords found.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	total, 2,000 NE category pairs, 5,184 keywords found.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Step 3.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Gather phrases using keywords Now, keyword top TF/ITF score selected phrase.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	phrase contain keywords, phrase discarded.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	example, 905 phrases CC- domain, 211 phrases contain keywords found step 2.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	total, across domains, kept 13,976 phrases keywords.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Step 4.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Link phrases based instance pairs Using NE instance pairs clue, find links sets phrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	CC-domain, 32 sets phrases contain 2 phrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	concentrate sets.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Among 32 sets, found following pairs sets two links.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	set represented keyword number parentheses indicates number shared NE pair instances.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) clear links form two clusters mostly correct.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	describe evaluation clusters next subsection.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	3.3 Evaluation Results.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	evaluated results based two metrics.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	One accuracy within set phrases share keyword; accuracy links.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	picked two domains, CC-domain “Person – Company” domain (PC-domain), evaluation, entire system output large evaluate.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	easy make clear definition “paraphrase”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Sometimes extracted phrases meaningful consider without context, set following criteria.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	two phrases used express relationship within information extraction application (“scenario”), two phrases paraphrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Although precise criterion, cases evaluated relatively clear-cut.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	general, different modalities (“planned buy”, “agreed buy”, “bought”) considered express relationship within extraction setting.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	problem classifying modified noun phrases modified phrase represent qualified restricted form head, like “chairman” “vice chairman”, represented keyword “chairman”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	specific case, two titles could fill column IE table, regarded paraphrases evaluation.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Evaluation within set evaluation paraphrases within set phrases share keyword illustrated Figure 4.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	set, phrases bracketed frequencies considered paraphrases set.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	example, phrase “'s New York-based trust unit,” paraphrase phrases “unit” set.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	see figure, accuracy domain quite high except “agree” set, contains various expressions representing different relationships IE application.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	accuracy calculated ratio number paraphrases total number phrases set.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	results, along total number phrases, shown Table 1.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	ai n # ph ras es l p h r e ac cu ra cy C C 7 r r e 1 0 5 8 7 . 6 % 6 r l e 1 0 6 6 7 . 0 % P C 7 r r e 3 5 9 9 9 . 2 % 6 r l e 2 5 5 6 5 . 1 % Table 1.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Evaluation results within sets Table 1 shows evaluation result based number phrases set.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	larger sets accurate small sets.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	make several observations cause errors.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	One smaller sets sometime meaningless keywords, like “strength” “add” CC-domain, “compare” PC-domain.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Eight thirteen errors high frequency phrases CC-domain phrases “agree”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	seen Figure 3, phrases “agree” set include completely different relationships, paraphrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	errors include NE tagging errors errors due phrase includes NEs.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	example, phrase “Company-A last week purchased rival Marshalls Company-B”, purchased company Marshalls, Company-B.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Also cases one two NEs belong phrase outside relation.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	example, sentence “Mr.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Smith estimates Lotus make profit quarter…”, system extracts “Smith esti mates Lotus” instance.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Obviously “Lotus” part following clause rather object “estimates” extracted instance makes sense.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	return issues discussion section.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Evaluation links link two sets considered correct majority phrases sets meaning, i.e. link indicates paraphrase.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	links “CC-domain shown Step 4 subsection 3.2.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	15 links, 4 errors, namely “buy - pay”, “acquire - pay”, “purchase - stake” “acquisition - stake”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	company buys another company, paying event occur, two phrases indicate event.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	similar explanation applies link “stake” set.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	checked whether discovered links listed WordNet.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	2 link CC- domain (buy-purchase, acquire-acquisition) 2 links (trader-dealer head-chief) PC- domain found synset Word- Net 2.1 (http://wordnet.princeton.edu/).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	result suggests benefit using automatic discovery method.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	ai n Li n k ac cu ra cy W N c v e r g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Evaluation results links	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	work reported closely related [Ha- segawa et al. 04].	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	First, describe method compare method.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	first collect NE instance pairs contexts, like method.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	However, next step clearly different.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	cluster NE instance pairs based words contexts using bag- of-words method.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	order create good-sized vectors similarity calculation, set high frequency threshold, 30.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	threshold, NE instance pairs could used hence variety phrases also limited.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Instead, focused phrases set frequency threshold 2, able utilize lot phrases minimizing noise.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	[Hasegawa et al. 04] reported relation discovery, one could easily acquire para phrases results.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	number NE instance pairs used experiment less half method.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	kinds efforts discover paraphrase automatically corpora.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	One approaches uses comparable documents, sets documents whose content found/known almost same, different newspaper stories event [Shinyama Sekine 03] different translations story [Barzilay 01].	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	availability comparable corpora limited, significant limitation approach.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Another approach finding paraphrases find phrases take similar subjects objects large corpora using mutual information word distribution [Lin Pantel 01].	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	approach needs phrase initial seed thus possible relationships extracted naturally limited.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	also work using bootstrap- ping approach [Brin 98; Agichtein Gravano 00; Ravichandran Hovy 02].	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	basic strategy is, given pair entity types, start examples, like several famous book title author pairs; find expressions contains names; using found expressions, find author book title pairs.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	repeated several times collect list author / book title pairs expressions.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	However, methods need initial seeds, relation entities known advance.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	limitation obstacle making technology “open domain”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Keywords one word evaluation, explained “chairman” “vice chairman” considered paraphrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	However, desirable separate them.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	problem arises keywords consist one word.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Sometime, multiple words needed, like “vice chairman”, “prime minister” “pay for” (“pay” “pay for” different senses CC-domain).	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	One possibility use n-grams based mutual information.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	frequent multi-word sequence domain, could use keyword candidate.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Keyword detection error Even keyword consists single word, words desirable keywords domain.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	explained results section, “strength” “add” desirable keywords CC-domain.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	experiment, set threshold TF/ITF score empirically using small development corpus; finer adjustment threshold could reduce number keywords.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Also, “agree” CC-domain desirable keyword.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	relatively frequent word domain, used different extraction scenarios.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	domain major scenarios involve things agreed on, rather mere fact agreed.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	“Agree” subject control verb, dominates another verb whose subject “agree”; latter verb generally one interest extraction.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	checked similar verbs major domains, one.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Using structural information explained results section, extracted examples like “Smith estimates Lotus”, sentence like “Mr.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Smith estimates Lotus make profit quarter…”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	order solve problem, parse tree needed understand “Lotus” object “estimates”.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Chunking enough find relationships.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	remains future work.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Limitations several limitations methods.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	phrases expressions length less 5 chunks, appear two NEs.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Also, method using keywords rules phrases don’t contain popular words domain.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	claiming method almighty.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Rather believe several methods developed using different heuristics discover wider variety paraphrases.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Applications discovered paraphrases multiple applications.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	One obvious application information extraction.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	IE, creating patterns express requested scenario, e.g. “management succession” “corporate merger acquisition” regarded hardest task.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	discovered paraphrases big help reduce human labor create comprehensive pattern set.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	Also, expanding techniques automatic generation extraction patterns (Riloff 96; Sudo 03) using method, extraction patterns meaning automatically linked, enabling us produce final table fully automatically.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	obstacles completing idea, believe automatic paraphrase discovery important component building fully automatic information extraction system.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	proposed unsupervised method discover paraphrases large untagged corpus.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	focusing phrases two Named Entities (NEs), types phrases important IE applications.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	tagging large corpus automatic NE tagger, method tries find sets paraphrases automatically without given seed phrase kind cue.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	total 13,976 phrases assigned sets phrases, accuracy evaluation data ranges 65 99%, depending domain size sets.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	accuracies link 73% 86% two evaluated domains.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	results promising several avenues improving results.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	research supported part Defense Advanced Research Projects Agency part Translingual Information Detection, Extraction Summarization (TIDES) program, Grant N66001001-18917 Space Naval Warfare Systems Center, San Diego, National Science Foundation Grant IIS00325657.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	paper necessarily reflect position U.S. Government.	0
As for paraphrase, Sekineâ€™s Paraphrase Database (Sekine, 2005) is collected using an unsupervised method, and focuses on phrases connecting two Named Entities.	would like thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa Mr. Yusuke Shinyama useful comments, discussion evaluation.	0
