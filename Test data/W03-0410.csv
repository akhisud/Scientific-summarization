Stevenson and Joanis, 2003 for English semantic verb classes	Semi-supervised Verb Class Discovery Using Noisy Features	0
Stevenson and Joanis, 2003 for English semantic verb classes	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Stevenson and Joanis, 2003 for English semantic verb classes	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Stevenson and Joanis, 2003 for English semantic verb classes	find unsupervised method tried cannot consistently applied data.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Stevenson and Joanis, 2003 for English semantic verb classes	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Stevenson and Joanis, 2003 for English semantic verb classes	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	1
Stevenson and Joanis, 2003 for English semantic verb classes	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Stevenson and Joanis, 2003 for English semantic verb classes	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Stevenson and Joanis, 2003 for English semantic verb classes	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Stevenson and Joanis, 2003 for English semantic verb classes	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Stevenson and Joanis, 2003 for English semantic verb classes	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Stevenson and Joanis, 2003 for English semantic verb classes	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Stevenson and Joanis, 2003 for English semantic verb classes	avoiding dependence precise feature extraction, approach portable new languages.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, general feature space means features irrelevant given verb discrimination task.	0
Stevenson and Joanis, 2003 for English semantic verb classes	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Stevenson and Joanis, 2003 for English semantic verb classes	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Stevenson and Joanis, 2003 for English semantic verb classes	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Stevenson and Joanis, 2003 for English semantic verb classes	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Stevenson and Joanis, 2003 for English semantic verb classes	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Stevenson and Joanis, 2003 for English semantic verb classes	unsupervised feature selection method, hand, usable data.	0
Stevenson and Joanis, 2003 for English semantic verb classes	remainder paper, first briefly review feature space present experimental classes verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Stevenson and Joanis, 2003 for English semantic verb classes	conclude discussion related work, contributions, future directions.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Stevenson and Joanis, 2003 for English semantic verb classes	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Stevenson and Joanis, 2003 for English semantic verb classes	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Thus, features serve approximations underlying distinctions among classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Stevenson and Joanis, 2003 for English semantic verb classes	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Stevenson and Joanis, 2003 for English semantic verb classes	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Stevenson and Joanis, 2003 for English semantic verb classes	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Stevenson and Joanis, 2003 for English semantic verb classes	allowable alternations expressions arguments vary according class verb.	0
Stevenson and Joanis, 2003 for English semantic verb classes	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Stevenson and Joanis, 2003 for English semantic verb classes	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Stevenson and Joanis, 2003 for English semantic verb classes	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Stevenson and Joanis, 2003 for English semantic verb classes	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Stevenson and Joanis, 2003 for English semantic verb classes	describe selection experimental classes verbs, estimation feature values.	0
Stevenson and Joanis, 2003 for English semantic verb classes	3.1 Verb Classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Stevenson and Joanis, 2003 for English semantic verb classes	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Stevenson and Joanis, 2003 for English semantic verb classes	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Benefactive versus Recipient verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Mary baked... cake Joan/Joan cake.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Mary gave... cake Joan/Joan cake.	0
Stevenson and Joanis, 2003 for English semantic verb classes	dative alternation verbs differ preposition semantic role object.	0
Stevenson and Joanis, 2003 for English semantic verb classes	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Admire versus Amuse verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	admire Jane.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Run versus Sound Emission verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	kids ran room./*The room ran kids.	0
Stevenson and Joanis, 2003 for English semantic verb classes	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Cheat versus Steal Remove verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	cheated...	0
Stevenson and Joanis, 2003 for English semantic verb classes	Jane money/*the money Jane.	0
Stevenson and Joanis, 2003 for English semantic verb classes	stole...	0
Stevenson and Joanis, 2003 for English semantic verb classes	*Jane money/the money Jane.	0
Stevenson and Joanis, 2003 for English semantic verb classes	classes also assign semantic arguments, differ prepositional alternants.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Wipe versus Steal Remove verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Wipe... dust/the dust table/the table.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Steal... money/the money bank/*the bank.	0
Stevenson and Joanis, 2003 for English semantic verb classes	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Stevenson and Joanis, 2003 for English semantic verb classes	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Stevenson and Joanis, 2003 for English semantic verb classes	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Stevenson and Joanis, 2003 for English semantic verb classes	loaded... hay wagon/the wagon hay.	0
Stevenson and Joanis, 2003 for English semantic verb classes	filled...	0
Stevenson and Joanis, 2003 for English semantic verb classes	*hay wagon/the wagon hay.	0
Stevenson and Joanis, 2003 for English semantic verb classes	put... hay wagon/*the wagon hay.	0
Stevenson and Joanis, 2003 for English semantic verb classes	three classes also assign semantic roles differ prepositional alternants.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Note, however, options Spray/Load verbs overlap two types verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Stevenson and Joanis, 2003 for English semantic verb classes	horse raced./The jockey raced horse.	0
Stevenson and Joanis, 2003 for English semantic verb classes	butter melted./The cook melted butter.	0
Stevenson and Joanis, 2003 for English semantic verb classes	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Stevenson and Joanis, 2003 for English semantic verb classes	(Note Object Drop verbs superset Benefactives above.)	0
Stevenson and Joanis, 2003 for English semantic verb classes	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Dorr Jones, 1996).	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Indicators PP usage thus useful definitive.	0
Stevenson and Joanis, 2003 for English semantic verb classes	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Stevenson and Joanis, 2003 for English semantic verb classes	1, 26.	0
Stevenson and Joanis, 2003 for English semantic verb classes	3 3 5 ci pi en 13.	0
Stevenson and Joanis, 2003 for English semantic verb classes	1, 13.	0
Stevenson and Joanis, 2003 for English semantic verb classes	3 2 7 Ad mi 31.	0
Stevenson and Joanis, 2003 for English semantic verb classes	2 3 5 us e 31.	0
Stevenson and Joanis, 2003 for English semantic verb classes	1 1 3 4 Ru n 51.	0
Stevenson and Joanis, 2003 for English semantic verb classes	3.2 7 9 un E mi ssi 43.	0
Stevenson and Joanis, 2003 for English semantic verb classes	2 5 6 C 10.	0
Stevenson and Joanis, 2003 for English semantic verb classes	6 2 9 St ea l ov e 10.	0
Stevenson and Joanis, 2003 for English semantic verb classes	5, 10.	0
Stevenson and Joanis, 2003 for English semantic verb classes	1 4 5 Wi pe 10.	0
Stevenson and Joanis, 2003 for English semantic verb classes	4.1 , 10.	0
Stevenson and Joanis, 2003 for English semantic verb classes	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Stevenson and Joanis, 2003 for English semantic verb classes	1– 4 1 6 9 bj ec Dr op 26.	0
Stevenson and Joanis, 2003 for English semantic verb classes	1, 26.	0
Stevenson and Joanis, 2003 for English semantic verb classes	3, 26.	0
Stevenson and Joanis, 2003 for English semantic verb classes	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Stevenson and Joanis, 2003 for English semantic verb classes	3.2 Verb Selection.	0
Stevenson and Joanis, 2003 for English semantic verb classes	experimental verbs selected follows.	0
Stevenson and Joanis, 2003 for English semantic verb classes	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Stevenson and Joanis, 2003 for English semantic verb classes	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Table 1 shows number verbs class end process.	0
Stevenson and Joanis, 2003 for English semantic verb classes	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Stevenson and Joanis, 2003 for English semantic verb classes	began set 20 verbs per class current work.	0
Stevenson and Joanis, 2003 for English semantic verb classes	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Stevenson and Joanis, 2003 for English semantic verb classes	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Stevenson and Joanis, 2003 for English semantic verb classes	3.3 Feature Extraction.	0
Stevenson and Joanis, 2003 for English semantic verb classes	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Since general corpus, expect strong overall domain bias verb usage.	0
Stevenson and Joanis, 2003 for English semantic verb classes	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Stevenson and Joanis, 2003 for English semantic verb classes	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Stevenson and Joanis, 2003 for English semantic verb classes	4.1 Clustering Parameters.	0
Stevenson and Joanis, 2003 for English semantic verb classes	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Stevenson and Joanis, 2003 for English semantic verb classes	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Stevenson and Joanis, 2003 for English semantic verb classes	used simple Euclidean distance former, Ward linkage latter.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Stevenson and Joanis, 2003 for English semantic verb classes	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Stevenson and Joanis, 2003 for English semantic verb classes	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Stevenson and Joanis, 2003 for English semantic verb classes	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Stevenson and Joanis, 2003 for English semantic verb classes	4.2 Evaluation Measures.	0
Stevenson and Joanis, 2003 for English semantic verb classes	use three separate evaluation measures, tap different properties clusterings.	0
Stevenson and Joanis, 2003 for English semantic verb classes	4.2.1 Accuracy assign cluster class label majority members.	0
Stevenson and Joanis, 2003 for English semantic verb classes	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Stevenson and Joanis, 2003 for English semantic verb classes	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Stevenson and Joanis, 2003 for English semantic verb classes	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Stevenson and Joanis, 2003 for English semantic verb classes	theoretical maximum is, course, 1.	0
Stevenson and Joanis, 2003 for English semantic verb classes	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Stevenson and Joanis, 2003 for English semantic verb classes	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Stevenson and Joanis, 2003 for English semantic verb classes	figures reported results Table 2 below.	0
Stevenson and Joanis, 2003 for English semantic verb classes	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Stevenson and Joanis, 2003 for English semantic verb classes	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Stevenson and Joanis, 2003 for English semantic verb classes	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Stevenson and Joanis, 2003 for English semantic verb classes	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Stevenson and Joanis, 2003 for English semantic verb classes	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, useful relative measure good-.	0
Stevenson and Joanis, 2003 for English semantic verb classes	ness, comparing clusterings arising different feature sets.	0
Stevenson and Joanis, 2003 for English semantic verb classes	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, since fix number clusters number classes, measure remains informative.	0
Stevenson and Joanis, 2003 for English semantic verb classes	3 experiments estimating baseline, in-.	0
Stevenson and Joanis, 2003 for English semantic verb classes	deed found mean value 0.00 random clusterings.	0
Stevenson and Joanis, 2003 for English semantic verb classes	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Stevenson and Joanis, 2003 for English semantic verb classes	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Stevenson and Joanis, 2003 for English semantic verb classes	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Stevenson and Joanis, 2003 for English semantic verb classes	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Stevenson and Joanis, 2003 for English semantic verb classes	higher (.89 vs. .33) reflects better separation data.	0
Stevenson and Joanis, 2003 for English semantic verb classes	regard target classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Stevenson and Joanis, 2003 for English semantic verb classes	value 0 suggests point clearly particular cluster.	0
Stevenson and Joanis, 2003 for English semantic verb classes	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Stevenson and Joanis, 2003 for English semantic verb classes	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Stevenson and Joanis, 2003 for English semantic verb classes	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Stevenson and Joanis, 2003 for English semantic verb classes	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Stevenson and Joanis, 2003 for English semantic verb classes	measure independent true classification, could high dependent measures low, vice versa.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Stevenson and Joanis, 2003 for English semantic verb classes	alternations.	0
Stevenson and Joanis, 2003 for English semantic verb classes	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Stevenson and Joanis, 2003 for English semantic verb classes	13-way task includes classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Stevenson and Joanis, 2003 for English semantic verb classes	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Stevenson and Joanis, 2003 for English semantic verb classes	third column Table 2 gives baseline calculated random clusterings.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Recall upper bound random performance.	0
Stevenson and Joanis, 2003 for English semantic verb classes	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Stevenson and Joanis, 2003 for English semantic verb classes	5.1 Full Feature Set.	0
Stevenson and Joanis, 2003 for English semantic verb classes	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Although generally higher baseline, well supervised learner, generally low.	0
Stevenson and Joanis, 2003 for English semantic verb classes	5.2 Manual Feature Selection.	0
Stevenson and Joanis, 2003 for English semantic verb classes	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Stevenson and Joanis, 2003 for English semantic verb classes	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Stevenson and Joanis, 2003 for English semantic verb classes	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Stevenson and Joanis, 2003 for English semantic verb classes	C5.0 supervised accuracy; Base random clusters.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Stevenson and Joanis, 2003 for English semantic verb classes	See text description.	0
Stevenson and Joanis, 2003 for English semantic verb classes	indicated class description given Levin.	0
Stevenson and Joanis, 2003 for English semantic verb classes	task, then, linguistically-relevant subset defined union subsets classes task.	0
Stevenson and Joanis, 2003 for English semantic verb classes	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Stevenson and Joanis, 2003 for English semantic verb classes	2-way tasks, performance average close full feature set measures.	0
Stevenson and Joanis, 2003 for English semantic verb classes	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Stevenson and Joanis, 2003 for English semantic verb classes	performance comparison tentatively suggests good feature selection helpful task.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, important find method depend existing classification, since interested applying approach classification exist.	0
Stevenson and Joanis, 2003 for English semantic verb classes	next two sections, present unsupervised minimally supervised approaches problem.	0
Stevenson and Joanis, 2003 for English semantic verb classes	5.3 Unsupervised Feature Selection.	0
Stevenson and Joanis, 2003 for English semantic verb classes	order deal excessive dimensionality, Dash et al.	0
Stevenson and Joanis, 2003 for English semantic verb classes	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Unfortunately, promising method prove practical data.	0
Stevenson and Joanis, 2003 for English semantic verb classes	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Stevenson and Joanis, 2003 for English semantic verb classes	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Stevenson and Joanis, 2003 for English semantic verb classes	5.4 Semi-Supervised Feature Selection.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Stevenson and Joanis, 2003 for English semantic verb classes	domain particular, verb class discovery “in vacuum” necessary.	0
Stevenson and Joanis, 2003 for English semantic verb classes	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Stevenson and Joanis, 2003 for English semantic verb classes	model kind approach, selected sample five seed verbs class.	0
Stevenson and Joanis, 2003 for English semantic verb classes	set verbs judged (by authors’ intuition alone) “representative” class.	0
Stevenson and Joanis, 2003 for English semantic verb classes	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Stevenson and Joanis, 2003 for English semantic verb classes	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Stevenson and Joanis, 2003 for English semantic verb classes	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Stevenson and Joanis, 2003 for English semantic verb classes	extracted resulting decision trees union features used, formed reduced feature set task.	0
Stevenson and Joanis, 2003 for English semantic verb classes	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Stevenson and Joanis, 2003 for English semantic verb classes	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Stevenson and Joanis, 2003 for English semantic verb classes	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Stevenson and Joanis, 2003 for English semantic verb classes	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Stevenson and Joanis, 2003 for English semantic verb classes	5.5 Discussion.	0
Stevenson and Joanis, 2003 for English semantic verb classes	clustering experiments, find smaller subsets features generally perform better full set features.	0
Stevenson and Joanis, 2003 for English semantic verb classes	(See Table 3 number features Ling Seed sets.)	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, small set features adequate.	0
Stevenson and Joanis, 2003 for English semantic verb classes	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Stevenson and Joanis, 2003 for English semantic verb classes	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Stevenson and Joanis, 2003 for English semantic verb classes	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Stevenson and Joanis, 2003 for English semantic verb classes	number classes (a simple linear function roughly approximating number features Seed sets).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Interestingly, generally high, indicating structure data, matches classification.	0
Stevenson and Joanis, 2003 for English semantic verb classes	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Stevenson and Joanis, 2003 for English semantic verb classes	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Stevenson and Joanis, 2003 for English semantic verb classes	might also ask, would subset verbs well?	0
Stevenson and Joanis, 2003 for English semantic verb classes	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Stevenson and Joanis, 2003 for English semantic verb classes	found mean values Seed set reported above, mean little lower.	0
Stevenson and Joanis, 2003 for English semantic verb classes	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Stevenson and Joanis, 2003 for English semantic verb classes	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, study used small set five features manually devised set three particular classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Stevenson and Joanis, 2003 for English semantic verb classes	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Stevenson and Joanis, 2003 for English semantic verb classes	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Stevenson and Joanis, 2003 for English semantic verb classes	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Stevenson and Joanis, 2003 for English semantic verb classes	question future research explore effect variables clustering performance.	0
Stevenson and Joanis, 2003 for English semantic verb classes	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Stevenson and Joanis, 2003 for English semantic verb classes	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Stevenson and Joanis, 2003 for English semantic verb classes	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Stevenson and Joanis, 2003 for English semantic verb classes	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Stevenson and Joanis, 2003 for English semantic verb classes	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Stevenson and Joanis, 2003 for English semantic verb classes	successful seed set features is, still achieve accuracy supervised learner.	0
Stevenson and Joanis, 2003 for English semantic verb classes	research needed definition general feature space, well methods selecting useful set features clustering.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Furthermore, might question clustering approach itself, context verb class discovery.	0
Stevenson and Joanis, 2003 for English semantic verb classes	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Stevenson and Joanis, 2003 for English semantic verb classes	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Stevenson and Joanis, 2003 for English semantic verb classes	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Stevenson and Joanis, 2003 for English semantic verb classes	indebted Allan Jepson helpful discussions suggestions.	0
Stevenson and Joanis, 2003 for English semantic verb classes	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Semi-supervised Verb Class Discovery Using Noisy Features	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	find unsupervised method tried cannot consistently applied data.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	1
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	avoiding dependence precise feature extraction, approach portable new languages.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, general feature space means features irrelevant given verb discrimination task.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	unsupervised feature selection method, hand, usable data.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	remainder paper, first briefly review feature space present experimental classes verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	conclude discussion related work, contributions, future directions.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Thus, features serve approximations underlying distinctions among classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	allowable alternations expressions arguments vary according class verb.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	describe selection experimental classes verbs, estimation feature values.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3.1 Verb Classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Benefactive versus Recipient verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Mary baked... cake Joan/Joan cake.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Mary gave... cake Joan/Joan cake.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	dative alternation verbs differ preposition semantic role object.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Admire versus Amuse verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	admire Jane.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Run versus Sound Emission verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	kids ran room./*The room ran kids.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Cheat versus Steal Remove verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	cheated...	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Jane money/*the money Jane.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	stole...	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	*Jane money/the money Jane.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	classes also assign semantic arguments, differ prepositional alternants.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Wipe versus Steal Remove verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Wipe... dust/the dust table/the table.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Steal... money/the money bank/*the bank.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	loaded... hay wagon/the wagon hay.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	filled...	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	*hay wagon/the wagon hay.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	put... hay wagon/*the wagon hay.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	three classes also assign semantic roles differ prepositional alternants.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Note, however, options Spray/Load verbs overlap two types verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	horse raced./The jockey raced horse.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	butter melted./The cook melted butter.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(Note Object Drop verbs superset Benefactives above.)	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Dorr Jones, 1996).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Indicators PP usage thus useful definitive.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1, 26.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3 3 5 ci pi en 13.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1, 13.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3 2 7 Ad mi 31.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	2 3 5 us e 31.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1 1 3 4 Ru n 51.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3.2 7 9 un E mi ssi 43.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	2 5 6 C 10.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	6 2 9 St ea l ov e 10.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5, 10.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1 4 5 Wi pe 10.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.1 , 10.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1– 4 1 6 9 bj ec Dr op 26.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1, 26.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3, 26.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3.2 Verb Selection.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	experimental verbs selected follows.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Table 1 shows number verbs class end process.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	began set 20 verbs per class current work.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3.3 Feature Extraction.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Since general corpus, expect strong overall domain bias verb usage.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.1 Clustering Parameters.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	used simple Euclidean distance former, Ward linkage latter.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2 Evaluation Measures.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	use three separate evaluation measures, tap different properties clusterings.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2.1 Accuracy assign cluster class label majority members.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	theoretical maximum is, course, 1.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	figures reported results Table 2 below.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, useful relative measure good-.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	ness, comparing clusterings arising different feature sets.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, since fix number clusters number classes, measure remains informative.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3 experiments estimating baseline, in-.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	deed found mean value 0.00 random clusterings.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	higher (.89 vs. .33) reflects better separation data.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	regard target classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	value 0 suggests point clearly particular cluster.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	measure independent true classification, could high dependent measures low, vice versa.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	alternations.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	13-way task includes classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	third column Table 2 gives baseline calculated random clusterings.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Recall upper bound random performance.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.1 Full Feature Set.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Although generally higher baseline, well supervised learner, generally low.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.2 Manual Feature Selection.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	C5.0 supervised accuracy; Base random clusters.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	See text description.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	indicated class description given Levin.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	task, then, linguistically-relevant subset defined union subsets classes task.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	2-way tasks, performance average close full feature set measures.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	performance comparison tentatively suggests good feature selection helpful task.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, important find method depend existing classification, since interested applying approach classification exist.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	next two sections, present unsupervised minimally supervised approaches problem.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.3 Unsupervised Feature Selection.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	order deal excessive dimensionality, Dash et al.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Unfortunately, promising method prove practical data.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.4 Semi-Supervised Feature Selection.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	domain particular, verb class discovery “in vacuum” necessary.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	model kind approach, selected sample five seed verbs class.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	set verbs judged (by authors’ intuition alone) “representative” class.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	extracted resulting decision trees union features used, formed reduced feature set task.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.5 Discussion.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	clustering experiments, find smaller subsets features generally perform better full set features.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(See Table 3 number features Ling Seed sets.)	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, small set features adequate.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	number classes (a simple linear function roughly approximating number features Seed sets).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Interestingly, generally high, indicating structure data, matches classification.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	might also ask, would subset verbs well?	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	found mean values Seed set reported above, mean little lower.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, study used small set five features manually devised set three particular classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	question future research explore effect variables clustering performance.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	successful seed set features is, still achieve accuracy supervised learner.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	research needed definition general feature space, well methods selecting useful set features clustering.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Furthermore, might question clustering approach itself, context verb class discovery.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	indebted Allan Jepson helpful discussions suggestions.	0
Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Semi-supervised Verb Class Discovery Using Noisy Features	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	find unsupervised method tried cannot consistently applied data.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	1
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	avoiding dependence precise feature extraction, approach portable new languages.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, general feature space means features irrelevant given verb discrimination task.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	unsupervised feature selection method, hand, usable data.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	remainder paper, first briefly review feature space present experimental classes verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	conclude discussion related work, contributions, future directions.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Thus, features serve approximations underlying distinctions among classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	allowable alternations expressions arguments vary according class verb.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	describe selection experimental classes verbs, estimation feature values.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3.1 Verb Classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Benefactive versus Recipient verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Mary baked... cake Joan/Joan cake.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Mary gave... cake Joan/Joan cake.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	dative alternation verbs differ preposition semantic role object.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Admire versus Amuse verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	admire Jane.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Run versus Sound Emission verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	kids ran room./*The room ran kids.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Cheat versus Steal Remove verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	cheated...	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Jane money/*the money Jane.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	stole...	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	*Jane money/the money Jane.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	classes also assign semantic arguments, differ prepositional alternants.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Wipe versus Steal Remove verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Wipe... dust/the dust table/the table.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Steal... money/the money bank/*the bank.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	loaded... hay wagon/the wagon hay.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	filled...	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	*hay wagon/the wagon hay.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	put... hay wagon/*the wagon hay.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	three classes also assign semantic roles differ prepositional alternants.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Note, however, options Spray/Load verbs overlap two types verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	horse raced./The jockey raced horse.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	butter melted./The cook melted butter.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(Note Object Drop verbs superset Benefactives above.)	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Dorr Jones, 1996).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Indicators PP usage thus useful definitive.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1, 26.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3 3 5 ci pi en 13.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1, 13.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3 2 7 Ad mi 31.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	2 3 5 us e 31.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1 1 3 4 Ru n 51.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3.2 7 9 un E mi ssi 43.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	2 5 6 C 10.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	6 2 9 St ea l ov e 10.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5, 10.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1 4 5 Wi pe 10.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.1 , 10.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1– 4 1 6 9 bj ec Dr op 26.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1, 26.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3, 26.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3.2 Verb Selection.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	experimental verbs selected follows.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Table 1 shows number verbs class end process.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	began set 20 verbs per class current work.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3.3 Feature Extraction.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Since general corpus, expect strong overall domain bias verb usage.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.1 Clustering Parameters.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	used simple Euclidean distance former, Ward linkage latter.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2 Evaluation Measures.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	use three separate evaluation measures, tap different properties clusterings.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2.1 Accuracy assign cluster class label majority members.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	theoretical maximum is, course, 1.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	figures reported results Table 2 below.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, useful relative measure good-.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	ness, comparing clusterings arising different feature sets.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, since fix number clusters number classes, measure remains informative.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3 experiments estimating baseline, in-.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	deed found mean value 0.00 random clusterings.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	higher (.89 vs. .33) reflects better separation data.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	regard target classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	value 0 suggests point clearly particular cluster.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	measure independent true classification, could high dependent measures low, vice versa.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	alternations.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	13-way task includes classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	third column Table 2 gives baseline calculated random clusterings.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Recall upper bound random performance.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.1 Full Feature Set.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Although generally higher baseline, well supervised learner, generally low.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.2 Manual Feature Selection.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	C5.0 supervised accuracy; Base random clusters.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	See text description.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	indicated class description given Levin.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	task, then, linguistically-relevant subset defined union subsets classes task.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	2-way tasks, performance average close full feature set measures.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	performance comparison tentatively suggests good feature selection helpful task.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, important find method depend existing classification, since interested applying approach classification exist.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	next two sections, present unsupervised minimally supervised approaches problem.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.3 Unsupervised Feature Selection.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	order deal excessive dimensionality, Dash et al.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Unfortunately, promising method prove practical data.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.4 Semi-Supervised Feature Selection.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	domain particular, verb class discovery “in vacuum” necessary.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	model kind approach, selected sample five seed verbs class.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	set verbs judged (by authors’ intuition alone) “representative” class.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	extracted resulting decision trees union features used, formed reduced feature set task.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	5.5 Discussion.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	clustering experiments, find smaller subsets features generally perform better full set features.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	(See Table 3 number features Ling Seed sets.)	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, small set features adequate.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	number classes (a simple linear function roughly approximating number features Seed sets).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Interestingly, generally high, indicating structure data, matches classification.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	might also ask, would subset verbs well?	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	found mean values Seed set reported above, mean little lower.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, study used small set five features manually devised set three particular classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	question future research explore effect variables clustering performance.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	successful seed set features is, still achieve accuracy supervised learner.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	research needed definition general feature space, well methods selecting useful set features clustering.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Furthermore, might question clustering approach itself, context verb class discovery.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	indebted Allan Jepson helpful discussions suggestions.	0
The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Semi-supervised Verb Class Discovery Using Noisy Features	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	feature set previously shown work well supervised learning setting, using known English verb classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	find unsupervised method tried cannot consistently applied data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	avoiding dependence precise feature extraction, approach portable new languages.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, general feature space means features irrelevant given verb discrimination task.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	unsupervised feature selection method, hand, usable data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	remainder paper, first briefly review feature space present experimental classes verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	describe clustering methodology, measures use evaluate clustering, experimental results.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	conclude discussion related work, contributions, future directions.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Thus, features serve approximations underlying distinctions among classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	allowable alternations expressions arguments vary according class verb.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	describe selection experimental classes verbs, estimation feature values.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	3.1 Verb Classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Benefactive versus Recipient verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Mary baked... cake Joan/Joan cake.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Mary gave... cake Joan/Joan cake.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	dative alternation verbs differ preposition semantic role object.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Admire versus Amuse verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	admire Jane.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Run versus Sound Emission verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	kids ran room./*The room ran kids.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Cheat versus Steal Remove verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	cheated...	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Jane money/*the money Jane.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	stole...	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	*Jane money/the money Jane.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	classes also assign semantic arguments, differ prepositional alternants.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Wipe versus Steal Remove verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Wipe... dust/the dust table/the table.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Steal... money/the money bank/*the bank.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	classes generally allow syntactic frames, differ possible semantic role assignment.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	loaded... hay wagon/the wagon hay.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	filled...	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	*hay wagon/the wagon hay.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	put... hay wagon/*the wagon hay.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	three classes also assign semantic roles differ prepositional alternants.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Note, however, options Spray/Load verbs overlap two types verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	horse raced./The jockey raced horse.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	butter melted./The cook melted butter.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	(Note Object Drop verbs superset Benefactives above.)	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Dorr Jones, 1996).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Indicators PP usage thus useful definitive.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	1, 26.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	3 3 5 ci pi en 13.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	1, 13.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	3 2 7 Ad mi 31.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	2 3 5 us e 31.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	1 1 3 4 Ru n 51.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	3.2 7 9 un E mi ssi 43.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	2 5 6 C 10.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	6 2 9 St ea l ov e 10.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	5, 10.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	1 4 5 Wi pe 10.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	4.1 , 10.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	1– 4 1 6 9 bj ec Dr op 26.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	1, 26.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	3, 26.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	3.2 Verb Selection.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	experimental verbs selected follows.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Table 1 shows number verbs class end process.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	began set 20 verbs per class current work.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	3.3 Feature Extraction.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Since general corpus, expect strong overall domain bias verb usage.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	4.1 Clustering Parameters.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	1
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	used simple Euclidean distance former, Ward linkage latter.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	explore this, induce number clusters making cut particular level clustering hierarchy.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	4.2 Evaluation Measures.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	use three separate evaluation measures, tap different properties clusterings.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	4.2.1 Accuracy assign cluster class label majority members.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	theoretical maximum is, course, 1.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	figures reported results Table 2 below.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, useful relative measure good-.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	ness, comparing clusterings arising different feature sets.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, since fix number clusters number classes, measure remains informative.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	3 experiments estimating baseline, in-.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	deed found mean value 0.00 random clusterings.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	higher (.89 vs. .33) reflects better separation data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	regard target classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	use , mean silhouette measure Matlab, measures distant data point clusters.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	value 0 suggests point clearly particular cluster.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	measure independent true classification, could high dependent measures low, vice versa.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	alternations.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	13-way task includes classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	third column Table 2 gives baseline calculated random clusterings.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Recall upper bound random performance.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	5.1 Full Feature Set.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Although generally higher baseline, well supervised learner, generally low.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	5.2 Manual Feature Selection.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	One approach dimensionality reduction hand- select features one believes relevant given task.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	C5.0 supervised accuracy; Base random clusters.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	See text description.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	indicated class description given Levin.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	task, then, linguistically-relevant subset defined union subsets classes task.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	2-way tasks, performance average close full feature set measures.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	performance comparison tentatively suggests good feature selection helpful task.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, important find method depend existing classification, since interested applying approach classification exist.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	next two sections, present unsupervised minimally supervised approaches problem.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	5.3 Unsupervised Feature Selection.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	order deal excessive dimensionality, Dash et al.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Unfortunately, promising method prove practical data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Many feature sets performed well, far outperformed best results using feature selection methods.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	5.4 Semi-Supervised Feature Selection.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	domain particular, verb class discovery “in vacuum” necessary.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	model kind approach, selected sample five seed verbs class.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	set verbs judged (by authors’ intuition alone) “representative” class.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	extracted resulting decision trees union features used, formed reduced feature set task.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Another striking result difference values, much higher Ling (which turn much higher Full).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	5.5 Discussion.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	clustering experiments, find smaller subsets features generally perform better full set features.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	(See Table 3 number features Ling Seed sets.)	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, small set features adequate.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	number classes (a simple linear function roughly approximating number features Seed sets).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Interestingly, generally high, indicating structure data, matches classification.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	confirms appropriate feature selection, small number features, important task verb class discovery.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	might also ask, would subset verbs well?	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	found mean values Seed set reported above, mean little lower.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, study used small set five features manually devised set three particular classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	question future research explore effect variables clustering performance.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Furthermore, method relatively insensitive precise makeup selected seed set.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	successful seed set features is, still achieve accuracy supervised learner.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	research needed definition general feature space, well methods selecting useful set features clustering.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Furthermore, might question clustering approach itself, context verb class discovery.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	indebted Allan Jepson helpful discussions suggestions.	0
We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003)	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Semi-supervised Verb Class Discovery Using Noisy Features	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	find unsupervised method tried cannot consistently applied data.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	avoiding dependence precise feature extraction, approach portable new languages.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, general feature space means features irrelevant given verb discrimination task.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	unsupervised feature selection method, hand, usable data.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	remainder paper, first briefly review feature space present experimental classes verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	conclude discussion related work, contributions, future directions.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Thus, features serve approximations underlying distinctions among classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	allowable alternations expressions arguments vary according class verb.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	describe selection experimental classes verbs, estimation feature values.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	3.1 Verb Classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Benefactive versus Recipient verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Mary baked... cake Joan/Joan cake.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Mary gave... cake Joan/Joan cake.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	dative alternation verbs differ preposition semantic role object.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Admire versus Amuse verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	admire Jane.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Run versus Sound Emission verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	kids ran room./*The room ran kids.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Cheat versus Steal Remove verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	cheated...	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Jane money/*the money Jane.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	stole...	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	*Jane money/the money Jane.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	classes also assign semantic arguments, differ prepositional alternants.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Wipe versus Steal Remove verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Wipe... dust/the dust table/the table.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Steal... money/the money bank/*the bank.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	loaded... hay wagon/the wagon hay.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	filled...	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	*hay wagon/the wagon hay.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	put... hay wagon/*the wagon hay.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	three classes also assign semantic roles differ prepositional alternants.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Note, however, options Spray/Load verbs overlap two types verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	horse raced./The jockey raced horse.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	butter melted./The cook melted butter.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	(Note Object Drop verbs superset Benefactives above.)	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Dorr Jones, 1996).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Indicators PP usage thus useful definitive.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	1, 26.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	3 3 5 ci pi en 13.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	1, 13.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	3 2 7 Ad mi 31.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	2 3 5 us e 31.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	1 1 3 4 Ru n 51.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	3.2 7 9 un E mi ssi 43.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	2 5 6 C 10.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	6 2 9 St ea l ov e 10.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	5, 10.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	1 4 5 Wi pe 10.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	4.1 , 10.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	1– 4 1 6 9 bj ec Dr op 26.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	1, 26.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	3, 26.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	3.2 Verb Selection.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	experimental verbs selected follows.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Table 1 shows number verbs class end process.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	began set 20 verbs per class current work.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	3.3 Feature Extraction.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Since general corpus, expect strong overall domain bias verb usage.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	4.1 Clustering Parameters.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	used simple Euclidean distance former, Ward linkage latter.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	4.2 Evaluation Measures.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	use three separate evaluation measures, tap different properties clusterings.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	4.2.1 Accuracy assign cluster class label majority members.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	theoretical maximum is, course, 1.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	figures reported results Table 2 below.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, useful relative measure good-.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	ness, comparing clusterings arising different feature sets.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, since fix number clusters number classes, measure remains informative.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	3 experiments estimating baseline, in-.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	deed found mean value 0.00 random clusterings.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	higher (.89 vs. .33) reflects better separation data.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	regard target classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	value 0 suggests point clearly particular cluster.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	measure independent true classification, could high dependent measures low, vice versa.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	alternations.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	13-way task includes classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	third column Table 2 gives baseline calculated random clusterings.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Recall upper bound random performance.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	5.1 Full Feature Set.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Although generally higher baseline, well supervised learner, generally low.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	5.2 Manual Feature Selection.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	C5.0 supervised accuracy; Base random clusters.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	See text description.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	indicated class description given Levin.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	task, then, linguistically-relevant subset defined union subsets classes task.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	2-way tasks, performance average close full feature set measures.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	performance comparison tentatively suggests good feature selection helpful task.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, important find method depend existing classification, since interested applying approach classification exist.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	next two sections, present unsupervised minimally supervised approaches problem.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	5.3 Unsupervised Feature Selection.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	order deal excessive dimensionality, Dash et al.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Unfortunately, promising method prove practical data.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	5.4 Semi-Supervised Feature Selection.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	domain particular, verb class discovery “in vacuum” necessary.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	model kind approach, selected sample five seed verbs class.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	set verbs judged (by authors’ intuition alone) “representative” class.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	extracted resulting decision trees union features used, formed reduced feature set task.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	5.5 Discussion.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	clustering experiments, find smaller subsets features generally perform better full set features.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	(See Table 3 number features Ling Seed sets.)	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, small set features adequate.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	number classes (a simple linear function roughly approximating number features Seed sets).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Interestingly, generally high, indicating structure data, matches classification.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	might also ask, would subset verbs well?	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	found mean values Seed set reported above, mean little lower.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, study used small set five features manually devised set three particular classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	question future research explore effect variables clustering performance.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	successful seed set features is, still achieve accuracy supervised learner.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	research needed definition general feature space, well methods selecting useful set features clustering.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Furthermore, might question clustering approach itself, context verb class discovery.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	indebted Allan Jepson helpful discussions suggestions.	0
We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Semi-supervised Verb Class Discovery Using Noisy Features	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	find unsupervised method tried cannot consistently applied data.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	avoiding dependence precise feature extraction, approach portable new languages.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, general feature space means features irrelevant given verb discrimination task.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	unsupervised feature selection method, hand, usable data.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	remainder paper, first briefly review feature space present experimental classes verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	conclude discussion related work, contributions, future directions.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Thus, features serve approximations underlying distinctions among classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	allowable alternations expressions arguments vary according class verb.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	describe selection experimental classes verbs, estimation feature values.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	3.1 Verb Classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Benefactive versus Recipient verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Mary baked... cake Joan/Joan cake.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Mary gave... cake Joan/Joan cake.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	dative alternation verbs differ preposition semantic role object.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Admire versus Amuse verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	admire Jane.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Run versus Sound Emission verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	kids ran room./*The room ran kids.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Cheat versus Steal Remove verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	cheated...	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Jane money/*the money Jane.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	stole...	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	*Jane money/the money Jane.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	classes also assign semantic arguments, differ prepositional alternants.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Wipe versus Steal Remove verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Wipe... dust/the dust table/the table.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Steal... money/the money bank/*the bank.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	loaded... hay wagon/the wagon hay.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	filled...	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	*hay wagon/the wagon hay.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	put... hay wagon/*the wagon hay.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	three classes also assign semantic roles differ prepositional alternants.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Note, however, options Spray/Load verbs overlap two types verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	horse raced./The jockey raced horse.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	butter melted./The cook melted butter.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	(Note Object Drop verbs superset Benefactives above.)	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Dorr Jones, 1996).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Indicators PP usage thus useful definitive.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	1, 26.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	3 3 5 ci pi en 13.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	1, 13.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	3 2 7 Ad mi 31.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	2 3 5 us e 31.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	1 1 3 4 Ru n 51.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	3.2 7 9 un E mi ssi 43.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	2 5 6 C 10.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	6 2 9 St ea l ov e 10.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	5, 10.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	1 4 5 Wi pe 10.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	4.1 , 10.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	1– 4 1 6 9 bj ec Dr op 26.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	1, 26.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	3, 26.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	3.2 Verb Selection.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	experimental verbs selected follows.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Table 1 shows number verbs class end process.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	began set 20 verbs per class current work.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	1
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	3.3 Feature Extraction.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Since general corpus, expect strong overall domain bias verb usage.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	4.1 Clustering Parameters.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	used simple Euclidean distance former, Ward linkage latter.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	4.2 Evaluation Measures.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	use three separate evaluation measures, tap different properties clusterings.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	4.2.1 Accuracy assign cluster class label majority members.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	theoretical maximum is, course, 1.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	figures reported results Table 2 below.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, useful relative measure good-.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	ness, comparing clusterings arising different feature sets.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, since fix number clusters number classes, measure remains informative.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	3 experiments estimating baseline, in-.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	deed found mean value 0.00 random clusterings.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	higher (.89 vs. .33) reflects better separation data.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	regard target classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	value 0 suggests point clearly particular cluster.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	measure independent true classification, could high dependent measures low, vice versa.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	alternations.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	13-way task includes classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	third column Table 2 gives baseline calculated random clusterings.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Recall upper bound random performance.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	5.1 Full Feature Set.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Although generally higher baseline, well supervised learner, generally low.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	5.2 Manual Feature Selection.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	C5.0 supervised accuracy; Base random clusters.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	See text description.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	indicated class description given Levin.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	task, then, linguistically-relevant subset defined union subsets classes task.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	2-way tasks, performance average close full feature set measures.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	performance comparison tentatively suggests good feature selection helpful task.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, important find method depend existing classification, since interested applying approach classification exist.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	next two sections, present unsupervised minimally supervised approaches problem.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	5.3 Unsupervised Feature Selection.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	order deal excessive dimensionality, Dash et al.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Unfortunately, promising method prove practical data.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	5.4 Semi-Supervised Feature Selection.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	domain particular, verb class discovery “in vacuum” necessary.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	model kind approach, selected sample five seed verbs class.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	set verbs judged (by authors’ intuition alone) “representative” class.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	extracted resulting decision trees union features used, formed reduced feature set task.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	5.5 Discussion.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	clustering experiments, find smaller subsets features generally perform better full set features.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	(See Table 3 number features Ling Seed sets.)	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, small set features adequate.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	number classes (a simple linear function roughly approximating number features Seed sets).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Interestingly, generally high, indicating structure data, matches classification.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	might also ask, would subset verbs well?	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	found mean values Seed set reported above, mean little lower.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, study used small set five features manually devised set three particular classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	question future research explore effect variables clustering performance.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	successful seed set features is, still achieve accuracy supervised learner.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	research needed definition general feature space, well methods selecting useful set features clustering.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Furthermore, might question clustering approach itself, context verb class discovery.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	indebted Allan Jepson helpful discussions suggestions.	0
Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Semi-supervised Verb Class Discovery Using Noisy Features	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	find unsupervised method tried cannot consistently applied data.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	avoiding dependence precise feature extraction, approach portable new languages.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, general feature space means features irrelevant given verb discrimination task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	unsupervised feature selection method, hand, usable data.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	remainder paper, first briefly review feature space present experimental classes verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	conclude discussion related work, contributions, future directions.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Thus, features serve approximations underlying distinctions among classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	allowable alternations expressions arguments vary according class verb.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	describe selection experimental classes verbs, estimation feature values.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	3.1 Verb Classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Benefactive versus Recipient verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Mary baked... cake Joan/Joan cake.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Mary gave... cake Joan/Joan cake.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	dative alternation verbs differ preposition semantic role object.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Admire versus Amuse verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	admire Jane.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Run versus Sound Emission verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	kids ran room./*The room ran kids.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Cheat versus Steal Remove verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	cheated...	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Jane money/*the money Jane.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	stole...	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	*Jane money/the money Jane.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	classes also assign semantic arguments, differ prepositional alternants.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Wipe versus Steal Remove verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Wipe... dust/the dust table/the table.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Steal... money/the money bank/*the bank.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	loaded... hay wagon/the wagon hay.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	filled...	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	*hay wagon/the wagon hay.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	put... hay wagon/*the wagon hay.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	three classes also assign semantic roles differ prepositional alternants.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Note, however, options Spray/Load verbs overlap two types verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	horse raced./The jockey raced horse.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	butter melted./The cook melted butter.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	(Note Object Drop verbs superset Benefactives above.)	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Dorr Jones, 1996).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Indicators PP usage thus useful definitive.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	1, 26.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	3 3 5 ci pi en 13.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	1, 13.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	3 2 7 Ad mi 31.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	2 3 5 us e 31.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	1 1 3 4 Ru n 51.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	3.2 7 9 un E mi ssi 43.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	2 5 6 C 10.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	6 2 9 St ea l ov e 10.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	5, 10.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	1 4 5 Wi pe 10.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	4.1 , 10.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	1– 4 1 6 9 bj ec Dr op 26.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	1, 26.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	3, 26.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	3.2 Verb Selection.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	experimental verbs selected follows.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Table 1 shows number verbs class end process.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	began set 20 verbs per class current work.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	3.3 Feature Extraction.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Since general corpus, expect strong overall domain bias verb usage.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	4.1 Clustering Parameters.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	used simple Euclidean distance former, Ward linkage latter.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	4.2 Evaluation Measures.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	use three separate evaluation measures, tap different properties clusterings.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	4.2.1 Accuracy assign cluster class label majority members.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	theoretical maximum is, course, 1.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	figures reported results Table 2 below.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, useful relative measure good-.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	ness, comparing clusterings arising different feature sets.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, since fix number clusters number classes, measure remains informative.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	3 experiments estimating baseline, in-.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	deed found mean value 0.00 random clusterings.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	higher (.89 vs. .33) reflects better separation data.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	regard target classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	value 0 suggests point clearly particular cluster.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	measure independent true classification, could high dependent measures low, vice versa.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	alternations.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	13-way task includes classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	third column Table 2 gives baseline calculated random clusterings.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Recall upper bound random performance.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	5.1 Full Feature Set.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Although generally higher baseline, well supervised learner, generally low.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	5.2 Manual Feature Selection.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	C5.0 supervised accuracy; Base random clusters.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	See text description.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	indicated class description given Levin.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	task, then, linguistically-relevant subset defined union subsets classes task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	2-way tasks, performance average close full feature set measures.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	performance comparison tentatively suggests good feature selection helpful task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, important find method depend existing classification, since interested applying approach classification exist.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	next two sections, present unsupervised minimally supervised approaches problem.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	5.3 Unsupervised Feature Selection.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	order deal excessive dimensionality, Dash et al.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Unfortunately, promising method prove practical data.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	5.4 Semi-Supervised Feature Selection.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	domain particular, verb class discovery “in vacuum” necessary.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	model kind approach, selected sample five seed verbs class.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	set verbs judged (by authors’ intuition alone) “representative” class.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	extracted resulting decision trees union features used, formed reduced feature set task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	5.5 Discussion.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	clustering experiments, find smaller subsets features generally perform better full set features.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	(See Table 3 number features Ling Seed sets.)	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, small set features adequate.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	number classes (a simple linear function roughly approximating number features Seed sets).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Interestingly, generally high, indicating structure data, matches classification.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	might also ask, would subset verbs well?	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	found mean values Seed set reported above, mean little lower.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, study used small set five features manually devised set three particular classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	question future research explore effect variables clustering performance.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	successful seed set features is, still achieve accuracy supervised learner.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	research needed definition general feature space, well methods selecting useful set features clustering.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Furthermore, might question clustering approach itself, context verb class discovery.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	indebted Allan Jepson helpful discussions suggestions.	0
Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Semi-supervised Verb Class Discovery Using Noisy Features	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	find unsupervised method tried cannot consistently applied data.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	avoiding dependence precise feature extraction, approach portable new languages.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, general feature space means features irrelevant given verb discrimination task.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	unsupervised feature selection method, hand, usable data.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	remainder paper, first briefly review feature space present experimental classes verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	conclude discussion related work, contributions, future directions.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Thus, features serve approximations underlying distinctions among classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	allowable alternations expressions arguments vary according class verb.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	describe selection experimental classes verbs, estimation feature values.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	3.1 Verb Classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Benefactive versus Recipient verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Mary baked... cake Joan/Joan cake.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Mary gave... cake Joan/Joan cake.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	dative alternation verbs differ preposition semantic role object.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Admire versus Amuse verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	admire Jane.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Run versus Sound Emission verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	kids ran room./*The room ran kids.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Cheat versus Steal Remove verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	cheated...	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Jane money/*the money Jane.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	stole...	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	*Jane money/the money Jane.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	classes also assign semantic arguments, differ prepositional alternants.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Wipe versus Steal Remove verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Wipe... dust/the dust table/the table.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Steal... money/the money bank/*the bank.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	loaded... hay wagon/the wagon hay.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	filled...	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	*hay wagon/the wagon hay.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	put... hay wagon/*the wagon hay.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	three classes also assign semantic roles differ prepositional alternants.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Note, however, options Spray/Load verbs overlap two types verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	horse raced./The jockey raced horse.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	butter melted./The cook melted butter.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	(Note Object Drop verbs superset Benefactives above.)	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Dorr Jones, 1996).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Indicators PP usage thus useful definitive.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	1, 26.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	3 3 5 ci pi en 13.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	1, 13.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	3 2 7 Ad mi 31.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	2 3 5 us e 31.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	1 1 3 4 Ru n 51.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	3.2 7 9 un E mi ssi 43.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	2 5 6 C 10.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	6 2 9 St ea l ov e 10.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	5, 10.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	1 4 5 Wi pe 10.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	4.1 , 10.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	1– 4 1 6 9 bj ec Dr op 26.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	1, 26.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	3, 26.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	3.2 Verb Selection.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	experimental verbs selected follows.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Table 1 shows number verbs class end process.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	began set 20 verbs per class current work.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	3.3 Feature Extraction.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Since general corpus, expect strong overall domain bias verb usage.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	4.1 Clustering Parameters.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	used simple Euclidean distance former, Ward linkage latter.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	experiments here, however, report results , since found principled way automatically determining good cutoff.	1
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	4.2 Evaluation Measures.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	use three separate evaluation measures, tap different properties clusterings.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	4.2.1 Accuracy assign cluster class label majority members.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	theoretical maximum is, course, 1.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	figures reported results Table 2 below.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, useful relative measure good-.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	ness, comparing clusterings arising different feature sets.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, since fix number clusters number classes, measure remains informative.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	3 experiments estimating baseline, in-.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	deed found mean value 0.00 random clusterings.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	higher (.89 vs. .33) reflects better separation data.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	regard target classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	value 0 suggests point clearly particular cluster.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	measure independent true classification, could high dependent measures low, vice versa.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	alternations.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	13-way task includes classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	third column Table 2 gives baseline calculated random clusterings.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Recall upper bound random performance.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	5.1 Full Feature Set.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Although generally higher baseline, well supervised learner, generally low.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	5.2 Manual Feature Selection.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	C5.0 supervised accuracy; Base random clusters.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	See text description.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	indicated class description given Levin.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	task, then, linguistically-relevant subset defined union subsets classes task.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	2-way tasks, performance average close full feature set measures.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	performance comparison tentatively suggests good feature selection helpful task.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, important find method depend existing classification, since interested applying approach classification exist.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	next two sections, present unsupervised minimally supervised approaches problem.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	5.3 Unsupervised Feature Selection.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	order deal excessive dimensionality, Dash et al.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Unfortunately, promising method prove practical data.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	5.4 Semi-Supervised Feature Selection.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	domain particular, verb class discovery “in vacuum” necessary.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	model kind approach, selected sample five seed verbs class.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	set verbs judged (by authors’ intuition alone) “representative” class.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	extracted resulting decision trees union features used, formed reduced feature set task.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	5.5 Discussion.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	clustering experiments, find smaller subsets features generally perform better full set features.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	(See Table 3 number features Ling Seed sets.)	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, small set features adequate.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	number classes (a simple linear function roughly approximating number features Seed sets).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Interestingly, generally high, indicating structure data, matches classification.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	might also ask, would subset verbs well?	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	found mean values Seed set reported above, mean little lower.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, study used small set five features manually devised set three particular classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	question future research explore effect variables clustering performance.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	successful seed set features is, still achieve accuracy supervised learner.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	research needed definition general feature space, well methods selecting useful set features clustering.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Furthermore, might question clustering approach itself, context verb class discovery.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	indebted Allan Jepson helpful discussions suggestions.	0
Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).  In addition, a significant amount of information is lost in pairwise clustering.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Semi-supervised Verb Class Discovery Using Noisy Features	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	find unsupervised method tried cannot consistently applied data.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	avoiding dependence precise feature extraction, approach portable new languages.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, general feature space means features irrelevant given verb discrimination task.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	1
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	unsupervised feature selection method, hand, usable data.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	remainder paper, first briefly review feature space present experimental classes verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	conclude discussion related work, contributions, future directions.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Thus, features serve approximations underlying distinctions among classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	allowable alternations expressions arguments vary according class verb.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	describe selection experimental classes verbs, estimation feature values.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	3.1 Verb Classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Benefactive versus Recipient verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Mary baked... cake Joan/Joan cake.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Mary gave... cake Joan/Joan cake.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	dative alternation verbs differ preposition semantic role object.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Admire versus Amuse verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	admire Jane.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Run versus Sound Emission verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	kids ran room./*The room ran kids.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Cheat versus Steal Remove verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	cheated...	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Jane money/*the money Jane.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	stole...	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	*Jane money/the money Jane.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	classes also assign semantic arguments, differ prepositional alternants.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Wipe versus Steal Remove verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Wipe... dust/the dust table/the table.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Steal... money/the money bank/*the bank.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	loaded... hay wagon/the wagon hay.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	filled...	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	*hay wagon/the wagon hay.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	put... hay wagon/*the wagon hay.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	three classes also assign semantic roles differ prepositional alternants.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Note, however, options Spray/Load verbs overlap two types verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	horse raced./The jockey raced horse.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	butter melted./The cook melted butter.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	(Note Object Drop verbs superset Benefactives above.)	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Dorr Jones, 1996).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Indicators PP usage thus useful definitive.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1, 26.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	3 3 5 ci pi en 13.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1, 13.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	3 2 7 Ad mi 31.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	2 3 5 us e 31.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1 1 3 4 Ru n 51.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	3.2 7 9 un E mi ssi 43.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	2 5 6 C 10.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	6 2 9 St ea l ov e 10.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	5, 10.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1 4 5 Wi pe 10.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	4.1 , 10.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1– 4 1 6 9 bj ec Dr op 26.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1, 26.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	3, 26.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	3.2 Verb Selection.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	experimental verbs selected follows.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Table 1 shows number verbs class end process.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	began set 20 verbs per class current work.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	3.3 Feature Extraction.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Since general corpus, expect strong overall domain bias verb usage.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	4.1 Clustering Parameters.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	used simple Euclidean distance former, Ward linkage latter.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	4.2 Evaluation Measures.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	use three separate evaluation measures, tap different properties clusterings.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	4.2.1 Accuracy assign cluster class label majority members.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	theoretical maximum is, course, 1.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	figures reported results Table 2 below.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, useful relative measure good-.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	ness, comparing clusterings arising different feature sets.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, since fix number clusters number classes, measure remains informative.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	3 experiments estimating baseline, in-.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	deed found mean value 0.00 random clusterings.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	higher (.89 vs. .33) reflects better separation data.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	regard target classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	value 0 suggests point clearly particular cluster.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	measure independent true classification, could high dependent measures low, vice versa.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	alternations.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	13-way task includes classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	third column Table 2 gives baseline calculated random clusterings.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Recall upper bound random performance.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	5.1 Full Feature Set.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Although generally higher baseline, well supervised learner, generally low.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	5.2 Manual Feature Selection.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	C5.0 supervised accuracy; Base random clusters.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	See text description.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	indicated class description given Levin.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	task, then, linguistically-relevant subset defined union subsets classes task.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	2-way tasks, performance average close full feature set measures.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	performance comparison tentatively suggests good feature selection helpful task.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, important find method depend existing classification, since interested applying approach classification exist.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	next two sections, present unsupervised minimally supervised approaches problem.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	5.3 Unsupervised Feature Selection.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	order deal excessive dimensionality, Dash et al.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Unfortunately, promising method prove practical data.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	5.4 Semi-Supervised Feature Selection.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	domain particular, verb class discovery “in vacuum” necessary.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	model kind approach, selected sample five seed verbs class.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	set verbs judged (by authors’ intuition alone) “representative” class.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	extracted resulting decision trees union features used, formed reduced feature set task.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	5.5 Discussion.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	clustering experiments, find smaller subsets features generally perform better full set features.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	(See Table 3 number features Ling Seed sets.)	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, small set features adequate.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	number classes (a simple linear function roughly approximating number features Seed sets).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Interestingly, generally high, indicating structure data, matches classification.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	might also ask, would subset verbs well?	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	found mean values Seed set reported above, mean little lower.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, study used small set five features manually devised set three particular classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	question future research explore effect variables clustering performance.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	successful seed set features is, still achieve accuracy supervised learner.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	research needed definition general feature space, well methods selecting useful set features clustering.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Furthermore, might question clustering approach itself, context verb class discovery.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	indebted Allan Jepson helpful discussions suggestions.	0
Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Semi-supervised Verb Class Discovery Using Noisy Features	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	find unsupervised method tried cannot consistently applied data.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	avoiding dependence precise feature extraction, approach portable new languages.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, general feature space means features irrelevant given verb discrimination task.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	1
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	unsupervised feature selection method, hand, usable data.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	remainder paper, first briefly review feature space present experimental classes verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	conclude discussion related work, contributions, future directions.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Thus, features serve approximations underlying distinctions among classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	allowable alternations expressions arguments vary according class verb.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	describe selection experimental classes verbs, estimation feature values.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	3.1 Verb Classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Benefactive versus Recipient verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Mary baked... cake Joan/Joan cake.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Mary gave... cake Joan/Joan cake.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	dative alternation verbs differ preposition semantic role object.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Admire versus Amuse verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	admire Jane.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Run versus Sound Emission verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	kids ran room./*The room ran kids.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Cheat versus Steal Remove verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	cheated...	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Jane money/*the money Jane.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	stole...	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	*Jane money/the money Jane.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	classes also assign semantic arguments, differ prepositional alternants.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Wipe versus Steal Remove verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Wipe... dust/the dust table/the table.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Steal... money/the money bank/*the bank.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	loaded... hay wagon/the wagon hay.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	filled...	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	*hay wagon/the wagon hay.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	put... hay wagon/*the wagon hay.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	three classes also assign semantic roles differ prepositional alternants.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Note, however, options Spray/Load verbs overlap two types verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	horse raced./The jockey raced horse.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	butter melted./The cook melted butter.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	(Note Object Drop verbs superset Benefactives above.)	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Dorr Jones, 1996).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Indicators PP usage thus useful definitive.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	1, 26.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	3 3 5 ci pi en 13.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	1, 13.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	3 2 7 Ad mi 31.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	2 3 5 us e 31.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	1 1 3 4 Ru n 51.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	3.2 7 9 un E mi ssi 43.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	2 5 6 C 10.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	6 2 9 St ea l ov e 10.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	5, 10.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	1 4 5 Wi pe 10.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	4.1 , 10.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	1– 4 1 6 9 bj ec Dr op 26.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	1, 26.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	3, 26.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	3.2 Verb Selection.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	experimental verbs selected follows.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Table 1 shows number verbs class end process.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	began set 20 verbs per class current work.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	3.3 Feature Extraction.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Since general corpus, expect strong overall domain bias verb usage.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	4.1 Clustering Parameters.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	used simple Euclidean distance former, Ward linkage latter.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	4.2 Evaluation Measures.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	use three separate evaluation measures, tap different properties clusterings.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	4.2.1 Accuracy assign cluster class label majority members.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	theoretical maximum is, course, 1.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	figures reported results Table 2 below.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, useful relative measure good-.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	ness, comparing clusterings arising different feature sets.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, since fix number clusters number classes, measure remains informative.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	3 experiments estimating baseline, in-.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	deed found mean value 0.00 random clusterings.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	higher (.89 vs. .33) reflects better separation data.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	regard target classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	value 0 suggests point clearly particular cluster.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	measure independent true classification, could high dependent measures low, vice versa.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	alternations.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	13-way task includes classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	third column Table 2 gives baseline calculated random clusterings.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Recall upper bound random performance.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	5.1 Full Feature Set.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Although generally higher baseline, well supervised learner, generally low.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	5.2 Manual Feature Selection.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	C5.0 supervised accuracy; Base random clusters.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	See text description.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	indicated class description given Levin.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	task, then, linguistically-relevant subset defined union subsets classes task.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	2-way tasks, performance average close full feature set measures.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	performance comparison tentatively suggests good feature selection helpful task.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, important find method depend existing classification, since interested applying approach classification exist.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	next two sections, present unsupervised minimally supervised approaches problem.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	5.3 Unsupervised Feature Selection.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	order deal excessive dimensionality, Dash et al.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Unfortunately, promising method prove practical data.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	5.4 Semi-Supervised Feature Selection.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	domain particular, verb class discovery “in vacuum” necessary.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	model kind approach, selected sample five seed verbs class.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	set verbs judged (by authors’ intuition alone) “representative” class.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	extracted resulting decision trees union features used, formed reduced feature set task.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	5.5 Discussion.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	clustering experiments, find smaller subsets features generally perform better full set features.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	(See Table 3 number features Ling Seed sets.)	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, small set features adequate.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	number classes (a simple linear function roughly approximating number features Seed sets).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Interestingly, generally high, indicating structure data, matches classification.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	might also ask, would subset verbs well?	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	found mean values Seed set reported above, mean little lower.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, study used small set five features manually devised set three particular classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	question future research explore effect variables clustering performance.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	successful seed set features is, still achieve accuracy supervised learner.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	research needed definition general feature space, well methods selecting useful set features clustering.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Furthermore, might question clustering approach itself, context verb class discovery.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	indebted Allan Jepson helpful discussions suggestions.	0
Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Semi-supervised Verb Class Discovery Using Noisy Features	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	find unsupervised method tried cannot consistently applied data.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	avoiding dependence precise feature extraction, approach portable new languages.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, general feature space means features irrelevant given verb discrimination task.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	unsupervised feature selection method, hand, usable data.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	remainder paper, first briefly review feature space present experimental classes verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	conclude discussion related work, contributions, future directions.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Thus, features serve approximations underlying distinctions among classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	allowable alternations expressions arguments vary according class verb.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	describe selection experimental classes verbs, estimation feature values.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	3.1 Verb Classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Benefactive versus Recipient verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Mary baked... cake Joan/Joan cake.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Mary gave... cake Joan/Joan cake.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	dative alternation verbs differ preposition semantic role object.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Admire versus Amuse verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	admire Jane.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Run versus Sound Emission verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	kids ran room./*The room ran kids.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Cheat versus Steal Remove verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	cheated...	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Jane money/*the money Jane.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	stole...	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	*Jane money/the money Jane.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	classes also assign semantic arguments, differ prepositional alternants.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Wipe versus Steal Remove verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Wipe... dust/the dust table/the table.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Steal... money/the money bank/*the bank.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	loaded... hay wagon/the wagon hay.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	filled...	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	*hay wagon/the wagon hay.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	put... hay wagon/*the wagon hay.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	three classes also assign semantic roles differ prepositional alternants.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Note, however, options Spray/Load verbs overlap two types verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	horse raced./The jockey raced horse.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	butter melted./The cook melted butter.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	(Note Object Drop verbs superset Benefactives above.)	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Dorr Jones, 1996).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Indicators PP usage thus useful definitive.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	1, 26.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	3 3 5 ci pi en 13.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	1, 13.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	3 2 7 Ad mi 31.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	2 3 5 us e 31.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	1 1 3 4 Ru n 51.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	3.2 7 9 un E mi ssi 43.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	2 5 6 C 10.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	6 2 9 St ea l ov e 10.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	5, 10.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	1 4 5 Wi pe 10.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	4.1 , 10.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	1– 4 1 6 9 bj ec Dr op 26.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	1, 26.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	3, 26.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	1
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	3.2 Verb Selection.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	experimental verbs selected follows.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Table 1 shows number verbs class end process.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	began set 20 verbs per class current work.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	3.3 Feature Extraction.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Since general corpus, expect strong overall domain bias verb usage.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	4.1 Clustering Parameters.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	used simple Euclidean distance former, Ward linkage latter.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	4.2 Evaluation Measures.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	use three separate evaluation measures, tap different properties clusterings.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	4.2.1 Accuracy assign cluster class label majority members.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	theoretical maximum is, course, 1.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	figures reported results Table 2 below.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, useful relative measure good-.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	ness, comparing clusterings arising different feature sets.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, since fix number clusters number classes, measure remains informative.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	3 experiments estimating baseline, in-.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	deed found mean value 0.00 random clusterings.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	higher (.89 vs. .33) reflects better separation data.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	regard target classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	value 0 suggests point clearly particular cluster.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	measure independent true classification, could high dependent measures low, vice versa.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	alternations.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	13-way task includes classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	third column Table 2 gives baseline calculated random clusterings.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Recall upper bound random performance.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	5.1 Full Feature Set.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Although generally higher baseline, well supervised learner, generally low.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	5.2 Manual Feature Selection.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	C5.0 supervised accuracy; Base random clusters.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	See text description.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	indicated class description given Levin.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	task, then, linguistically-relevant subset defined union subsets classes task.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	2-way tasks, performance average close full feature set measures.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	performance comparison tentatively suggests good feature selection helpful task.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, important find method depend existing classification, since interested applying approach classification exist.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	next two sections, present unsupervised minimally supervised approaches problem.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	5.3 Unsupervised Feature Selection.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	order deal excessive dimensionality, Dash et al.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Unfortunately, promising method prove practical data.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	5.4 Semi-Supervised Feature Selection.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	domain particular, verb class discovery “in vacuum” necessary.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	model kind approach, selected sample five seed verbs class.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	set verbs judged (by authors’ intuition alone) “representative” class.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	extracted resulting decision trees union features used, formed reduced feature set task.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	5.5 Discussion.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	clustering experiments, find smaller subsets features generally perform better full set features.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	(See Table 3 number features Ling Seed sets.)	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, small set features adequate.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	number classes (a simple linear function roughly approximating number features Seed sets).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Interestingly, generally high, indicating structure data, matches classification.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	might also ask, would subset verbs well?	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	found mean values Seed set reported above, mean little lower.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, study used small set five features manually devised set three particular classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	question future research explore effect variables clustering performance.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	successful seed set features is, still achieve accuracy supervised learner.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	research needed definition general feature space, well methods selecting useful set features clustering.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Furthermore, might question clustering approach itself, context verb class discovery.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	indebted Allan Jepson helpful discussions suggestions.	0
In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Semi-supervised Verb Class Discovery Using Noisy Features	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	find unsupervised method tried cannot consistently applied data.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	avoiding dependence precise feature extraction, approach portable new languages.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, general feature space means features irrelevant given verb discrimination task.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	unsupervised feature selection method, hand, usable data.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	remainder paper, first briefly review feature space present experimental classes verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	conclude discussion related work, contributions, future directions.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Thus, features serve approximations underlying distinctions among classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	allowable alternations expressions arguments vary according class verb.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	describe selection experimental classes verbs, estimation feature values.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	3.1 Verb Classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Benefactive versus Recipient verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Mary baked... cake Joan/Joan cake.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Mary gave... cake Joan/Joan cake.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	dative alternation verbs differ preposition semantic role object.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Admire versus Amuse verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	admire Jane.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Run versus Sound Emission verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	kids ran room./*The room ran kids.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Cheat versus Steal Remove verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	cheated...	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Jane money/*the money Jane.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	stole...	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	*Jane money/the money Jane.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	classes also assign semantic arguments, differ prepositional alternants.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Wipe versus Steal Remove verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Wipe... dust/the dust table/the table.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Steal... money/the money bank/*the bank.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	loaded... hay wagon/the wagon hay.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	filled...	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	*hay wagon/the wagon hay.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	put... hay wagon/*the wagon hay.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	three classes also assign semantic roles differ prepositional alternants.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Note, however, options Spray/Load verbs overlap two types verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	horse raced./The jockey raced horse.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	butter melted./The cook melted butter.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	(Note Object Drop verbs superset Benefactives above.)	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Dorr Jones, 1996).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Indicators PP usage thus useful definitive.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	1, 26.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	3 3 5 ci pi en 13.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	1, 13.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	3 2 7 Ad mi 31.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	2 3 5 us e 31.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	1 1 3 4 Ru n 51.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	3.2 7 9 un E mi ssi 43.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	2 5 6 C 10.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	6 2 9 St ea l ov e 10.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	5, 10.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	1 4 5 Wi pe 10.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	4.1 , 10.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	1– 4 1 6 9 bj ec Dr op 26.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	1, 26.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	3, 26.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	3.2 Verb Selection.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	experimental verbs selected follows.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Table 1 shows number verbs class end process.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	began set 20 verbs per class current work.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	3.3 Feature Extraction.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Since general corpus, expect strong overall domain bias verb usage.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	4.1 Clustering Parameters.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	used simple Euclidean distance former, Ward linkage latter.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	4.2 Evaluation Measures.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	use three separate evaluation measures, tap different properties clusterings.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	4.2.1 Accuracy assign cluster class label majority members.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	theoretical maximum is, course, 1.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	figures reported results Table 2 below.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, useful relative measure good-.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	ness, comparing clusterings arising different feature sets.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	1
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, since fix number clusters number classes, measure remains informative.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	3 experiments estimating baseline, in-.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	deed found mean value 0.00 random clusterings.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	higher (.89 vs. .33) reflects better separation data.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	regard target classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	value 0 suggests point clearly particular cluster.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	measure independent true classification, could high dependent measures low, vice versa.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	alternations.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	13-way task includes classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	third column Table 2 gives baseline calculated random clusterings.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Recall upper bound random performance.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	5.1 Full Feature Set.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Although generally higher baseline, well supervised learner, generally low.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	5.2 Manual Feature Selection.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	C5.0 supervised accuracy; Base random clusters.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	See text description.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	indicated class description given Levin.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	task, then, linguistically-relevant subset defined union subsets classes task.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	2-way tasks, performance average close full feature set measures.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	performance comparison tentatively suggests good feature selection helpful task.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, important find method depend existing classification, since interested applying approach classification exist.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	next two sections, present unsupervised minimally supervised approaches problem.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	5.3 Unsupervised Feature Selection.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	order deal excessive dimensionality, Dash et al.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Unfortunately, promising method prove practical data.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	5.4 Semi-Supervised Feature Selection.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	domain particular, verb class discovery “in vacuum” necessary.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	model kind approach, selected sample five seed verbs class.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	set verbs judged (by authors’ intuition alone) “representative” class.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	extracted resulting decision trees union features used, formed reduced feature set task.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	5.5 Discussion.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	clustering experiments, find smaller subsets features generally perform better full set features.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	(See Table 3 number features Ling Seed sets.)	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, small set features adequate.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	number classes (a simple linear function roughly approximating number features Seed sets).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Interestingly, generally high, indicating structure data, matches classification.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	might also ask, would subset verbs well?	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	found mean values Seed set reported above, mean little lower.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, study used small set five features manually devised set three particular classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	question future research explore effect variables clustering performance.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	successful seed set features is, still achieve accuracy supervised learner.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	research needed definition general feature space, well methods selecting useful set features clustering.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Furthermore, might question clustering approach itself, context verb class discovery.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	indebted Allan Jepson helpful discussions suggestions.	0
For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Semi-supervised Verb Class Discovery Using Noisy Features	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	find unsupervised method tried cannot consistently applied data.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	avoiding dependence precise feature extraction, approach portable new languages.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, general feature space means features irrelevant given verb discrimination task.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	unsupervised feature selection method, hand, usable data.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	remainder paper, first briefly review feature space present experimental classes verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	conclude discussion related work, contributions, future directions.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Thus, features serve approximations underlying distinctions among classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	allowable alternations expressions arguments vary according class verb.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	describe selection experimental classes verbs, estimation feature values.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	3.1 Verb Classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Benefactive versus Recipient verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Mary baked... cake Joan/Joan cake.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Mary gave... cake Joan/Joan cake.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	dative alternation verbs differ preposition semantic role object.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Admire versus Amuse verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	admire Jane.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Run versus Sound Emission verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	kids ran room./*The room ran kids.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Cheat versus Steal Remove verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	cheated...	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Jane money/*the money Jane.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	stole...	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	*Jane money/the money Jane.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	classes also assign semantic arguments, differ prepositional alternants.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Wipe versus Steal Remove verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Wipe... dust/the dust table/the table.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Steal... money/the money bank/*the bank.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	loaded... hay wagon/the wagon hay.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	filled...	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	*hay wagon/the wagon hay.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	put... hay wagon/*the wagon hay.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	three classes also assign semantic roles differ prepositional alternants.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Note, however, options Spray/Load verbs overlap two types verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	horse raced./The jockey raced horse.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	butter melted./The cook melted butter.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	(Note Object Drop verbs superset Benefactives above.)	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Dorr Jones, 1996).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Indicators PP usage thus useful definitive.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	1, 26.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	3 3 5 ci pi en 13.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	1, 13.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	3 2 7 Ad mi 31.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	2 3 5 us e 31.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	1 1 3 4 Ru n 51.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	3.2 7 9 un E mi ssi 43.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	2 5 6 C 10.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	6 2 9 St ea l ov e 10.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	5, 10.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	1 4 5 Wi pe 10.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	4.1 , 10.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	1– 4 1 6 9 bj ec Dr op 26.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	1, 26.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	3, 26.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	3.2 Verb Selection.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	experimental verbs selected follows.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Table 1 shows number verbs class end process.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	began set 20 verbs per class current work.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	3.3 Feature Extraction.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Since general corpus, expect strong overall domain bias verb usage.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	4.1 Clustering Parameters.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	used simple Euclidean distance former, Ward linkage latter.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	4.2 Evaluation Measures.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	use three separate evaluation measures, tap different properties clusterings.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	4.2.1 Accuracy assign cluster class label majority members.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	theoretical maximum is, course, 1.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	figures reported results Table 2 below.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, useful relative measure good-.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	ness, comparing clusterings arising different feature sets.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, since fix number clusters number classes, measure remains informative.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	3 experiments estimating baseline, in-.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	deed found mean value 0.00 random clusterings.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	higher (.89 vs. .33) reflects better separation data.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	regard target classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	value 0 suggests point clearly particular cluster.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	measure independent true classification, could high dependent measures low, vice versa.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	alternations.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	13-way task includes classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	third column Table 2 gives baseline calculated random clusterings.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Recall upper bound random performance.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	5.1 Full Feature Set.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Although generally higher baseline, well supervised learner, generally low.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	5.2 Manual Feature Selection.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	C5.0 supervised accuracy; Base random clusters.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	See text description.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	indicated class description given Levin.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	task, then, linguistically-relevant subset defined union subsets classes task.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	2-way tasks, performance average close full feature set measures.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	performance comparison tentatively suggests good feature selection helpful task.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, important find method depend existing classification, since interested applying approach classification exist.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	next two sections, present unsupervised minimally supervised approaches problem.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	5.3 Unsupervised Feature Selection.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	order deal excessive dimensionality, Dash et al.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Unfortunately, promising method prove practical data.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	5.4 Semi-Supervised Feature Selection.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	domain particular, verb class discovery “in vacuum” necessary.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	model kind approach, selected sample five seed verbs class.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	set verbs judged (by authors’ intuition alone) “representative” class.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	extracted resulting decision trees union features used, formed reduced feature set task.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	5.5 Discussion.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	clustering experiments, find smaller subsets features generally perform better full set features.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	(See Table 3 number features Ling Seed sets.)	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, small set features adequate.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	number classes (a simple linear function roughly approximating number features Seed sets).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Interestingly, generally high, indicating structure data, matches classification.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	might also ask, would subset verbs well?	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	found mean values Seed set reported above, mean little lower.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, study used small set five features manually devised set three particular classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	question future research explore effect variables clustering performance.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	successful seed set features is, still achieve accuracy supervised learner.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	research needed definition general feature space, well methods selecting useful set features clustering.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Furthermore, might question clustering approach itself, context verb class discovery.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	indebted Allan Jepson helpful discussions suggestions.	0
In recent work, Stevenson and Joanis (2003) compared their supervised method for verb classification with semisupervised and unsupervised techniques.  In these experiments, they enlarged the number of gold standard English verb classes to 14 classes related to Levin classes, with a total of 841 verbs.  Low- frequency and ambiguous verbs were excluded from the classes.  They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Semi-supervised Verb Class Discovery Using Noisy Features	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	find unsupervised method tried cannot consistently applied data.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	avoiding dependence precise feature extraction, approach portable new languages.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, general feature space means features irrelevant given verb discrimination task.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	unsupervised feature selection method, hand, usable data.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	remainder paper, first briefly review feature space present experimental classes verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	conclude discussion related work, contributions, future directions.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Thus, features serve approximations underlying distinctions among classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	allowable alternations expressions arguments vary according class verb.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	describe selection experimental classes verbs, estimation feature values.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	3.1 Verb Classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Benefactive versus Recipient verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Mary baked... cake Joan/Joan cake.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Mary gave... cake Joan/Joan cake.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	dative alternation verbs differ preposition semantic role object.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Admire versus Amuse verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	admire Jane.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Run versus Sound Emission verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	kids ran room./*The room ran kids.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Cheat versus Steal Remove verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	cheated...	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Jane money/*the money Jane.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	stole...	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	*Jane money/the money Jane.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	classes also assign semantic arguments, differ prepositional alternants.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Wipe versus Steal Remove verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Wipe... dust/the dust table/the table.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Steal... money/the money bank/*the bank.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	loaded... hay wagon/the wagon hay.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	filled...	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	*hay wagon/the wagon hay.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	put... hay wagon/*the wagon hay.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	three classes also assign semantic roles differ prepositional alternants.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Note, however, options Spray/Load verbs overlap two types verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	horse raced./The jockey raced horse.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	butter melted./The cook melted butter.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	(Note Object Drop verbs superset Benefactives above.)	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Dorr Jones, 1996).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Indicators PP usage thus useful definitive.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	1, 26.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	3 3 5 ci pi en 13.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	1, 13.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	3 2 7 Ad mi 31.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	2 3 5 us e 31.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	1 1 3 4 Ru n 51.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	3.2 7 9 un E mi ssi 43.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	2 5 6 C 10.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	6 2 9 St ea l ov e 10.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	5, 10.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	1 4 5 Wi pe 10.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	4.1 , 10.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	1– 4 1 6 9 bj ec Dr op 26.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	1, 26.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	3, 26.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	3.2 Verb Selection.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	experimental verbs selected follows.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Table 1 shows number verbs class end process.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	began set 20 verbs per class current work.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	3.3 Feature Extraction.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Since general corpus, expect strong overall domain bias verb usage.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	4.1 Clustering Parameters.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	used simple Euclidean distance former, Ward linkage latter.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	4.2 Evaluation Measures.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	use three separate evaluation measures, tap different properties clusterings.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	4.2.1 Accuracy assign cluster class label majority members.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	1
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	theoretical maximum is, course, 1.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	figures reported results Table 2 below.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, useful relative measure good-.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	ness, comparing clusterings arising different feature sets.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, since fix number clusters number classes, measure remains informative.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	3 experiments estimating baseline, in-.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	deed found mean value 0.00 random clusterings.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	higher (.89 vs. .33) reflects better separation data.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	regard target classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	value 0 suggests point clearly particular cluster.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	measure independent true classification, could high dependent measures low, vice versa.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	alternations.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	13-way task includes classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	third column Table 2 gives baseline calculated random clusterings.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Recall upper bound random performance.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	5.1 Full Feature Set.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Although generally higher baseline, well supervised learner, generally low.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	5.2 Manual Feature Selection.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	C5.0 supervised accuracy; Base random clusters.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	See text description.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	indicated class description given Levin.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	task, then, linguistically-relevant subset defined union subsets classes task.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	2-way tasks, performance average close full feature set measures.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	performance comparison tentatively suggests good feature selection helpful task.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, important find method depend existing classification, since interested applying approach classification exist.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	next two sections, present unsupervised minimally supervised approaches problem.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	5.3 Unsupervised Feature Selection.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	order deal excessive dimensionality, Dash et al.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Unfortunately, promising method prove practical data.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	5.4 Semi-Supervised Feature Selection.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	domain particular, verb class discovery “in vacuum” necessary.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	model kind approach, selected sample five seed verbs class.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	set verbs judged (by authors’ intuition alone) “representative” class.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	extracted resulting decision trees union features used, formed reduced feature set task.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	5.5 Discussion.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	clustering experiments, find smaller subsets features generally perform better full set features.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	(See Table 3 number features Ling Seed sets.)	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, small set features adequate.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	number classes (a simple linear function roughly approximating number features Seed sets).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Interestingly, generally high, indicating structure data, matches classification.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	might also ask, would subset verbs well?	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	found mean values Seed set reported above, mean little lower.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, study used small set five features manually devised set three particular classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	question future research explore effect variables clustering performance.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	successful seed set features is, still achieve accuracy supervised learner.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	research needed definition general feature space, well methods selecting useful set features clustering.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Furthermore, might question clustering approach itself, context verb class discovery.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	indebted Allan Jepson helpful discussions suggestions.	0
Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Semi-supervised Verb Class Discovery Using Noisy Features	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	find unsupervised method tried cannot consistently applied data.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	avoiding dependence precise feature extraction, approach portable new languages.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, general feature space means features irrelevant given verb discrimination task.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	unsupervised feature selection method, hand, usable data.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	remainder paper, first briefly review feature space present experimental classes verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	conclude discussion related work, contributions, future directions.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Thus, features serve approximations underlying distinctions among classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	allowable alternations expressions arguments vary according class verb.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	describe selection experimental classes verbs, estimation feature values.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	3.1 Verb Classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Benefactive versus Recipient verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Mary baked... cake Joan/Joan cake.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Mary gave... cake Joan/Joan cake.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	dative alternation verbs differ preposition semantic role object.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Admire versus Amuse verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	admire Jane.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Run versus Sound Emission verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	kids ran room./*The room ran kids.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Cheat versus Steal Remove verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	cheated...	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Jane money/*the money Jane.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	stole...	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	*Jane money/the money Jane.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	classes also assign semantic arguments, differ prepositional alternants.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Wipe versus Steal Remove verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Wipe... dust/the dust table/the table.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Steal... money/the money bank/*the bank.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	loaded... hay wagon/the wagon hay.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	filled...	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	*hay wagon/the wagon hay.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	put... hay wagon/*the wagon hay.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	three classes also assign semantic roles differ prepositional alternants.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Note, however, options Spray/Load verbs overlap two types verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	horse raced./The jockey raced horse.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	butter melted./The cook melted butter.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	(Note Object Drop verbs superset Benefactives above.)	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Dorr Jones, 1996).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Indicators PP usage thus useful definitive.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	1, 26.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	3 3 5 ci pi en 13.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	1, 13.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	3 2 7 Ad mi 31.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	2 3 5 us e 31.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	1 1 3 4 Ru n 51.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	3.2 7 9 un E mi ssi 43.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	2 5 6 C 10.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	6 2 9 St ea l ov e 10.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	5, 10.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	1 4 5 Wi pe 10.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	4.1 , 10.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	1– 4 1 6 9 bj ec Dr op 26.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	1, 26.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	3, 26.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	3.2 Verb Selection.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	experimental verbs selected follows.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Table 1 shows number verbs class end process.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	began set 20 verbs per class current work.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	3.3 Feature Extraction.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Since general corpus, expect strong overall domain bias verb usage.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	4.1 Clustering Parameters.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	used simple Euclidean distance former, Ward linkage latter.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	4.2 Evaluation Measures.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	use three separate evaluation measures, tap different properties clusterings.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	4.2.1 Accuracy assign cluster class label majority members.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	1
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	theoretical maximum is, course, 1.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	figures reported results Table 2 below.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, useful relative measure good-.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	ness, comparing clusterings arising different feature sets.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, since fix number clusters number classes, measure remains informative.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	3 experiments estimating baseline, in-.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	deed found mean value 0.00 random clusterings.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	higher (.89 vs. .33) reflects better separation data.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	regard target classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	value 0 suggests point clearly particular cluster.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	measure independent true classification, could high dependent measures low, vice versa.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	alternations.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	13-way task includes classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	third column Table 2 gives baseline calculated random clusterings.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Recall upper bound random performance.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	5.1 Full Feature Set.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Although generally higher baseline, well supervised learner, generally low.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	5.2 Manual Feature Selection.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	C5.0 supervised accuracy; Base random clusters.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	See text description.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	indicated class description given Levin.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	task, then, linguistically-relevant subset defined union subsets classes task.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	2-way tasks, performance average close full feature set measures.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	performance comparison tentatively suggests good feature selection helpful task.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, important find method depend existing classification, since interested applying approach classification exist.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	next two sections, present unsupervised minimally supervised approaches problem.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	5.3 Unsupervised Feature Selection.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	order deal excessive dimensionality, Dash et al.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Unfortunately, promising method prove practical data.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	5.4 Semi-Supervised Feature Selection.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	domain particular, verb class discovery “in vacuum” necessary.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	model kind approach, selected sample five seed verbs class.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	set verbs judged (by authors’ intuition alone) “representative” class.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	extracted resulting decision trees union features used, formed reduced feature set task.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	5.5 Discussion.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	clustering experiments, find smaller subsets features generally perform better full set features.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	(See Table 3 number features Ling Seed sets.)	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, small set features adequate.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	number classes (a simple linear function roughly approximating number features Seed sets).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Interestingly, generally high, indicating structure data, matches classification.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	might also ask, would subset verbs well?	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	found mean values Seed set reported above, mean little lower.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, study used small set five features manually devised set three particular classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	question future research explore effect variables clustering performance.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	successful seed set features is, still achieve accuracy supervised learner.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	research needed definition general feature space, well methods selecting useful set features clustering.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Furthermore, might question clustering approach itself, context verb class discovery.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	indebted Allan Jepson helpful discussions suggestions.	0
Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Semi-supervised Verb Class Discovery Using Noisy Features	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	find unsupervised method tried cannot consistently applied data.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	avoiding dependence precise feature extraction, approach portable new languages.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, general feature space means features irrelevant given verb discrimination task.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	unsupervised feature selection method, hand, usable data.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	remainder paper, first briefly review feature space present experimental classes verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	conclude discussion related work, contributions, future directions.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Thus, features serve approximations underlying distinctions among classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	allowable alternations expressions arguments vary according class verb.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	describe selection experimental classes verbs, estimation feature values.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	3.1 Verb Classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Benefactive versus Recipient verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Mary baked... cake Joan/Joan cake.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Mary gave... cake Joan/Joan cake.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	dative alternation verbs differ preposition semantic role object.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Admire versus Amuse verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	admire Jane.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Run versus Sound Emission verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	kids ran room./*The room ran kids.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Cheat versus Steal Remove verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	cheated...	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Jane money/*the money Jane.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	stole...	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	*Jane money/the money Jane.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	classes also assign semantic arguments, differ prepositional alternants.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Wipe versus Steal Remove verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Wipe... dust/the dust table/the table.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Steal... money/the money bank/*the bank.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	loaded... hay wagon/the wagon hay.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	filled...	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	*hay wagon/the wagon hay.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	put... hay wagon/*the wagon hay.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	three classes also assign semantic roles differ prepositional alternants.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Note, however, options Spray/Load verbs overlap two types verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	horse raced./The jockey raced horse.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	butter melted./The cook melted butter.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	(Note Object Drop verbs superset Benefactives above.)	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Dorr Jones, 1996).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Indicators PP usage thus useful definitive.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	1, 26.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	3 3 5 ci pi en 13.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	1, 13.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	3 2 7 Ad mi 31.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	2 3 5 us e 31.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	1 1 3 4 Ru n 51.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	3.2 7 9 un E mi ssi 43.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	2 5 6 C 10.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	6 2 9 St ea l ov e 10.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	5, 10.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	1 4 5 Wi pe 10.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	4.1 , 10.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	1– 4 1 6 9 bj ec Dr op 26.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	1, 26.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	3, 26.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	3.2 Verb Selection.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	experimental verbs selected follows.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Table 1 shows number verbs class end process.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	began set 20 verbs per class current work.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	3.3 Feature Extraction.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Since general corpus, expect strong overall domain bias verb usage.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	4.1 Clustering Parameters.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	used simple Euclidean distance former, Ward linkage latter.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	4.2 Evaluation Measures.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	use three separate evaluation measures, tap different properties clusterings.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	4.2.1 Accuracy assign cluster class label majority members.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	theoretical maximum is, course, 1.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	figures reported results Table 2 below.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, useful relative measure good-.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	ness, comparing clusterings arising different feature sets.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, since fix number clusters number classes, measure remains informative.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	3 experiments estimating baseline, in-.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	deed found mean value 0.00 random clusterings.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	higher (.89 vs. .33) reflects better separation data.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	regard target classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	value 0 suggests point clearly particular cluster.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	measure independent true classification, could high dependent measures low, vice versa.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	alternations.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	13-way task includes classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	third column Table 2 gives baseline calculated random clusterings.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Recall upper bound random performance.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	5.1 Full Feature Set.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Although generally higher baseline, well supervised learner, generally low.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	5.2 Manual Feature Selection.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	1
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	C5.0 supervised accuracy; Base random clusters.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	See text description.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	indicated class description given Levin.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	task, then, linguistically-relevant subset defined union subsets classes task.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	2-way tasks, performance average close full feature set measures.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	performance comparison tentatively suggests good feature selection helpful task.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, important find method depend existing classification, since interested applying approach classification exist.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	next two sections, present unsupervised minimally supervised approaches problem.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	5.3 Unsupervised Feature Selection.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	order deal excessive dimensionality, Dash et al.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Unfortunately, promising method prove practical data.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	5.4 Semi-Supervised Feature Selection.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	domain particular, verb class discovery “in vacuum” necessary.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	model kind approach, selected sample five seed verbs class.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	set verbs judged (by authors’ intuition alone) “representative” class.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	extracted resulting decision trees union features used, formed reduced feature set task.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	5.5 Discussion.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	clustering experiments, find smaller subsets features generally perform better full set features.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	(See Table 3 number features Ling Seed sets.)	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, small set features adequate.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	number classes (a simple linear function roughly approximating number features Seed sets).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Interestingly, generally high, indicating structure data, matches classification.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	might also ask, would subset verbs well?	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	found mean values Seed set reported above, mean little lower.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, study used small set five features manually devised set three particular classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	question future research explore effect variables clustering performance.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	successful seed set features is, still achieve accuracy supervised learner.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	research needed definition general feature space, well methods selecting useful set features clustering.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Furthermore, might question clustering approach itself, context verb class discovery.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	indebted Allan Jepson helpful discussions suggestions.	0
For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Semi-supervised Verb Class Discovery Using Noisy Features	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	find unsupervised method tried cannot consistently applied data.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	1
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	avoiding dependence precise feature extraction, approach portable new languages.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, general feature space means features irrelevant given verb discrimination task.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	unsupervised feature selection method, hand, usable data.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	remainder paper, first briefly review feature space present experimental classes verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	conclude discussion related work, contributions, future directions.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Thus, features serve approximations underlying distinctions among classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	allowable alternations expressions arguments vary according class verb.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	describe selection experimental classes verbs, estimation feature values.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	3.1 Verb Classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Benefactive versus Recipient verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Mary baked... cake Joan/Joan cake.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Mary gave... cake Joan/Joan cake.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	dative alternation verbs differ preposition semantic role object.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Admire versus Amuse verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	admire Jane.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Run versus Sound Emission verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	kids ran room./*The room ran kids.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Cheat versus Steal Remove verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	cheated...	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Jane money/*the money Jane.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	stole...	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	*Jane money/the money Jane.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	classes also assign semantic arguments, differ prepositional alternants.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Wipe versus Steal Remove verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Wipe... dust/the dust table/the table.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Steal... money/the money bank/*the bank.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	loaded... hay wagon/the wagon hay.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	filled...	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	*hay wagon/the wagon hay.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	put... hay wagon/*the wagon hay.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	three classes also assign semantic roles differ prepositional alternants.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Note, however, options Spray/Load verbs overlap two types verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	horse raced./The jockey raced horse.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	butter melted./The cook melted butter.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	(Note Object Drop verbs superset Benefactives above.)	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Dorr Jones, 1996).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Indicators PP usage thus useful definitive.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	1, 26.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	3 3 5 ci pi en 13.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	1, 13.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	3 2 7 Ad mi 31.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	2 3 5 us e 31.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	1 1 3 4 Ru n 51.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	3.2 7 9 un E mi ssi 43.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	2 5 6 C 10.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	6 2 9 St ea l ov e 10.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	5, 10.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	1 4 5 Wi pe 10.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	4.1 , 10.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	1– 4 1 6 9 bj ec Dr op 26.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	1, 26.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	3, 26.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	3.2 Verb Selection.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	experimental verbs selected follows.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Table 1 shows number verbs class end process.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	began set 20 verbs per class current work.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	3.3 Feature Extraction.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Since general corpus, expect strong overall domain bias verb usage.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	4.1 Clustering Parameters.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	used simple Euclidean distance former, Ward linkage latter.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	4.2 Evaluation Measures.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	use three separate evaluation measures, tap different properties clusterings.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	4.2.1 Accuracy assign cluster class label majority members.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	theoretical maximum is, course, 1.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	figures reported results Table 2 below.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, useful relative measure good-.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	ness, comparing clusterings arising different feature sets.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, since fix number clusters number classes, measure remains informative.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	3 experiments estimating baseline, in-.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	deed found mean value 0.00 random clusterings.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	higher (.89 vs. .33) reflects better separation data.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	regard target classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	value 0 suggests point clearly particular cluster.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	measure independent true classification, could high dependent measures low, vice versa.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	alternations.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	13-way task includes classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	third column Table 2 gives baseline calculated random clusterings.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Recall upper bound random performance.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	5.1 Full Feature Set.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Although generally higher baseline, well supervised learner, generally low.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	5.2 Manual Feature Selection.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	C5.0 supervised accuracy; Base random clusters.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	See text description.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	indicated class description given Levin.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	task, then, linguistically-relevant subset defined union subsets classes task.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	2-way tasks, performance average close full feature set measures.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	performance comparison tentatively suggests good feature selection helpful task.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, important find method depend existing classification, since interested applying approach classification exist.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	next two sections, present unsupervised minimally supervised approaches problem.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	5.3 Unsupervised Feature Selection.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	order deal excessive dimensionality, Dash et al.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Unfortunately, promising method prove practical data.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	5.4 Semi-Supervised Feature Selection.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	domain particular, verb class discovery “in vacuum” necessary.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	model kind approach, selected sample five seed verbs class.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	set verbs judged (by authors’ intuition alone) “representative” class.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	extracted resulting decision trees union features used, formed reduced feature set task.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	5.5 Discussion.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	clustering experiments, find smaller subsets features generally perform better full set features.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	(See Table 3 number features Ling Seed sets.)	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, small set features adequate.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	number classes (a simple linear function roughly approximating number features Seed sets).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Interestingly, generally high, indicating structure data, matches classification.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	might also ask, would subset verbs well?	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	found mean values Seed set reported above, mean little lower.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, study used small set five features manually devised set three particular classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	question future research explore effect variables clustering performance.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	successful seed set features is, still achieve accuracy supervised learner.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	research needed definition general feature space, well methods selecting useful set features clustering.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Furthermore, might question clustering approach itself, context verb class discovery.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	indebted Allan Jepson helpful discussions suggestions.	0
For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Semi-supervised Verb Class Discovery Using Noisy Features	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	find unsupervised method tried cannot consistently applied data.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	avoiding dependence precise feature extraction, approach portable new languages.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, general feature space means features irrelevant given verb discrimination task.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	unsupervised feature selection method, hand, usable data.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	remainder paper, first briefly review feature space present experimental classes verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	conclude discussion related work, contributions, future directions.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Thus, features serve approximations underlying distinctions among classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	allowable alternations expressions arguments vary according class verb.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	describe selection experimental classes verbs, estimation feature values.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	3.1 Verb Classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Benefactive versus Recipient verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Mary baked... cake Joan/Joan cake.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Mary gave... cake Joan/Joan cake.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	dative alternation verbs differ preposition semantic role object.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Admire versus Amuse verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	admire Jane.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Run versus Sound Emission verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	kids ran room./*The room ran kids.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Cheat versus Steal Remove verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	cheated...	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Jane money/*the money Jane.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	stole...	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	*Jane money/the money Jane.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	classes also assign semantic arguments, differ prepositional alternants.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Wipe versus Steal Remove verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Wipe... dust/the dust table/the table.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Steal... money/the money bank/*the bank.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	loaded... hay wagon/the wagon hay.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	filled...	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	*hay wagon/the wagon hay.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	put... hay wagon/*the wagon hay.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	three classes also assign semantic roles differ prepositional alternants.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Note, however, options Spray/Load verbs overlap two types verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	horse raced./The jockey raced horse.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	butter melted./The cook melted butter.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	(Note Object Drop verbs superset Benefactives above.)	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Dorr Jones, 1996).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Indicators PP usage thus useful definitive.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	1, 26.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	3 3 5 ci pi en 13.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	1, 13.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	3 2 7 Ad mi 31.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	2 3 5 us e 31.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	1 1 3 4 Ru n 51.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	3.2 7 9 un E mi ssi 43.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	2 5 6 C 10.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	6 2 9 St ea l ov e 10.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	5, 10.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	1 4 5 Wi pe 10.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	4.1 , 10.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	1– 4 1 6 9 bj ec Dr op 26.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	1, 26.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	3, 26.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	3.2 Verb Selection.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	experimental verbs selected follows.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Table 1 shows number verbs class end process.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	began set 20 verbs per class current work.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	3.3 Feature Extraction.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Since general corpus, expect strong overall domain bias verb usage.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	4.1 Clustering Parameters.	1
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	used simple Euclidean distance former, Ward linkage latter.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	4.2 Evaluation Measures.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	use three separate evaluation measures, tap different properties clusterings.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	4.2.1 Accuracy assign cluster class label majority members.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	theoretical maximum is, course, 1.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	figures reported results Table 2 below.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, useful relative measure good-.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	ness, comparing clusterings arising different feature sets.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, since fix number clusters number classes, measure remains informative.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	3 experiments estimating baseline, in-.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	deed found mean value 0.00 random clusterings.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	higher (.89 vs. .33) reflects better separation data.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	regard target classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	value 0 suggests point clearly particular cluster.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	measure independent true classification, could high dependent measures low, vice versa.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	alternations.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	13-way task includes classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	third column Table 2 gives baseline calculated random clusterings.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Recall upper bound random performance.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	5.1 Full Feature Set.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Although generally higher baseline, well supervised learner, generally low.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	5.2 Manual Feature Selection.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	C5.0 supervised accuracy; Base random clusters.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	See text description.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	indicated class description given Levin.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	task, then, linguistically-relevant subset defined union subsets classes task.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	2-way tasks, performance average close full feature set measures.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	performance comparison tentatively suggests good feature selection helpful task.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, important find method depend existing classification, since interested applying approach classification exist.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	next two sections, present unsupervised minimally supervised approaches problem.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	5.3 Unsupervised Feature Selection.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	order deal excessive dimensionality, Dash et al.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Unfortunately, promising method prove practical data.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	5.4 Semi-Supervised Feature Selection.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	domain particular, verb class discovery “in vacuum” necessary.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	model kind approach, selected sample five seed verbs class.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	set verbs judged (by authors’ intuition alone) “representative” class.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	extracted resulting decision trees union features used, formed reduced feature set task.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	5.5 Discussion.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	clustering experiments, find smaller subsets features generally perform better full set features.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	(See Table 3 number features Ling Seed sets.)	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, small set features adequate.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	number classes (a simple linear function roughly approximating number features Seed sets).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Interestingly, generally high, indicating structure data, matches classification.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	might also ask, would subset verbs well?	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	found mean values Seed set reported above, mean little lower.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, study used small set five features manually devised set three particular classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	question future research explore effect variables clustering performance.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	successful seed set features is, still achieve accuracy supervised learner.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	research needed definition general feature space, well methods selecting useful set features clustering.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Furthermore, might question clustering approach itself, context verb class discovery.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	indebted Allan Jepson helpful discussions suggestions.	0
Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Semi-supervised Verb Class Discovery Using Noisy Features	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	find unsupervised method tried cannot consistently applied data.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	avoiding dependence precise feature extraction, approach portable new languages.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, general feature space means features irrelevant given verb discrimination task.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	unsupervised feature selection method, hand, usable data.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	remainder paper, first briefly review feature space present experimental classes verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	conclude discussion related work, contributions, future directions.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Thus, features serve approximations underlying distinctions among classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	allowable alternations expressions arguments vary according class verb.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	describe selection experimental classes verbs, estimation feature values.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	3.1 Verb Classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Benefactive versus Recipient verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Mary baked... cake Joan/Joan cake.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Mary gave... cake Joan/Joan cake.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	dative alternation verbs differ preposition semantic role object.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Admire versus Amuse verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	admire Jane.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Run versus Sound Emission verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	kids ran room./*The room ran kids.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Cheat versus Steal Remove verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	cheated...	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Jane money/*the money Jane.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	stole...	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	*Jane money/the money Jane.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	classes also assign semantic arguments, differ prepositional alternants.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Wipe versus Steal Remove verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Wipe... dust/the dust table/the table.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Steal... money/the money bank/*the bank.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	loaded... hay wagon/the wagon hay.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	filled...	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	*hay wagon/the wagon hay.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	put... hay wagon/*the wagon hay.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	three classes also assign semantic roles differ prepositional alternants.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Note, however, options Spray/Load verbs overlap two types verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	horse raced./The jockey raced horse.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	butter melted./The cook melted butter.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	(Note Object Drop verbs superset Benefactives above.)	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Dorr Jones, 1996).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Indicators PP usage thus useful definitive.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	1, 26.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	3 3 5 ci pi en 13.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	1, 13.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	3 2 7 Ad mi 31.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	2 3 5 us e 31.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	1 1 3 4 Ru n 51.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	3.2 7 9 un E mi ssi 43.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	2 5 6 C 10.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	6 2 9 St ea l ov e 10.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	5, 10.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	1 4 5 Wi pe 10.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	4.1 , 10.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	1– 4 1 6 9 bj ec Dr op 26.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	1, 26.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	3, 26.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	3.2 Verb Selection.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	experimental verbs selected follows.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Table 1 shows number verbs class end process.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	began set 20 verbs per class current work.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	3.3 Feature Extraction.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Since general corpus, expect strong overall domain bias verb usage.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	4.1 Clustering Parameters.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	used simple Euclidean distance former, Ward linkage latter.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	4.2 Evaluation Measures.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	use three separate evaluation measures, tap different properties clusterings.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	4.2.1 Accuracy assign cluster class label majority members.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	theoretical maximum is, course, 1.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	figures reported results Table 2 below.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, useful relative measure good-.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	ness, comparing clusterings arising different feature sets.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, since fix number clusters number classes, measure remains informative.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	3 experiments estimating baseline, in-.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	deed found mean value 0.00 random clusterings.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	higher (.89 vs. .33) reflects better separation data.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	regard target classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	value 0 suggests point clearly particular cluster.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	measure independent true classification, could high dependent measures low, vice versa.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	alternations.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	13-way task includes classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	third column Table 2 gives baseline calculated random clusterings.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Recall upper bound random performance.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	5.1 Full Feature Set.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Although generally higher baseline, well supervised learner, generally low.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	5.2 Manual Feature Selection.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	C5.0 supervised accuracy; Base random clusters.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	See text description.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	indicated class description given Levin.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	task, then, linguistically-relevant subset defined union subsets classes task.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	2-way tasks, performance average close full feature set measures.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	performance comparison tentatively suggests good feature selection helpful task.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, important find method depend existing classification, since interested applying approach classification exist.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	next two sections, present unsupervised minimally supervised approaches problem.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	5.3 Unsupervised Feature Selection.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	order deal excessive dimensionality, Dash et al.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Unfortunately, promising method prove practical data.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	5.4 Semi-Supervised Feature Selection.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	domain particular, verb class discovery “in vacuum” necessary.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	model kind approach, selected sample five seed verbs class.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	set verbs judged (by authors’ intuition alone) “representative” class.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	extracted resulting decision trees union features used, formed reduced feature set task.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	5.5 Discussion.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	clustering experiments, find smaller subsets features generally perform better full set features.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	(See Table 3 number features Ling Seed sets.)	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, small set features adequate.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	number classes (a simple linear function roughly approximating number features Seed sets).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Interestingly, generally high, indicating structure data, matches classification.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	might also ask, would subset verbs well?	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	found mean values Seed set reported above, mean little lower.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, study used small set five features manually devised set three particular classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	question future research explore effect variables clustering performance.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	successful seed set features is, still achieve accuracy supervised learner.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	research needed definition general feature space, well methods selecting useful set features clustering.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Furthermore, might question clustering approach itself, context verb class discovery.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	indebted Allan Jepson helpful discussions suggestions.	0
Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Semi-supervised Verb Class Discovery Using Noisy Features	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	find unsupervised method tried cannot consistently applied data.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	avoiding dependence precise feature extraction, approach portable new languages.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, general feature space means features irrelevant given verb discrimination task.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	1
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	unsupervised feature selection method, hand, usable data.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	remainder paper, first briefly review feature space present experimental classes verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	conclude discussion related work, contributions, future directions.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Thus, features serve approximations underlying distinctions among classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	allowable alternations expressions arguments vary according class verb.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	describe selection experimental classes verbs, estimation feature values.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	3.1 Verb Classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Benefactive versus Recipient verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Mary baked... cake Joan/Joan cake.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Mary gave... cake Joan/Joan cake.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	dative alternation verbs differ preposition semantic role object.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Admire versus Amuse verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	admire Jane.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Run versus Sound Emission verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	kids ran room./*The room ran kids.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Cheat versus Steal Remove verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	cheated...	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Jane money/*the money Jane.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	stole...	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	*Jane money/the money Jane.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	classes also assign semantic arguments, differ prepositional alternants.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Wipe versus Steal Remove verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Wipe... dust/the dust table/the table.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Steal... money/the money bank/*the bank.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	loaded... hay wagon/the wagon hay.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	filled...	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	*hay wagon/the wagon hay.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	put... hay wagon/*the wagon hay.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	three classes also assign semantic roles differ prepositional alternants.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Note, however, options Spray/Load verbs overlap two types verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	horse raced./The jockey raced horse.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	butter melted./The cook melted butter.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	(Note Object Drop verbs superset Benefactives above.)	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Dorr Jones, 1996).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Indicators PP usage thus useful definitive.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	1, 26.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	3 3 5 ci pi en 13.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	1, 13.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	3 2 7 Ad mi 31.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	2 3 5 us e 31.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	1 1 3 4 Ru n 51.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	3.2 7 9 un E mi ssi 43.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	2 5 6 C 10.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	6 2 9 St ea l ov e 10.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	5, 10.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	1 4 5 Wi pe 10.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	4.1 , 10.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	1– 4 1 6 9 bj ec Dr op 26.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	1, 26.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	3, 26.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	3.2 Verb Selection.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	experimental verbs selected follows.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Table 1 shows number verbs class end process.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	began set 20 verbs per class current work.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	3.3 Feature Extraction.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Since general corpus, expect strong overall domain bias verb usage.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	4.1 Clustering Parameters.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	used simple Euclidean distance former, Ward linkage latter.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	4.2 Evaluation Measures.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	use three separate evaluation measures, tap different properties clusterings.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	4.2.1 Accuracy assign cluster class label majority members.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	theoretical maximum is, course, 1.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	figures reported results Table 2 below.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, useful relative measure good-.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	ness, comparing clusterings arising different feature sets.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, since fix number clusters number classes, measure remains informative.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	3 experiments estimating baseline, in-.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	deed found mean value 0.00 random clusterings.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	higher (.89 vs. .33) reflects better separation data.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	regard target classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	value 0 suggests point clearly particular cluster.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	measure independent true classification, could high dependent measures low, vice versa.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	alternations.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	13-way task includes classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	third column Table 2 gives baseline calculated random clusterings.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Recall upper bound random performance.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	5.1 Full Feature Set.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Although generally higher baseline, well supervised learner, generally low.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	5.2 Manual Feature Selection.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	C5.0 supervised accuracy; Base random clusters.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	See text description.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	indicated class description given Levin.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	task, then, linguistically-relevant subset defined union subsets classes task.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	2-way tasks, performance average close full feature set measures.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	performance comparison tentatively suggests good feature selection helpful task.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, important find method depend existing classification, since interested applying approach classification exist.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	next two sections, present unsupervised minimally supervised approaches problem.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	5.3 Unsupervised Feature Selection.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	order deal excessive dimensionality, Dash et al.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Unfortunately, promising method prove practical data.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	5.4 Semi-Supervised Feature Selection.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	domain particular, verb class discovery “in vacuum” necessary.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	model kind approach, selected sample five seed verbs class.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	set verbs judged (by authors’ intuition alone) “representative” class.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	extracted resulting decision trees union features used, formed reduced feature set task.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	5.5 Discussion.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	clustering experiments, find smaller subsets features generally perform better full set features.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	(See Table 3 number features Ling Seed sets.)	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, small set features adequate.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	number classes (a simple linear function roughly approximating number features Seed sets).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Interestingly, generally high, indicating structure data, matches classification.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	might also ask, would subset verbs well?	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	found mean values Seed set reported above, mean little lower.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, study used small set five features manually devised set three particular classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	question future research explore effect variables clustering performance.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	successful seed set features is, still achieve accuracy supervised learner.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	research needed definition general feature space, well methods selecting useful set features clustering.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Furthermore, might question clustering approach itself, context verb class discovery.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	indebted Allan Jepson helpful discussions suggestions.	0
(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Semi-supervised Verb Class Discovery Using Noisy Features	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	feature set previously shown work well supervised learning setting, using known English verb classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	find unsupervised method tried cannot consistently applied data.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	avoiding dependence precise feature extraction, approach portable new languages.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, general feature space means features irrelevant given verb discrimination task.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	unsupervised feature selection method, hand, usable data.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	remainder paper, first briefly review feature space present experimental classes verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	describe clustering methodology, measures use evaluate clustering, experimental results.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	conclude discussion related work, contributions, future directions.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Thus, features serve approximations underlying distinctions among classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	allowable alternations expressions arguments vary according class verb.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	describe selection experimental classes verbs, estimation feature values.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	3.1 Verb Classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Benefactive versus Recipient verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Mary baked... cake Joan/Joan cake.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Mary gave... cake Joan/Joan cake.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	dative alternation verbs differ preposition semantic role object.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Admire versus Amuse verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	admire Jane.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Run versus Sound Emission verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	kids ran room./*The room ran kids.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Cheat versus Steal Remove verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	cheated...	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Jane money/*the money Jane.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	stole...	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	*Jane money/the money Jane.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	classes also assign semantic arguments, differ prepositional alternants.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Wipe versus Steal Remove verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Wipe... dust/the dust table/the table.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Steal... money/the money bank/*the bank.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	classes generally allow syntactic frames, differ possible semantic role assignment.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	loaded... hay wagon/the wagon hay.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	filled...	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	*hay wagon/the wagon hay.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	put... hay wagon/*the wagon hay.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	three classes also assign semantic roles differ prepositional alternants.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Note, however, options Spray/Load verbs overlap two types verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	horse raced./The jockey raced horse.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	butter melted./The cook melted butter.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	(Note Object Drop verbs superset Benefactives above.)	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Dorr Jones, 1996).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Indicators PP usage thus useful definitive.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	1, 26.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	3 3 5 ci pi en 13.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	1, 13.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	3 2 7 Ad mi 31.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	2 3 5 us e 31.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	1 1 3 4 Ru n 51.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	3.2 7 9 un E mi ssi 43.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	2 5 6 C 10.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	6 2 9 St ea l ov e 10.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	5, 10.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	1 4 5 Wi pe 10.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	4.1 , 10.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	1– 4 1 6 9 bj ec Dr op 26.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	1, 26.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	3, 26.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	3.2 Verb Selection.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	experimental verbs selected follows.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Table 1 shows number verbs class end process.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	began set 20 verbs per class current work.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	3.3 Feature Extraction.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Since general corpus, expect strong overall domain bias verb usage.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	4.1 Clustering Parameters.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	used simple Euclidean distance former, Ward linkage latter.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	explore this, induce number clusters making cut particular level clustering hierarchy.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	4.2 Evaluation Measures.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	use three separate evaluation measures, tap different properties clusterings.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	4.2.1 Accuracy assign cluster class label majority members.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	theoretical maximum is, course, 1.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	figures reported results Table 2 below.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, useful relative measure good-.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	ness, comparing clusterings arising different feature sets.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, since fix number clusters number classes, measure remains informative.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	3 experiments estimating baseline, in-.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	deed found mean value 0.00 random clusterings.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	higher (.89 vs. .33) reflects better separation data.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	regard target classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	use , mean silhouette measure Matlab, measures distant data point clusters.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	value 0 suggests point clearly particular cluster.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	measure independent true classification, could high dependent measures low, vice versa.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	alternations.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	13-way task includes classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	third column Table 2 gives baseline calculated random clusterings.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Recall upper bound random performance.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	5.1 Full Feature Set.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Although generally higher baseline, well supervised learner, generally low.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	5.2 Manual Feature Selection.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	One approach dimensionality reduction hand- select features one believes relevant given task.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	C5.0 supervised accuracy; Base random clusters.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	See text description.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	indicated class description given Levin.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	task, then, linguistically-relevant subset defined union subsets classes task.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	2-way tasks, performance average close full feature set measures.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	performance comparison tentatively suggests good feature selection helpful task.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, important find method depend existing classification, since interested applying approach classification exist.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	next two sections, present unsupervised minimally supervised approaches problem.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	5.3 Unsupervised Feature Selection.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	order deal excessive dimensionality, Dash et al.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Unfortunately, promising method prove practical data.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Many feature sets performed well, far outperformed best results using feature selection methods.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	5.4 Semi-Supervised Feature Selection.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	domain particular, verb class discovery “in vacuum” necessary.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	model kind approach, selected sample five seed verbs class.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	set verbs judged (by authors’ intuition alone) “representative” class.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	extracted resulting decision trees union features used, formed reduced feature set task.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Another striking result difference values, much higher Ling (which turn much higher Full).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	5.5 Discussion.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	clustering experiments, find smaller subsets features generally perform better full set features.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	(See Table 3 number features Ling Seed sets.)	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, small set features adequate.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	number classes (a simple linear function roughly approximating number features Seed sets).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Interestingly, generally high, indicating structure data, matches classification.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	confirms appropriate feature selection, small number features, important task verb class discovery.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	might also ask, would subset verbs well?	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	found mean values Seed set reported above, mean little lower.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, study used small set five features manually devised set three particular classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	question future research explore effect variables clustering performance.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	1
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Furthermore, method relatively insensitive precise makeup selected seed set.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	successful seed set features is, still achieve accuracy supervised learner.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	research needed definition general feature space, well methods selecting useful set features clustering.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Furthermore, might question clustering approach itself, context verb class discovery.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	indebted Allan Jepson helpful discussions suggestions.	0
As an alternative to the resource-intensive manual classifications, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g.  (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Semi-supervised Verb Class Discovery Using Noisy Features	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	find unsupervised method tried cannot consistently applied data.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	avoiding dependence precise feature extraction, approach portable new languages.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, general feature space means features irrelevant given verb discrimination task.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	unsupervised feature selection method, hand, usable data.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	remainder paper, first briefly review feature space present experimental classes verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	conclude discussion related work, contributions, future directions.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Thus, features serve approximations underlying distinctions among classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	allowable alternations expressions arguments vary according class verb.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	describe selection experimental classes verbs, estimation feature values.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	3.1 Verb Classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Benefactive versus Recipient verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Mary baked... cake Joan/Joan cake.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Mary gave... cake Joan/Joan cake.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	dative alternation verbs differ preposition semantic role object.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Admire versus Amuse verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	admire Jane.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Run versus Sound Emission verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	kids ran room./*The room ran kids.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Cheat versus Steal Remove verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	cheated...	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Jane money/*the money Jane.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	stole...	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	*Jane money/the money Jane.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	classes also assign semantic arguments, differ prepositional alternants.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Wipe versus Steal Remove verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Wipe... dust/the dust table/the table.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Steal... money/the money bank/*the bank.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	loaded... hay wagon/the wagon hay.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	filled...	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	*hay wagon/the wagon hay.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	put... hay wagon/*the wagon hay.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	three classes also assign semantic roles differ prepositional alternants.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Note, however, options Spray/Load verbs overlap two types verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	horse raced./The jockey raced horse.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	butter melted./The cook melted butter.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	(Note Object Drop verbs superset Benefactives above.)	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Dorr Jones, 1996).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Indicators PP usage thus useful definitive.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	1, 26.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	3 3 5 ci pi en 13.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	1, 13.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	3 2 7 Ad mi 31.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	2 3 5 us e 31.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	1 1 3 4 Ru n 51.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	3.2 7 9 un E mi ssi 43.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	2 5 6 C 10.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	6 2 9 St ea l ov e 10.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	5, 10.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	1 4 5 Wi pe 10.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	4.1 , 10.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	1– 4 1 6 9 bj ec Dr op 26.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	1, 26.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	3, 26.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	3.2 Verb Selection.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	experimental verbs selected follows.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Table 1 shows number verbs class end process.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	began set 20 verbs per class current work.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	3.3 Feature Extraction.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Since general corpus, expect strong overall domain bias verb usage.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	4.1 Clustering Parameters.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	used simple Euclidean distance former, Ward linkage latter.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	4.2 Evaluation Measures.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	use three separate evaluation measures, tap different properties clusterings.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	4.2.1 Accuracy assign cluster class label majority members.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	theoretical maximum is, course, 1.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	figures reported results Table 2 below.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, useful relative measure good-.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	ness, comparing clusterings arising different feature sets.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, since fix number clusters number classes, measure remains informative.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	3 experiments estimating baseline, in-.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	deed found mean value 0.00 random clusterings.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	higher (.89 vs. .33) reflects better separation data.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	regard target classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	value 0 suggests point clearly particular cluster.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	measure independent true classification, could high dependent measures low, vice versa.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	alternations.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	13-way task includes classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	third column Table 2 gives baseline calculated random clusterings.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Recall upper bound random performance.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	5.1 Full Feature Set.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Although generally higher baseline, well supervised learner, generally low.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	5.2 Manual Feature Selection.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	C5.0 supervised accuracy; Base random clusters.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	See text description.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	indicated class description given Levin.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	task, then, linguistically-relevant subset defined union subsets classes task.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	2-way tasks, performance average close full feature set measures.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	performance comparison tentatively suggests good feature selection helpful task.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, important find method depend existing classification, since interested applying approach classification exist.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	next two sections, present unsupervised minimally supervised approaches problem.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	5.3 Unsupervised Feature Selection.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	order deal excessive dimensionality, Dash et al.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Unfortunately, promising method prove practical data.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	5.4 Semi-Supervised Feature Selection.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	domain particular, verb class discovery “in vacuum” necessary.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	model kind approach, selected sample five seed verbs class.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	set verbs judged (by authors’ intuition alone) “representative” class.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	extracted resulting decision trees union features used, formed reduced feature set task.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	5.5 Discussion.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	clustering experiments, find smaller subsets features generally perform better full set features.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	(See Table 3 number features Ling Seed sets.)	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, small set features adequate.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	number classes (a simple linear function roughly approximating number features Seed sets).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Interestingly, generally high, indicating structure data, matches classification.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	might also ask, would subset verbs well?	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	found mean values Seed set reported above, mean little lower.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, study used small set five features manually devised set three particular classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	question future research explore effect variables clustering performance.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	successful seed set features is, still achieve accuracy supervised learner.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	research needed definition general feature space, well methods selecting useful set features clustering.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Furthermore, might question clustering approach itself, context verb class discovery.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	indebted Allan Jepson helpful discussions suggestions.	0
In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Semi-supervised Verb Class Discovery Using Noisy Features	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	feature set previously shown work well supervised learning setting, using known English verb classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	find unsupervised method tried cannot consistently applied data.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	avoiding dependence precise feature extraction, approach portable new languages.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, general feature space means features irrelevant given verb discrimination task.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	unsupervised feature selection method, hand, usable data.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	remainder paper, first briefly review feature space present experimental classes verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	describe clustering methodology, measures use evaluate clustering, experimental results.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	conclude discussion related work, contributions, future directions.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Thus, features serve approximations underlying distinctions among classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	allowable alternations expressions arguments vary according class verb.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	describe selection experimental classes verbs, estimation feature values.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	3.1 Verb Classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Benefactive versus Recipient verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Mary baked... cake Joan/Joan cake.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Mary gave... cake Joan/Joan cake.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	dative alternation verbs differ preposition semantic role object.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Admire versus Amuse verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	admire Jane.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Run versus Sound Emission verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	kids ran room./*The room ran kids.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Cheat versus Steal Remove verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	cheated...	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Jane money/*the money Jane.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	stole...	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	*Jane money/the money Jane.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	classes also assign semantic arguments, differ prepositional alternants.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Wipe versus Steal Remove verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Wipe... dust/the dust table/the table.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Steal... money/the money bank/*the bank.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	classes generally allow syntactic frames, differ possible semantic role assignment.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	loaded... hay wagon/the wagon hay.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	filled...	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	*hay wagon/the wagon hay.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	put... hay wagon/*the wagon hay.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	three classes also assign semantic roles differ prepositional alternants.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Note, however, options Spray/Load verbs overlap two types verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	horse raced./The jockey raced horse.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	butter melted./The cook melted butter.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	(Note Object Drop verbs superset Benefactives above.)	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Dorr Jones, 1996).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Indicators PP usage thus useful definitive.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	1, 26.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	3 3 5 ci pi en 13.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	1, 13.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	3 2 7 Ad mi 31.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	2 3 5 us e 31.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	1 1 3 4 Ru n 51.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	3.2 7 9 un E mi ssi 43.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	2 5 6 C 10.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	6 2 9 St ea l ov e 10.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	5, 10.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	1 4 5 Wi pe 10.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	4.1 , 10.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	1– 4 1 6 9 bj ec Dr op 26.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	1, 26.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	3, 26.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	3.2 Verb Selection.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	experimental verbs selected follows.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Table 1 shows number verbs class end process.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	began set 20 verbs per class current work.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	3.3 Feature Extraction.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Since general corpus, expect strong overall domain bias verb usage.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	4.1 Clustering Parameters.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	used simple Euclidean distance former, Ward linkage latter.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	explore this, induce number clusters making cut particular level clustering hierarchy.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	4.2 Evaluation Measures.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	use three separate evaluation measures, tap different properties clusterings.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	4.2.1 Accuracy assign cluster class label majority members.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	1
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	theoretical maximum is, course, 1.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	figures reported results Table 2 below.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, useful relative measure good-.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	ness, comparing clusterings arising different feature sets.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, since fix number clusters number classes, measure remains informative.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	3 experiments estimating baseline, in-.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	deed found mean value 0.00 random clusterings.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	higher (.89 vs. .33) reflects better separation data.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	regard target classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	use , mean silhouette measure Matlab, measures distant data point clusters.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	value 0 suggests point clearly particular cluster.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	measure independent true classification, could high dependent measures low, vice versa.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	alternations.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	13-way task includes classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	third column Table 2 gives baseline calculated random clusterings.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Recall upper bound random performance.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	5.1 Full Feature Set.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Although generally higher baseline, well supervised learner, generally low.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	5.2 Manual Feature Selection.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	One approach dimensionality reduction hand- select features one believes relevant given task.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	C5.0 supervised accuracy; Base random clusters.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	See text description.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	indicated class description given Levin.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	task, then, linguistically-relevant subset defined union subsets classes task.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	2-way tasks, performance average close full feature set measures.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	performance comparison tentatively suggests good feature selection helpful task.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, important find method depend existing classification, since interested applying approach classification exist.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	next two sections, present unsupervised minimally supervised approaches problem.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	5.3 Unsupervised Feature Selection.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	order deal excessive dimensionality, Dash et al.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Unfortunately, promising method prove practical data.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Many feature sets performed well, far outperformed best results using feature selection methods.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	5.4 Semi-Supervised Feature Selection.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	domain particular, verb class discovery “in vacuum” necessary.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	model kind approach, selected sample five seed verbs class.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	set verbs judged (by authors’ intuition alone) “representative” class.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	extracted resulting decision trees union features used, formed reduced feature set task.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Another striking result difference values, much higher Ling (which turn much higher Full).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	5.5 Discussion.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	clustering experiments, find smaller subsets features generally perform better full set features.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	(See Table 3 number features Ling Seed sets.)	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, small set features adequate.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	number classes (a simple linear function roughly approximating number features Seed sets).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Interestingly, generally high, indicating structure data, matches classification.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	confirms appropriate feature selection, small number features, important task verb class discovery.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	might also ask, would subset verbs well?	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	found mean values Seed set reported above, mean little lower.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, study used small set five features manually devised set three particular classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	question future research explore effect variables clustering performance.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Furthermore, method relatively insensitive precise makeup selected seed set.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	successful seed set features is, still achieve accuracy supervised learner.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	research needed definition general feature space, well methods selecting useful set features clustering.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Furthermore, might question clustering approach itself, context verb class discovery.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	indebted Allan Jepson helpful discussions suggestions.	0
For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf.  (Stevenson and Joanis, 2003; Korhonen et al., 2003)	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Semi-supervised Verb Class Discovery Using Noisy Features	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	cluster verbs lexical semantic classes, using general set noisy features capture syntactic semantic properties verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	feature set previously shown work well supervised learning setting, using known English verb classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	moving scenario verb class discovery, using clustering, face problem large number irrelevant features particular clustering task.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	investigate various approaches feature selection, using unsupervised semi-supervised methods, comparing results subsets features manually chosen according linguistic properties.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	find unsupervised method tried cannot consistently applied data.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, semi- supervised approach (using seed set sample verbs) overall outperforms full set features, hand-selected features well.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Computational linguists face lexical acquisition bottleneck, vast amounts knowledge individual words required language technologies.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Learning argument structure properties verbs—the semantic roles assign mapping syntactic positions—is particularly important difficult.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	number supervised learning approaches extracted information verbs corpora, including argument roles (Gildea Jurafsky, 2002), selectional preferences (Resnik, 1996), lexical semantic classification (i.e., grouping verbs according argument structure properties) (Dorr Jones, 1996; Lapata Brew, 1999; Merlo Stevenson, 2001; Joanis Stevenson, 2003).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Unsupervised semi-supervised approaches successful well, tended restrictive, relying human filtering results (Riloff Schmelzenbach, 1998), hand- selection features (Stevenson Merlo, 1999), use extensive grammar (Schulte im Walde Brew, 2002).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	focus extending applicability unsupervised methods, (Schulte im Walde Brew, 2002; Stevenson Merlo, 1999), lexical semantic classification verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	classes group together verbs share common semantics (such transfer possession change state), set syntactic frames expressing arguments verb (Levin, 1993; FrameNet, 2003).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	such, serve means organizing complex knowledge verbs computational lexicon (Kipper et al., 2000).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, creating verb classification highly resource intensive, terms required time linguistic expertise.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Development minimally supervised methods particular importance automatically classify verbs languages English, substantial amounts labelled data available training classifiers.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	also necessary consider probable lack sophisticated grammars text processing tools extracting accurate features.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	previously shown broad set 220 noisy features performs well supervised verb classification (Joanis Stevenson, 2003).	1
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	contrast Merlo Stevenson (2001), confirmed set general features successfully used, without need manually determining relevant features distinguishing particular classes (cf.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Dorr Jones, 1996; Schulte im Walde Brew, 2002).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	hand, contrast Schulte im Walde Brew (2002), demonstrated accurate subcategorization statistics unnecessary (see also Sarkar Tripasai, 2002).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	avoiding dependence precise feature extraction, approach portable new languages.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, general feature space means features irrelevant given verb discrimination task.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	unsupervised (clustering) scenario verb class discovery, maintain benefit needing noisy features, without generality feature space leading “the curse dimensionality”?	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	supervised experiments, learner uses class labels training stage determine features relevant task hand.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	unsupervised setting, large number potentially irrelevant features becomes serious problem, since features may mislead learner.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Thus, problem dimensionality reduction key issue addressed verb class discovery.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	paper, report results several feature selection approaches problem: manual selection (based linguistic knowledge), unsupervised selection (based entropy measure among features, Dash et al., 1997), semi- supervised approach (in seed verbs used train supervised learner, extract useful features).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Although motivation verb class discovery, perform experiments English, accepted classification serve gold standard (Levin, 1993).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	preview results, find that, overall, semi-supervised method outperforms entire feature space, also manually selected subset features.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	unsupervised feature selection method, hand, usable data.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	remainder paper, first briefly review feature space present experimental classes verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	describe clustering methodology, measures use evaluate clustering, experimental results.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	conclude discussion related work, contributions, future directions.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Like others, assumed lexical semantic classes verbs defined Levin (1993) (hereafter Levin), served gold standard computational linguistics research (Dorr Jones, 1996; Kipper et al., 2000; Merlo Stevenson, 2001; Schulte im Walde Brew, 2002).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Levin’s classes form hierarchy verb groupings shared meaning syntax.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	feature space designed reflect classes capturing properties semantic arguments verbs mapping syntactic positions.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	important emphasize, however, features extracted part-of-speech (POS) tagged chunked text only: semantic tags kind.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Thus, features serve approximations underlying distinctions among classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	briefly describe features comprise feature space, refer interested reader Joanis Stevenson (2003) details.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Features Syntactic Slots (120 features) One set features encodes frequency syntactic slots occurring verb (subject, direct indirect object, prepositional phrases (PPs) indexed preposition), collectively serve rough approximations allowable syntactic frames verb.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	also count fixed elements certain slots (it there, rains appeared ship), since part syntactic frame specifications verb.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	addition approximating syntactic frames themselves, also want capture regularities mapping arguments particular slots.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	example, location argument, truck, direct object loaded truck hay, object preposition loaded hay onto truck.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	allowable alternations expressions arguments vary according class verb.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	measure behaviour using features encode degree two slots contain entities—that is, calculate overlap noun (lemma) usage pairs syntactic slots.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Tense, Voice, Aspect Features (24 features) Verb meaning, therefore class membership, interacts interesting ways voice, tense, aspect (Levin, 1993; Merlo Stevenson, 2001).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	addition verb POS (which often indicates tense) voice (passive/active), also include counts modals, auxiliaries, adverbs, partial indicators factors.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Animacy Features (76 features) Semantic properties arguments fill certain roles, animacy motion, challenging detect automatically.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Currently, feature extension animacy feature Merlo Stevenson (2001).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	approximate animacy 76 syntactic slots counting pronouns proper noun phrases (NPs) labelled “person” chunker (Abney, 1991).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	use classes example verbs supervised experiments Joanis Stevenson (2003) enable comparison performance unsupervised supervised methods.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	describe selection experimental classes verbs, estimation feature values.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	3.1 Verb Classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Pairs triples verb classes Levin selected form test pairs/triples number separate classification tasks.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	sets exhibit different contrasts verb classes terms semantic argument assignments, allowing us evaluate approach range conditions.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	example, classes differ semantic roles frames, others roles different frames, different roles frames.1 summarize argument structure distinctions classes; Table 1 lists classes Levin class numbers.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Benefactive versus Recipient verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Mary baked... cake Joan/Joan cake.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Mary gave... cake Joan/Joan cake.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	dative alternation verbs differ preposition semantic role object.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	1 practical reasons, well enabling us draw general conclusions results, classes also could neither small contain mostly infrequent verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Admire versus Amuse verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	admire Jane.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Jane amuses me. psychological state verbs differ Experiencer argument subject Admire verbs, object Amuse verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Run versus Sound Emission verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	kids ran room./*The room ran kids.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	birds sang trees./The trees sang birds.These activity verbs Agent subject transitive, differ prepositional alternations allow.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Cheat versus Steal Remove verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	cheated...	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Jane money/*the money Jane.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	stole...	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	*Jane money/the money Jane.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	classes also assign semantic arguments, differ prepositional alternants.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Wipe versus Steal Remove verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Wipe... dust/the dust table/the table.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Steal... money/the money bank/*the bank.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	classes generally allow syntactic frames, differ possible semantic role assignment.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	(Location direct object Wipe verbs Steal Remove verbs, shown.)	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Spray/Load versus Fill versus Verbs Putting (several related Levin classes).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	loaded... hay wagon/the wagon hay.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	filled...	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	*hay wagon/the wagon hay.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	put... hay wagon/*the wagon hay.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	three classes also assign semantic roles differ prepositional alternants.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Note, however, options Spray/Load verbs overlap two types verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Optionally Intransitive: Run versus Change State versus “Object Drop”.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	horse raced./The jockey raced horse.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	butter melted./The cook melted butter.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	boy played./The boy played soccer.These three classes optionally intransitive sign different semantic roles arguments (Merlo Stevenson, 2001).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	(Note Object Drop verbs superset Benefactives above.)	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	many tasks, knowing exactly PP arguments verb takes may sufficient perform classification (cf.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Dorr Jones, 1996).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, features give us perfect knowledge, since PP arguments adjuncts cannot distinguished high accuracy.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Using simple extraction tools, example, PP argument admired Jane honesty distinguished PP adjunct amused Jane money.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Furthermore, PP arguments differ frequency, highly distinguishing rarely used alternant likely useful.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Indicators PP usage thus useful definitive.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	rb Cl C la ss N u b er # rbs ne fa cti 26.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	1, 26.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	3 3 5 ci pi en 13.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	1, 13.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	3 2 7 Ad mi 31.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	2 3 5 us e 31.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	1 1 3 4 Ru n 51.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	3.2 7 9 un E mi ssi 43.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	2 5 6 C 10.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	6 2 9 St ea l ov e 10.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	5, 10.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	1 4 5 Wi pe 10.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	4.1 , 10.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi 9.8 6 3 Ot r V. Pu tti ng 9.1 –6 4 8 C ha ng e St e 45.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	1– 4 1 6 9 bj ec Dr op 26.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	1, 26.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	3, 26.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	7 5 0 Table 1: Verb classes (see Section 3.1), Levin class numbers, number experimental verbs (see Section 3.2).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	3.2 Verb Selection.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	experimental verbs selected follows.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	started list verbs given classes Levin, removing verb occur least 100 times corpus (the BNC, described below).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	make simplifying assumption single correct classification verb, also removed verb: deemed excessively polysemous; belonged another class consideration study; class correspond main sense.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Table 1 shows number verbs class end process.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	verbs, 20 class randomly selected use training data supervised experiments Joanis Stevenson (2003).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	began set 20 verbs per class current work.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	replaced 10 260 verbs (4%) enable us representative seed verbs certain classes semi-supervised experiments (e.g., could include wipe seed verb Wipe verbs, fill Fill verbs).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	experiments reported run final set 20 verbs per class (including replication earlier supervised experiments).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	3.3 Feature Extraction.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	features estimated counts British National Corpus (BNC), 100M word corpus text samples recent British English ranging wide spectrum domains.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Since general corpus, expect strong overall domain bias verb usage.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	used chunker (partial parser) Abney (1991) preprocess corpus, (noisily) determines NP subject direct object verb, well PPs potentially associated it.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Indirect objects identified less sophisticated (and even noisier) method, simply assuming two consecutive NPs verb constitute double object frame.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	extracted slots, calculate features described Section 2, yielding vector 220 normalized counts verb, forms input machine learning experiments.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	4.1 Clustering Parameters.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	used hierarchical clustering command Matlab, implements bottom-up agglomerative clustering, unsupervised experiments.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	performing hierarchical clustering, vector distance measure cluster distance (“linkage”) measure specified.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	used simple Euclidean distance former, Ward linkage latter.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Ward linkage essentially minimizes distances cluster points centroid, thus less sensitive outliers methods.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	chose hierarchical clustering may possible find coherent subclusters verbs even exactly good clusters, number classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	explore this, induce number clusters making cut particular level clustering hierarchy.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	experiments here, however, report results , since found principled way automatically determining good cutoff.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, experiment (as Strehl et al., 2000), found performance generally better (even measure, described below, discounts oversplitting).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	supports intuition approach may enable us find consistent clusters finer grain, without much fragmentation.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	4.2 Evaluation Measures.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	use three separate evaluation measures, tap different properties clusterings.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	4.2.1 Accuracy assign cluster class label majority members.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	verbs , consider classified correctly Class( )=ClusterLabel( ), Class( ) actual class ClusterLabel( ) label assigned cluster placed.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	accuracy standard definition:2 2 equivalent weighted mean precision clusters, weighted according cluster size.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	defined it, necessarily generally increases number clusters increases, extreme #verbs correctly classified #verbs total thus provides measure usefulness practice clustering—that is, one use clustering classification, measure tells accurate overall class assignments would be.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	theoretical maximum is, course, 1.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	calculate random baseline, evaluated 10,000 random clusterings number verbs classes experimental tasks.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	achieved depends precise size clusters, calculated mean best scenario (with equal-sized clusters), yielding conservative estimate (i.e., upper bound) baseline.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	figures reported results Table 2 below.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	4.2.2 Adjusted Rand Measure Accuracy relatively high clustering clusters good, others good.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	second measure, adjusted Rand measure used Schulte im Walde (2003), instead gives measure consistent given clustering overall respect gold standard classification.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	formula follows (Hubert Arabie, 1985): entry contingency table classification clustering, counting size intersection class cluster . Intuitively, measures similarity two partitions data considering agreements disagreements them— agreement, example, class cluster, disagreement not.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	scaled perfect agreement yields value 1, whereas random groupings (with number groups each) get value around 0.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	therefore considered “corrected chance,” given fixed number clusters.3 tests measure contrived cluster- ings, found quite conservative, experimental clusterings often attain values higher .25.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, useful relative measure good-.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	ness, comparing clusterings arising different feature sets.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	4.2.3 Mean Silhouette gives average individual goodness clusters, measure overall goodness, respect gold standard classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	final measure gives indication overall goodness clusters purely terms separation data, without number clusters equal number verbs.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, since fix number clusters number classes, measure remains informative.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	3 experiments estimating baseline, in-.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	deed found mean value 0.00 random clusterings.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	W e p th e su lt n u r cl us te ri n g ex - pe ri en ts, us g fe ur e se ts fo w s: (1 ) th e fu fe ur e sp ac e; (2 ) ua se le ct ed su bs et fe ur es ; (3 ) u n- su pe rv ed se le ct io n fe ur es ; (4 ) se mi su p er vi se se le ct io n, us g su pe rv ed le ar ne r ap pl ie se ed rb se le ct th e fe ur es . F ea ch ty pe fe ur e se t, w e pe rf ed th e sa e te n cl us te ri n g ta sk s, sh w n th e fir st co lu n Ta bl e 2.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	se ar e th e sa e ta sk pe rf ed th e su pe rv ed se t- ti n g Jo St ev en n (2 0 0 3) . 2- 3 w ay ta sk s, th ei r ot iv io n, w er e de sc ri ec ti n 3.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	1. hr ee ul ti w ay ta sk ex pl e pe rf ce r la rg er n u r cl se s: 6 w ay ta sk v ol v es th e C , St ea l– R e ov e, W ip e, pr ay /L d, Fi ll, “ th er V er bs P ut ti n g ” cl se s, al l w hi ch u n de rg si il ar lo ca ti v e Figure 1: dendrograms values 2-way Wipe/Steal–Remove task, using Ling Seed sets.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	higher (.89 vs. .33) reflects better separation data.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	regard target classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	use , mean silhouette measure Matlab, measures distant data point clusters.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Silhouette values vary +1 -1, +1 indicating point near centroid cluster, -1 indicating point close another cluster (and therefore likely wrong cluster).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	value 0 suggests point clearly particular cluster.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	calculate mean silhouette points clustering obtain overall measure well clusters separated.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Essentially, measure numerically captures intuitively grasp visual differences dendrograms “better” “worse” clusterings.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	(A dendrogram tree diagram whose leaves data points, whose branch lengths indicate similarity subclusters; roughly, shorter vertical lines indicate closer clusters.)	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	example, Figure 1 shows two dendrograms using different feature sets (Ling Seed, described Section 5) set verbs two classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Seed set slightly lower values , much higher value (.89) , indicating better separation data.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	captures reflected dendrogram, short lines connect verbs low tree, longer lines connect two main clusters.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	measure independent true classification, could high dependent measures low, vice versa.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, gives important information quality clustering: measures equal, clustering higher value indicates tighter separated clusters, suggesting stronger inherent patterns data.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	alternations.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	6, 8-way task adds Run Sound Emission verbs, also undergo locative alternations.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	13-way task includes classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	second column Table 2 includes accuracy supervised learner (the decision tree induction system, C5.0), verb sets clustering experiments.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	results 10-fold cross- validation (with boosting) repeated 50 times.4 earlier work, found cross-validation performance averaged .02, .04, .11 higher test performance 2-way, 3-way, multiway tasks, respectively, taken upper bound achieved.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	third column Table 2 gives baseline calculated random clusterings.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Recall upper bound random performance.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	use baseline calculating reductions error rate . remaining columns table give , , measures described Section 4.2, feature sets explored clustering, discuss turn below.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	5.1 Full Feature Set.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	first subcolumn (Full) three clustering evaluation measures Table 2 shows results using full set features (i.e., feature selection).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Although generally higher baseline, well supervised learner, generally low.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	5.2 Manual Feature Selection.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	One approach dimensionality reduction hand- select features one believes relevant given task.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Following Joanis Stevenson (2003), class, systematically identified subset features 4 results differ slightly reported Joanis Stevenson (2003), slight changes verb sets, discussed Section 3.2.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Steal–Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Steal–Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	C5.0 supervised accuracy; Base random clusters.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Full full feature set; Ling manually selected subset; Seed seed-verb-selected set.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	See text description.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	indicated class description given Levin.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	task, then, linguistically-relevant subset defined union subsets classes task.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	results feature sets clustering given second subcolumn (Ling) , , measures Table 2.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	2-way tasks, performance average close full feature set measures.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	3-way multiway tasks, larger performance gain using subset features, increase reduction error rate (over Base ) 67% full feature set.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Overall, small performance gain using Ling subset features (with increase error rate reduction 13% 17%).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Moreover, value manually selected features almost always much higher full feature set, indicating subset features focused properties lead better separation data.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	performance comparison tentatively suggests good feature selection helpful task.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, important find method depend existing classification, since interested applying approach classification exist.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	next two sections, present unsupervised minimally supervised approaches problem.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	5.3 Unsupervised Feature Selection.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	order deal excessive dimensionality, Dash et al.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	(1997) propose unsupervised method rank set features according ability organize data space, based entropy measure devise.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Unfortunately, promising method prove practical data.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	performed number experiments tested performance feature set cardinality 1 total number features, set size differs set size addition feature next highest rank (according proposed entropy measure).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Many feature sets performed well, far outperformed best results using feature selection methods.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, across 10 experimental tasks, consistent range feature ranks feature set sizes correlated good performance.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	could selected threshold might work reasonably well data, would little confidence would work well general, considering inconsistent pattern results.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	5.4 Semi-Supervised Feature Selection.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Unsupervised methods Dash et al.’s (1997) appealing require knowledge external data.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, many aspects computational linguistics, found small amount labelled data contains sufficient information allow us go beyond limits completely unsupervised approaches.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	domain particular, verb class discovery “in vacuum” necessary.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	plausible scenario researchers would examples verbs believe fall different classes interest, want separate verbs along lines.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	model kind approach, selected sample five seed verbs class.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	set verbs judged (by authors’ intuition alone) “representative” class.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	purposely carry linguistic analysis, although check verb reasonably frequent (with log frequencies ranging 2.6 5.1).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	experimental task, ran supervised Table 3: Feature counts Ling Seed feature sets.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	learner (C5.0) seed verbs classes, 5-fold cross-validation (without boosting).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	extracted resulting decision trees union features used, formed reduced feature set task.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	clustering experiment used full set 20 verbs per class; i.e., seed verbs included, following proposed model guided verb class discovery.5 results using feature sets shown third subcolumn (Seed) three evaluation measures Table 2.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	feature selection method highly successful, outperforming full feature set (Full) tasks, performing close remainder.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Moreover, seed set features outperforms manually selected set (Ling) half tasks.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	importantly, Seed set shows mean overall reduction error rate (over Base ) 28%, compared 17% Ling set.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	increased reduction error rate particularly striking 2-way tasks, 37% Seed set compared 20% Ling set.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Another striking result difference values, much higher Ling (which turn much higher Full).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Thus, see sizeable increase performance, also obtain tighter better separated clusters proposed feature selection approach.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	5.5 Discussion.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	clustering experiments, find smaller subsets features generally perform better full set features.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	(See Table 3 number features Ling Seed sets.)	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, small set features adequate.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	ran 50 experiments using randomly selected sets features cardinality , 5We also tried directly applying mutual information (MI) measure used decision-tree induction (Quinlan, 1986).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	calculated MI feature respect classification seed verbs, computed clusterings using features certain MI threshold.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	method work well running C5.0, presumably captures important feature interactions ignored individual MI calculations.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	number classes (a simple linear function roughly approximating number features Seed sets).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Mean clusterings much lower Seed sets, extremely low (below .08 cases).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Interestingly, generally high, indicating structure data, matches classification.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	confirms appropriate feature selection, small number features, important task verb class discovery.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	also find semi-supervised method (Seed) linguistically plausible, performs well better features manually determined based linguistic knowledge (Ling).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	might also ask, would subset verbs well?	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	answer this, ran experiments using 50 different randomly selected seed verb sets class.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	found mean values Seed set reported above, mean little lower.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	tentatively conclude that, yes, subset verbs appropriate class may sufficient seed set, although sets better others.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	promising method, shows precise selection seed set verbs crucial success semi-supervised approach.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Using measure ours, Stevenson Merlo (1999) achieved performance clustering close supervised classification.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, study used small set five features manually devised set three particular classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	feature set essentially generalization theirs, scaling feature space useful across English verb classes general, necessarily face dimensionality problem arise research.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Schulte im Walde Brew (2002) Schulte im Walde (2003), hand, use larger set features intended useful broad number classes, work.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	scores Schulte im Walde (2003) range .09 .18, range .02 .34, mean .17 across tasks.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	However, Schulte im Walde’s features rely accurate subcategorization statistics, experiments include much larger set classes (around 40), much smaller number verbs (average around 4).	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Performance differences may due types features (ours noisier, capture information beyond subcat), due number size classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	results generally decrease increase number classes, indicating tasks general may “easier” 40-way distinction, classes also many members (20 versus average 4) need grouped together.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	question future research explore effect variables clustering performance.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	explored manual, unsupervised, semi- supervised methods feature selection clustering approach verb class discovery.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	find manual selection subset features based known classification performs better using full set noisy features, demonstrating potential benefit feature selection task.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	unsupervised method tried (Dash et al., 1997) prove useful, problem consistent threshold feature inclusion.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	instead proposed semi-supervised method seed set verbs chosen training supervised classifier, useful features extracted use clustering.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	showed feature set outperformed full manually selected sets features three clustering evaluation metrics.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Furthermore, method relatively insensitive precise makeup selected seed set.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	successful seed set features is, still achieve accuracy supervised learner.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	research needed definition general feature space, well methods selecting useful set features clustering.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Furthermore, might question clustering approach itself, context verb class discovery.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	Rather trying separate set new verbs coherent clusters, suggest may useful perform nearest-neighbour type classification using seed set, asking new verb “is like not?” ways current clustering task easy, verbs one target classes.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	ways, however, difficult: learner distinguish multiple classes, rather focus important properties single class.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	next step explore issues, investigate methods appropriate practical problem grouping verbs new language.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	indebted Allan Jepson helpful discussions suggestions.	0
Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.	gratefully acknowledge financial support NSERC Canada Bell University Labs.	0
