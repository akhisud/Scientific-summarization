AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	1
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	found far existing implementations using character-based IOB tagging.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	give detailed description approach Section 2.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	∗ second author affiliated NTT.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	OOV recognition important word segmentation, higher IV rate also desired.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	work propose confidence measure approach lessen weakness.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	approach change R-oovs R-ivs find optimal tradeoff.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	approach described Section 2.2.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Section 3 presents experimental results.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Section 5 provides concluding remarks.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	word segmentation process illustrated Fig.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	1.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	example exhibiting step’s results also given figure.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Since dictionary-based approach well-known method, skip technical descriptions.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	use advantage confidence measure approach.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	2.1 Subword-based IOB tagging using CRFs.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	several steps train subword-based IOB tag- ger.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	chose single characters top multi- character words lexicon subset IOB tagging.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	subset consists Chinese characters only, character-based IOB tagger.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	regard words subset subwords IOB tagging.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	character-based IOB tagger, one possibility re-segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	However, multiple choices subword-based IOB tagger.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	course, backward maximal match (BMM) approaches also applicable.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	order overcome overfitting, gaussian prior imposed training.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	subscripts position indicators.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	defined cutoff value feature type selected features occurrence counts cutoff.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	forward-backward algorithm used training viterbi algorithm used decoding.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	2.2 Confidence-dependent word segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	However, neither perfect.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	section introduce confidence measure approach combine two results.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	found value 0.7 α, empirically.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Eq. 2 results IOB tagging reevaluated.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	confidence measure threshold, t, defined making decision based value.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	new OOV thus created.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Section 3.2 present experimental segmentation results confidence measure approach.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	detailed info.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	corpora scores, refer (Emerson, 2005).	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	dictionary-based approach, extracted word list training data vocabulary.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Table 1 shows performance dictionary-based segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Since single-character words present test data training data, R-oov rates zero experiment.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	fact, OOV recognition.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Hence, approach produced lower F-scores.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	However, R-ivs high.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	3.1 Effects Character-based the.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	character-based tagging, used Chinese characters.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	67 8 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	70 0 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	78 3 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	71 0 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	upper numbers character- based lower ones, subword-based.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	using FMM, labeled “IOB” tags CRFs.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	F-score changes PKU corpora, recall rates improved.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	However, R-iv rates getting worse return higher R-oov rates.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	tackle problem confidence measure approach.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	3.2 Effect confidence measure.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	slot, numbers top character-based approach numbers bottom subword-based.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	60 7 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	68 2 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	77 5 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	67 4 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	95 2 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	9 4 3 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	96 4 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	95 0 u r 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	95 1 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	9 5 1 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	97 1 0.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	proves proposed word-based IOB tagging effective.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	IOB tagging approach adopted work new idea.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	main contribution extend IOB tagging approach character-based subword-based.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	proved new approach enhanced word segmentation significantly.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	achieved highest F-scores CITYU, PKU MSR corpora.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	think proposed subword- based tagging played important role good results.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	could yield better results shown Table 4 using information.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	approaches produced wrong segmentations labeling inconsistency.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Another advantage word-based IOB tagging character-based speed.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	subword-based approach faster fewer words characters labeled.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	found speed training test.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	work used delicately.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
AS C U MS R PK U (Zh ang et al., 200 6) 95.  2 Table 5: Segmentation performance presented in previous work and of our combination model.	authors appreciate reviewers’ effort good advice improving paper.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	found far existing implementations using character-based IOB tagging.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	give detailed description approach Section 2.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	∗ second author affiliated NTT.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	OOV recognition important word segmentation, higher IV rate also desired.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	work propose confidence measure approach lessen weakness.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	approach change R-oovs R-ivs find optimal tradeoff.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	approach described Section 2.2.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Section 3 presents experimental results.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Section 5 provides concluding remarks.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	word segmentation process illustrated Fig.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	1.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	example exhibiting step’s results also given figure.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Since dictionary-based approach well-known method, skip technical descriptions.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	However, keep mind dictionary-based approach produce higher R-iv rate.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	use advantage confidence measure approach.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	2.1 Subword-based IOB tagging using CRFs.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	several steps train subword-based IOB tag- ger.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	chose single characters top multi- character words lexicon subset IOB tagging.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	subset consists Chinese characters only, character-based IOB tagger.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	regard words subset subwords IOB tagging.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	character-based IOB tagger, one possibility re-segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	However, multiple choices subword-based IOB tagger.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	course, backward maximal match (BMM) approaches also applicable.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	order overcome overfitting, gaussian prior imposed training.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	subscripts position indicators.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	defined cutoff value feature type selected features occurrence counts cutoff.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	forward-backward algorithm used training viterbi algorithm used decoding.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	2.2 Confidence-dependent word segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	However, neither perfect.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	section introduce confidence measure approach combine two results.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	found value 0.7 α, empirically.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Eq. 2 results IOB tagging reevaluated.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	confidence measure threshold, t, defined making decision based value.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	new OOV thus created.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Section 3.2 present experimental segmentation results confidence measure approach.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	detailed info.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	corpora scores, refer (Emerson, 2005).	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	dictionary-based approach, extracted word list training data vocabulary.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Table 1 shows performance dictionary-based segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Since single-character words present test data training data, R-oov rates zero experiment.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	fact, OOV recognition.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Hence, approach produced lower F-scores.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	However, R-ivs high.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	3.1 Effects Character-based the.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	character-based tagging, used Chinese characters.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	67 8 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	70 0 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	78 3 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	71 0 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	upper numbers character- based lower ones, subword-based.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	using FMM, labeled “IOB” tags CRFs.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	F-score changes PKU corpora, recall rates improved.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	However, R-iv rates getting worse return higher R-oov rates.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	tackle problem confidence measure approach.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	3.2 Effect confidence measure.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	slot, numbers top character-based approach numbers bottom subword-based.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	60 7 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	68 2 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	77 5 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	67 4 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	95 2 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	9 4 3 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	96 4 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	95 0 u r 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	95 1 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	9 5 1 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	97 1 0.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	proves proposed word-based IOB tagging effective.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	IOB tagging approach adopted work new idea.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	main contribution extend IOB tagging approach character-based subword-based.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	proved new approach enhanced word segmentation significantly.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	achieved highest F-scores CITYU, PKU MSR corpora.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	think proposed subword- based tagging played important role good results.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	could yield better results shown Table 4 using information.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	example, inconsistent errors foreign names fixed alphabetical characters known.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	approaches produced wrong segmentations labeling inconsistency.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Another advantage word-based IOB tagging character-based speed.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	subword-based approach faster fewer words characters labeled.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	found speed training test.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	work used delicately.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	also successfully employed confidence measure make confidence-dependent word segmentation.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.  (2006).	authors appreciate reviewers’ effort good advice improving paper.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	1
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	found far existing implementations using character-based IOB tagging.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	give detailed description approach Section 2.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	∗ second author affiliated NTT.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	OOV recognition important word segmentation, higher IV rate also desired.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	work propose confidence measure approach lessen weakness.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	approach change R-oovs R-ivs find optimal tradeoff.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	approach described Section 2.2.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Section 3 presents experimental results.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Section 5 provides concluding remarks.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	word segmentation process illustrated Fig.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	1.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	example exhibiting step’s results also given figure.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Since dictionary-based approach well-known method, skip technical descriptions.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	use advantage confidence measure approach.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	2.1 Subword-based IOB tagging using CRFs.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	several steps train subword-based IOB tag- ger.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	chose single characters top multi- character words lexicon subset IOB tagging.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	subset consists Chinese characters only, character-based IOB tagger.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	regard words subset subwords IOB tagging.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	character-based IOB tagger, one possibility re-segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	However, multiple choices subword-based IOB tagger.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	course, backward maximal match (BMM) approaches also applicable.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	order overcome overfitting, gaussian prior imposed training.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	subscripts position indicators.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	defined cutoff value feature type selected features occurrence counts cutoff.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	forward-backward algorithm used training viterbi algorithm used decoding.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	2.2 Confidence-dependent word segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	However, neither perfect.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	section introduce confidence measure approach combine two results.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	found value 0.7 α, empirically.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Eq. 2 results IOB tagging reevaluated.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	confidence measure threshold, t, defined making decision based value.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	new OOV thus created.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Section 3.2 present experimental segmentation results confidence measure approach.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	detailed info.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	corpora scores, refer (Emerson, 2005).	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	dictionary-based approach, extracted word list training data vocabulary.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Table 1 shows performance dictionary-based segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Since single-character words present test data training data, R-oov rates zero experiment.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	fact, OOV recognition.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Hence, approach produced lower F-scores.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	However, R-ivs high.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	3.1 Effects Character-based the.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	character-based tagging, used Chinese characters.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	67 8 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	70 0 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	78 3 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	71 0 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	upper numbers character- based lower ones, subword-based.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	using FMM, labeled “IOB” tags CRFs.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	F-score changes PKU corpora, recall rates improved.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	However, R-iv rates getting worse return higher R-oov rates.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	tackle problem confidence measure approach.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	3.2 Effect confidence measure.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	slot, numbers top character-based approach numbers bottom subword-based.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	60 7 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	68 2 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	77 5 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	67 4 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	95 2 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	9 4 3 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	96 4 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	95 0 u r 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	95 1 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	9 5 1 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	97 1 0.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	proves proposed word-based IOB tagging effective.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	IOB tagging approach adopted work new idea.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	main contribution extend IOB tagging approach character-based subword-based.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	proved new approach enhanced word segmentation significantly.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	achieved highest F-scores CITYU, PKU MSR corpora.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	think proposed subword- based tagging played important role good results.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	could yield better results shown Table 4 using information.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	approaches produced wrong segmentations labeling inconsistency.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Another advantage word-based IOB tagging character-based speed.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	subword-based approach faster fewer words characters labeled.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	found speed training test.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	work used delicately.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).	authors appreciate reviewers’ effort good advice improving paper.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	found far existing implementations using character-based IOB tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	give detailed description approach Section 2.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	∗ second author affiliated NTT.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	OOV recognition important word segmentation, higher IV rate also desired.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	work propose confidence measure approach lessen weakness.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	approach change R-oovs R-ivs find optimal tradeoff.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	approach described Section 2.2.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Section 3 presents experimental results.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Section 5 provides concluding remarks.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	word segmentation process illustrated Fig.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	1.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	example exhibiting step’s results also given figure.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Since dictionary-based approach well-known method, skip technical descriptions.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	use advantage confidence measure approach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	2.1 Subword-based IOB tagging using CRFs.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	several steps train subword-based IOB tag- ger.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	chose single characters top multi- character words lexicon subset IOB tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	subset consists Chinese characters only, character-based IOB tagger.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	regard words subset subwords IOB tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	character-based IOB tagger, one possibility re-segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	However, multiple choices subword-based IOB tagger.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	course, backward maximal match (BMM) approaches also applicable.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	order overcome overfitting, gaussian prior imposed training.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	subscripts position indicators.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	defined cutoff value feature type selected features occurrence counts cutoff.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	forward-backward algorithm used training viterbi algorithm used decoding.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	2.2 Confidence-dependent word segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	However, neither perfect.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	section introduce confidence measure approach combine two results.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	found value 0.7 α, empirically.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Eq. 2 results IOB tagging reevaluated.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	confidence measure threshold, t, defined making decision based value.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	new OOV thus created.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Section 3.2 present experimental segmentation results confidence measure approach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	detailed info.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	corpora scores, refer (Emerson, 2005).	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	dictionary-based approach, extracted word list training data vocabulary.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Table 1 shows performance dictionary-based segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Since single-character words present test data training data, R-oov rates zero experiment.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	fact, OOV recognition.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Hence, approach produced lower F-scores.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	However, R-ivs high.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	3.1 Effects Character-based the.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	character-based tagging, used Chinese characters.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	67 8 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	70 0 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	78 3 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	71 0 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	upper numbers character- based lower ones, subword-based.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	using FMM, labeled “IOB” tags CRFs.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	F-score changes PKU corpora, recall rates improved.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	However, R-iv rates getting worse return higher R-oov rates.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	tackle problem confidence measure approach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	3.2 Effect confidence measure.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	slot, numbers top character-based approach numbers bottom subword-based.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	60 7 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	68 2 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	77 5 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	67 4 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	95 2 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	9 4 3 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	96 4 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	95 0 u r 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	95 1 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	9 5 1 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	97 1 0.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	proves proposed word-based IOB tagging effective.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	IOB tagging approach adopted work new idea.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	main contribution extend IOB tagging approach character-based subword-based.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	proved new approach enhanced word segmentation significantly.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	achieved highest F-scores CITYU, PKU MSR corpora.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	think proposed subword- based tagging played important role good results.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	could yield better results shown Table 4 using information.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	approaches produced wrong segmentations labeling inconsistency.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Another advantage word-based IOB tagging character-based speed.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	subword-based approach faster fewer words characters labeled.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	found speed training test.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	work used delicately.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).	authors appreciate reviewers’ effort good advice improving paper.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	found far existing implementations using character-based IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	give detailed description approach Section 2.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	∗ second author affiliated NTT.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	OOV recognition important word segmentation, higher IV rate also desired.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	work propose confidence measure approach lessen weakness.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	approach change R-oovs R-ivs find optimal tradeoff.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	approach described Section 2.2.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Section 3 presents experimental results.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Section 5 provides concluding remarks.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	word segmentation process illustrated Fig.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	1.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	example exhibiting step’s results also given figure.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Since dictionary-based approach well-known method, skip technical descriptions.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	However, keep mind dictionary-based approach produce higher R-iv rate.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	use advantage confidence measure approach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	2.1 Subword-based IOB tagging using CRFs.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	several steps train subword-based IOB tag- ger.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	chose single characters top multi- character words lexicon subset IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	subset consists Chinese characters only, character-based IOB tagger.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	regard words subset subwords IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	character-based IOB tagger, one possibility re-segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	However, multiple choices subword-based IOB tagger.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	course, backward maximal match (BMM) approaches also applicable.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	order overcome overfitting, gaussian prior imposed training.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	subscripts position indicators.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	defined cutoff value feature type selected features occurrence counts cutoff.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	forward-backward algorithm used training viterbi algorithm used decoding.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	2.2 Confidence-dependent word segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	However, neither perfect.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	section introduce confidence measure approach combine two results.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	found value 0.7 α, empirically.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Eq. 2 results IOB tagging reevaluated.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	confidence measure threshold, t, defined making decision based value.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	new OOV thus created.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Section 3.2 present experimental segmentation results confidence measure approach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	detailed info.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	corpora scores, refer (Emerson, 2005).	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	dictionary-based approach, extracted word list training data vocabulary.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Table 1 shows performance dictionary-based segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Since single-character words present test data training data, R-oov rates zero experiment.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	fact, OOV recognition.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Hence, approach produced lower F-scores.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	However, R-ivs high.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	3.1 Effects Character-based the.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	character-based tagging, used Chinese characters.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	67 8 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	70 0 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	78 3 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	71 0 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	upper numbers character- based lower ones, subword-based.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	using FMM, labeled “IOB” tags CRFs.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	F-score changes PKU corpora, recall rates improved.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	1
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	However, R-iv rates getting worse return higher R-oov rates.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	tackle problem confidence measure approach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	3.2 Effect confidence measure.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	slot, numbers top character-based approach numbers bottom subword-based.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	60 7 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	68 2 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	77 5 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	67 4 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	95 2 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	9 4 3 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	96 4 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	95 0 u r 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	95 1 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	9 5 1 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	97 1 0.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	proves proposed word-based IOB tagging effective.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	IOB tagging approach adopted work new idea.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	main contribution extend IOB tagging approach character-based subword-based.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	proved new approach enhanced word segmentation significantly.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	achieved highest F-scores CITYU, PKU MSR corpora.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	think proposed subword- based tagging played important role good results.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	could yield better results shown Table 4 using information.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	example, inconsistent errors foreign names fixed alphabetical characters known.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	approaches produced wrong segmentations labeling inconsistency.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Another advantage word-based IOB tagging character-based speed.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	subword-based approach faster fewer words characters labeled.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	found speed training test.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	work used delicately.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	also successfully employed confidence measure make confidence-dependent word segmentation.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).	authors appreciate reviewers’ effort good advice improving paper.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	found far existing implementations using character-based IOB tagging.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	give detailed description approach Section 2.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	∗ second author affiliated NTT.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	OOV recognition important word segmentation, higher IV rate also desired.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	work propose confidence measure approach lessen weakness.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	approach change R-oovs R-ivs find optimal tradeoff.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	approach described Section 2.2.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Section 3 presents experimental results.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Section 5 provides concluding remarks.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	word segmentation process illustrated Fig.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	1.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	example exhibiting step’s results also given figure.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Since dictionary-based approach well-known method, skip technical descriptions.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	However, keep mind dictionary-based approach produce higher R-iv rate.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	use advantage confidence measure approach.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	2.1 Subword-based IOB tagging using CRFs.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	several steps train subword-based IOB tag- ger.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	chose single characters top multi- character words lexicon subset IOB tagging.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	subset consists Chinese characters only, character-based IOB tagger.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	regard words subset subwords IOB tagging.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	character-based IOB tagger, one possibility re-segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	However, multiple choices subword-based IOB tagger.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	course, backward maximal match (BMM) approaches also applicable.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	order overcome overfitting, gaussian prior imposed training.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	subscripts position indicators.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	defined cutoff value feature type selected features occurrence counts cutoff.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	forward-backward algorithm used training viterbi algorithm used decoding.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	2.2 Confidence-dependent word segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	However, neither perfect.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	section introduce confidence measure approach combine two results.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	found value 0.7 α, empirically.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Eq. 2 results IOB tagging reevaluated.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	confidence measure threshold, t, defined making decision based value.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	new OOV thus created.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Section 3.2 present experimental segmentation results confidence measure approach.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	detailed info.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	corpora scores, refer (Emerson, 2005).	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	dictionary-based approach, extracted word list training data vocabulary.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Table 1 shows performance dictionary-based segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Since single-character words present test data training data, R-oov rates zero experiment.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	fact, OOV recognition.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Hence, approach produced lower F-scores.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	However, R-ivs high.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	3.1 Effects Character-based the.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	character-based tagging, used Chinese characters.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	67 8 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	70 0 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	78 3 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	71 0 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	upper numbers character- based lower ones, subword-based.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	using FMM, labeled “IOB” tags CRFs.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	F-score changes PKU corpora, recall rates improved.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	However, R-iv rates getting worse return higher R-oov rates.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	tackle problem confidence measure approach.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	3.2 Effect confidence measure.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	slot, numbers top character-based approach numbers bottom subword-based.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	60 7 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	68 2 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	77 5 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	67 4 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	95 2 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	9 4 3 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	96 4 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	95 0 u r 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	95 1 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	9 5 1 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	97 1 0.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	proves proposed word-based IOB tagging effective.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	IOB tagging approach adopted work new idea.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	main contribution extend IOB tagging approach character-based subword-based.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	proved new approach enhanced word segmentation significantly.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	achieved highest F-scores CITYU, PKU MSR corpora.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	think proposed subword- based tagging played important role good results.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	could yield better results shown Table 4 using information.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	example, inconsistent errors foreign names fixed alphabetical characters known.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	approaches produced wrong segmentations labeling inconsistency.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Another advantage word-based IOB tagging character-based speed.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	subword-based approach faster fewer words characters labeled.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	found speed training test.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	work used delicately.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	also successfully employed confidence measure make confidence-dependent word segmentation.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).	authors appreciate reviewers’ effort good advice improving paper.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	found far existing implementations using character-based IOB tagging.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	give detailed description approach Section 2.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	∗ second author affiliated NTT.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	OOV recognition important word segmentation, higher IV rate also desired.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	work propose confidence measure approach lessen weakness.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	approach change R-oovs R-ivs find optimal tradeoff.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	approach described Section 2.2.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Section 3 presents experimental results.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Section 5 provides concluding remarks.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	word segmentation process illustrated Fig.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	1.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	example exhibiting step’s results also given figure.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Since dictionary-based approach well-known method, skip technical descriptions.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	use advantage confidence measure approach.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	2.1 Subword-based IOB tagging using CRFs.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	several steps train subword-based IOB tag- ger.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	chose single characters top multi- character words lexicon subset IOB tagging.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	subset consists Chinese characters only, character-based IOB tagger.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	regard words subset subwords IOB tagging.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	character-based IOB tagger, one possibility re-segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	However, multiple choices subword-based IOB tagger.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	course, backward maximal match (BMM) approaches also applicable.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	order overcome overfitting, gaussian prior imposed training.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	subscripts position indicators.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	defined cutoff value feature type selected features occurrence counts cutoff.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	forward-backward algorithm used training viterbi algorithm used decoding.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	2.2 Confidence-dependent word segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	However, neither perfect.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	section introduce confidence measure approach combine two results.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	found value 0.7 α, empirically.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Eq. 2 results IOB tagging reevaluated.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	confidence measure threshold, t, defined making decision based value.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	new OOV thus created.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Section 3.2 present experimental segmentation results confidence measure approach.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	detailed info.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	corpora scores, refer (Emerson, 2005).	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	dictionary-based approach, extracted word list training data vocabulary.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Table 1 shows performance dictionary-based segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Since single-character words present test data training data, R-oov rates zero experiment.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	fact, OOV recognition.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Hence, approach produced lower F-scores.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	However, R-ivs high.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	3.1 Effects Character-based the.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	character-based tagging, used Chinese characters.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	67 8 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	70 0 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	78 3 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	71 0 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	upper numbers character- based lower ones, subword-based.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	using FMM, labeled “IOB” tags CRFs.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	F-score changes PKU corpora, recall rates improved.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	However, R-iv rates getting worse return higher R-oov rates.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	tackle problem confidence measure approach.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	3.2 Effect confidence measure.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	slot, numbers top character-based approach numbers bottom subword-based.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	60 7 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	68 2 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	77 5 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	67 4 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	95 2 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	9 4 3 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	96 4 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	95 0 u r 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	95 1 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	9 5 1 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	97 1 0.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	proves proposed word-based IOB tagging effective.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	IOB tagging approach adopted work new idea.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	main contribution extend IOB tagging approach character-based subword-based.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	proved new approach enhanced word segmentation significantly.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	achieved highest F-scores CITYU, PKU MSR corpora.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	think proposed subword- based tagging played important role good results.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	could yield better results shown Table 4 using information.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	approaches produced wrong segmentations labeling inconsistency.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Another advantage word-based IOB tagging character-based speed.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	subword-based approach faster fewer words characters labeled.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	found speed training test.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	work used delicately.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.	authors appreciate reviewers’ effort good advice improving paper.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	found far existing implementations using character-based IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	1
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	give detailed description approach Section 2.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	∗ second author affiliated NTT.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	OOV recognition important word segmentation, higher IV rate also desired.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	work propose confidence measure approach lessen weakness.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	approach change R-oovs R-ivs find optimal tradeoff.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	approach described Section 2.2.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Section 3 presents experimental results.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Section 5 provides concluding remarks.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	word segmentation process illustrated Fig.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	1.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	example exhibiting step’s results also given figure.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Since dictionary-based approach well-known method, skip technical descriptions.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	use advantage confidence measure approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	2.1 Subword-based IOB tagging using CRFs.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	several steps train subword-based IOB tag- ger.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	chose single characters top multi- character words lexicon subset IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	subset consists Chinese characters only, character-based IOB tagger.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	regard words subset subwords IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	character-based IOB tagger, one possibility re-segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	However, multiple choices subword-based IOB tagger.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	course, backward maximal match (BMM) approaches also applicable.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	order overcome overfitting, gaussian prior imposed training.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	subscripts position indicators.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	defined cutoff value feature type selected features occurrence counts cutoff.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	forward-backward algorithm used training viterbi algorithm used decoding.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	2.2 Confidence-dependent word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	However, neither perfect.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	section introduce confidence measure approach combine two results.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	found value 0.7 α, empirically.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Eq. 2 results IOB tagging reevaluated.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	confidence measure threshold, t, defined making decision based value.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	new OOV thus created.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Section 3.2 present experimental segmentation results confidence measure approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	detailed info.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	corpora scores, refer (Emerson, 2005).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	dictionary-based approach, extracted word list training data vocabulary.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Table 1 shows performance dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Since single-character words present test data training data, R-oov rates zero experiment.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	fact, OOV recognition.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Hence, approach produced lower F-scores.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	However, R-ivs high.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	3.1 Effects Character-based the.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	character-based tagging, used Chinese characters.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	67 8 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	70 0 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	78 3 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	71 0 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	upper numbers character- based lower ones, subword-based.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	using FMM, labeled “IOB” tags CRFs.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	F-score changes PKU corpora, recall rates improved.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	However, R-iv rates getting worse return higher R-oov rates.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	tackle problem confidence measure approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	3.2 Effect confidence measure.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	slot, numbers top character-based approach numbers bottom subword-based.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	60 7 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	68 2 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	77 5 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	67 4 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	95 2 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	9 4 3 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	96 4 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	95 0 u r 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	95 1 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	9 5 1 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	97 1 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	proves proposed word-based IOB tagging effective.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	IOB tagging approach adopted work new idea.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	main contribution extend IOB tagging approach character-based subword-based.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	proved new approach enhanced word segmentation significantly.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	achieved highest F-scores CITYU, PKU MSR corpora.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	think proposed subword- based tagging played important role good results.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	could yield better results shown Table 4 using information.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	approaches produced wrong segmentations labeling inconsistency.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Another advantage word-based IOB tagging character-based speed.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	subword-based approach faster fewer words characters labeled.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	found speed training test.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	work used delicately.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.	authors appreciate reviewers’ effort good advice improving paper.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	found far existing implementations using character-based IOB tagging.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	give detailed description approach Section 2.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	∗ second author affiliated NTT.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	OOV recognition important word segmentation, higher IV rate also desired.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	work propose confidence measure approach lessen weakness.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	approach change R-oovs R-ivs find optimal tradeoff.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	approach described Section 2.2.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Section 3 presents experimental results.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Section 5 provides concluding remarks.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	word segmentation process illustrated Fig.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	1.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	1
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	example exhibiting step’s results also given figure.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Since dictionary-based approach well-known method, skip technical descriptions.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	use advantage confidence measure approach.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	2.1 Subword-based IOB tagging using CRFs.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	several steps train subword-based IOB tag- ger.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	chose single characters top multi- character words lexicon subset IOB tagging.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	subset consists Chinese characters only, character-based IOB tagger.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	regard words subset subwords IOB tagging.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	character-based IOB tagger, one possibility re-segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	However, multiple choices subword-based IOB tagger.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	course, backward maximal match (BMM) approaches also applicable.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	order overcome overfitting, gaussian prior imposed training.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	subscripts position indicators.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	defined cutoff value feature type selected features occurrence counts cutoff.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	forward-backward algorithm used training viterbi algorithm used decoding.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	2.2 Confidence-dependent word segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	However, neither perfect.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	section introduce confidence measure approach combine two results.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	found value 0.7 α, empirically.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Eq. 2 results IOB tagging reevaluated.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	confidence measure threshold, t, defined making decision based value.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	new OOV thus created.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Section 3.2 present experimental segmentation results confidence measure approach.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	detailed info.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	corpora scores, refer (Emerson, 2005).	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	dictionary-based approach, extracted word list training data vocabulary.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Table 1 shows performance dictionary-based segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Since single-character words present test data training data, R-oov rates zero experiment.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	fact, OOV recognition.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Hence, approach produced lower F-scores.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	However, R-ivs high.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	3.1 Effects Character-based the.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	character-based tagging, used Chinese characters.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	67 8 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	70 0 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	78 3 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	71 0 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	upper numbers character- based lower ones, subword-based.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	using FMM, labeled “IOB” tags CRFs.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	F-score changes PKU corpora, recall rates improved.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	However, R-iv rates getting worse return higher R-oov rates.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	tackle problem confidence measure approach.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	3.2 Effect confidence measure.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	slot, numbers top character-based approach numbers bottom subword-based.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	60 7 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	68 2 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	77 5 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	67 4 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	95 2 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	9 4 3 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	96 4 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	95 0 u r 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	95 1 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	9 5 1 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	97 1 0.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	proves proposed word-based IOB tagging effective.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	IOB tagging approach adopted work new idea.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	main contribution extend IOB tagging approach character-based subword-based.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	proved new approach enhanced word segmentation significantly.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	achieved highest F-scores CITYU, PKU MSR corpora.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	think proposed subword- based tagging played important role good results.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	could yield better results shown Table 4 using information.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	approaches produced wrong segmentations labeling inconsistency.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Another advantage word-based IOB tagging character-based speed.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	subword-based approach faster fewer words characters labeled.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	found speed training test.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	work used delicately.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);	authors appreciate reviewers’ effort good advice improving paper.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	1
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	found far existing implementations using character-based IOB tagging.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	give detailed description approach Section 2.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	∗ second author affiliated NTT.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	OOV recognition important word segmentation, higher IV rate also desired.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	work propose confidence measure approach lessen weakness.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	approach change R-oovs R-ivs find optimal tradeoff.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	approach described Section 2.2.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Section 3 presents experimental results.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Section 5 provides concluding remarks.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	word segmentation process illustrated Fig.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	1.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	example exhibiting step’s results also given figure.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Since dictionary-based approach well-known method, skip technical descriptions.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	use advantage confidence measure approach.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	2.1 Subword-based IOB tagging using CRFs.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	several steps train subword-based IOB tag- ger.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	chose single characters top multi- character words lexicon subset IOB tagging.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	subset consists Chinese characters only, character-based IOB tagger.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	regard words subset subwords IOB tagging.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	character-based IOB tagger, one possibility re-segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	However, multiple choices subword-based IOB tagger.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	course, backward maximal match (BMM) approaches also applicable.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	order overcome overfitting, gaussian prior imposed training.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	subscripts position indicators.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	defined cutoff value feature type selected features occurrence counts cutoff.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	forward-backward algorithm used training viterbi algorithm used decoding.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	2.2 Confidence-dependent word segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	However, neither perfect.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	section introduce confidence measure approach combine two results.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	found value 0.7 α, empirically.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Eq. 2 results IOB tagging reevaluated.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	confidence measure threshold, t, defined making decision based value.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	new OOV thus created.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Section 3.2 present experimental segmentation results confidence measure approach.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	detailed info.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	corpora scores, refer (Emerson, 2005).	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	dictionary-based approach, extracted word list training data vocabulary.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Table 1 shows performance dictionary-based segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Since single-character words present test data training data, R-oov rates zero experiment.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	fact, OOV recognition.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Hence, approach produced lower F-scores.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	However, R-ivs high.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	3.1 Effects Character-based the.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	character-based tagging, used Chinese characters.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	67 8 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	70 0 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	78 3 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	71 0 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	upper numbers character- based lower ones, subword-based.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	using FMM, labeled “IOB” tags CRFs.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	F-score changes PKU corpora, recall rates improved.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	However, R-iv rates getting worse return higher R-oov rates.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	tackle problem confidence measure approach.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	3.2 Effect confidence measure.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	slot, numbers top character-based approach numbers bottom subword-based.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	60 7 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	68 2 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	77 5 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	67 4 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	95 2 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	9 4 3 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	96 4 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	95 0 u r 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	95 1 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	9 5 1 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	97 1 0.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	proves proposed word-based IOB tagging effective.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	IOB tagging approach adopted work new idea.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	main contribution extend IOB tagging approach character-based subword-based.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	proved new approach enhanced word segmentation significantly.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	achieved highest F-scores CITYU, PKU MSR corpora.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	think proposed subword- based tagging played important role good results.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	could yield better results shown Table 4 using information.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	approaches produced wrong segmentations labeling inconsistency.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Another advantage word-based IOB tagging character-based speed.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	subword-based approach faster fewer words characters labeled.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	found speed training test.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	work used delicately.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).	authors appreciate reviewers’ effort good advice improving paper.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	found far existing implementations using character-based IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	1
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	give detailed description approach Section 2.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	∗ second author affiliated NTT.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	OOV recognition important word segmentation, higher IV rate also desired.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	work propose confidence measure approach lessen weakness.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	approach change R-oovs R-ivs find optimal tradeoff.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	approach described Section 2.2.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Section 3 presents experimental results.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Section 5 provides concluding remarks.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	word segmentation process illustrated Fig.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	1.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	example exhibiting step’s results also given figure.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Since dictionary-based approach well-known method, skip technical descriptions.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	use advantage confidence measure approach.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	2.1 Subword-based IOB tagging using CRFs.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	several steps train subword-based IOB tag- ger.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	chose single characters top multi- character words lexicon subset IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	subset consists Chinese characters only, character-based IOB tagger.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	regard words subset subwords IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	character-based IOB tagger, one possibility re-segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	However, multiple choices subword-based IOB tagger.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	course, backward maximal match (BMM) approaches also applicable.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	order overcome overfitting, gaussian prior imposed training.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	subscripts position indicators.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	defined cutoff value feature type selected features occurrence counts cutoff.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	forward-backward algorithm used training viterbi algorithm used decoding.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	2.2 Confidence-dependent word segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	However, neither perfect.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	section introduce confidence measure approach combine two results.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	found value 0.7 α, empirically.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Eq. 2 results IOB tagging reevaluated.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	confidence measure threshold, t, defined making decision based value.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	new OOV thus created.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Section 3.2 present experimental segmentation results confidence measure approach.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	detailed info.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	corpora scores, refer (Emerson, 2005).	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	dictionary-based approach, extracted word list training data vocabulary.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Table 1 shows performance dictionary-based segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Since single-character words present test data training data, R-oov rates zero experiment.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	fact, OOV recognition.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Hence, approach produced lower F-scores.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	However, R-ivs high.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	3.1 Effects Character-based the.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	character-based tagging, used Chinese characters.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	67 8 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	70 0 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	78 3 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	71 0 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	upper numbers character- based lower ones, subword-based.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	using FMM, labeled “IOB” tags CRFs.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	F-score changes PKU corpora, recall rates improved.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	However, R-iv rates getting worse return higher R-oov rates.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	tackle problem confidence measure approach.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	3.2 Effect confidence measure.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	slot, numbers top character-based approach numbers bottom subword-based.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	60 7 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	68 2 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	77 5 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	67 4 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	95 2 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	9 4 3 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	96 4 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	95 0 u r 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	95 1 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	9 5 1 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	97 1 0.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	proves proposed word-based IOB tagging effective.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	IOB tagging approach adopted work new idea.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	main contribution extend IOB tagging approach character-based subword-based.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	proved new approach enhanced word segmentation significantly.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	achieved highest F-scores CITYU, PKU MSR corpora.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	think proposed subword- based tagging played important role good results.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	could yield better results shown Table 4 using information.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	approaches produced wrong segmentations labeling inconsistency.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Another advantage word-based IOB tagging character-based speed.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	subword-based approach faster fewer words characters labeled.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	found speed training test.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	work used delicately.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
One existing method that is based on sub-word information, Zhang et al.  (2006), combines a C R F and a rule-based model.	authors appreciate reviewers’ effort good advice improving paper.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	found far existing implementations using character-based IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	1
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	give detailed description approach Section 2.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	∗ second author affiliated NTT.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	OOV recognition important word segmentation, higher IV rate also desired.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	work propose confidence measure approach lessen weakness.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	approach change R-oovs R-ivs find optimal tradeoff.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	approach described Section 2.2.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Section 3 presents experimental results.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Section 5 provides concluding remarks.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	word segmentation process illustrated Fig.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	1.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	example exhibiting step’s results also given figure.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Since dictionary-based approach well-known method, skip technical descriptions.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	use advantage confidence measure approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	2.1 Subword-based IOB tagging using CRFs.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	several steps train subword-based IOB tag- ger.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	chose single characters top multi- character words lexicon subset IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	subset consists Chinese characters only, character-based IOB tagger.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	regard words subset subwords IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	character-based IOB tagger, one possibility re-segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	However, multiple choices subword-based IOB tagger.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	course, backward maximal match (BMM) approaches also applicable.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	order overcome overfitting, gaussian prior imposed training.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	subscripts position indicators.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	defined cutoff value feature type selected features occurrence counts cutoff.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	forward-backward algorithm used training viterbi algorithm used decoding.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	2.2 Confidence-dependent word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	However, neither perfect.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	section introduce confidence measure approach combine two results.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	found value 0.7 α, empirically.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Eq. 2 results IOB tagging reevaluated.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	confidence measure threshold, t, defined making decision based value.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	new OOV thus created.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Section 3.2 present experimental segmentation results confidence measure approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	detailed info.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	corpora scores, refer (Emerson, 2005).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	dictionary-based approach, extracted word list training data vocabulary.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Table 1 shows performance dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Since single-character words present test data training data, R-oov rates zero experiment.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	fact, OOV recognition.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Hence, approach produced lower F-scores.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	However, R-ivs high.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	3.1 Effects Character-based the.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	character-based tagging, used Chinese characters.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	67 8 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	70 0 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	78 3 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	71 0 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	upper numbers character- based lower ones, subword-based.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	using FMM, labeled “IOB” tags CRFs.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	F-score changes PKU corpora, recall rates improved.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	However, R-iv rates getting worse return higher R-oov rates.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	tackle problem confidence measure approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	3.2 Effect confidence measure.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	slot, numbers top character-based approach numbers bottom subword-based.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	60 7 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	68 2 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	77 5 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	67 4 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	95 2 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	9 4 3 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	96 4 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	95 0 u r 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	95 1 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	9 5 1 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	97 1 0.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	proves proposed word-based IOB tagging effective.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	IOB tagging approach adopted work new idea.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	main contribution extend IOB tagging approach character-based subword-based.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	proved new approach enhanced word segmentation significantly.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	achieved highest F-scores CITYU, PKU MSR corpora.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	think proposed subword- based tagging played important role good results.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	could yield better results shown Table 4 using information.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	approaches produced wrong segmentations labeling inconsistency.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Another advantage word-based IOB tagging character-based speed.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	subword-based approach faster fewer words characters labeled.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	found speed training test.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	work used delicately.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.  (2006) for comparison.	authors appreciate reviewers’ effort good advice improving paper.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	found far existing implementations using character-based IOB tagging.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	give detailed description approach Section 2.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	∗ second author affiliated NTT.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	OOV recognition important word segmentation, higher IV rate also desired.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	work propose confidence measure approach lessen weakness.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	approach change R-oovs R-ivs find optimal tradeoff.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	approach described Section 2.2.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Section 3 presents experimental results.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Section 5 provides concluding remarks.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	word segmentation process illustrated Fig.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	1.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	1
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	example exhibiting step’s results also given figure.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Since dictionary-based approach well-known method, skip technical descriptions.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	use advantage confidence measure approach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	2.1 Subword-based IOB tagging using CRFs.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	several steps train subword-based IOB tag- ger.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	chose single characters top multi- character words lexicon subset IOB tagging.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	subset consists Chinese characters only, character-based IOB tagger.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	regard words subset subwords IOB tagging.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	character-based IOB tagger, one possibility re-segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	However, multiple choices subword-based IOB tagger.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	course, backward maximal match (BMM) approaches also applicable.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	order overcome overfitting, gaussian prior imposed training.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	subscripts position indicators.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	defined cutoff value feature type selected features occurrence counts cutoff.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	forward-backward algorithm used training viterbi algorithm used decoding.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	2.2 Confidence-dependent word segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	However, neither perfect.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	section introduce confidence measure approach combine two results.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	found value 0.7 α, empirically.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Eq. 2 results IOB tagging reevaluated.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	confidence measure threshold, t, defined making decision based value.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	new OOV thus created.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Section 3.2 present experimental segmentation results confidence measure approach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	detailed info.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	corpora scores, refer (Emerson, 2005).	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	dictionary-based approach, extracted word list training data vocabulary.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Table 1 shows performance dictionary-based segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Since single-character words present test data training data, R-oov rates zero experiment.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	fact, OOV recognition.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Hence, approach produced lower F-scores.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	However, R-ivs high.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	3.1 Effects Character-based the.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	character-based tagging, used Chinese characters.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	67 8 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	70 0 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	78 3 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	71 0 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	upper numbers character- based lower ones, subword-based.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	using FMM, labeled “IOB” tags CRFs.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	F-score changes PKU corpora, recall rates improved.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	However, R-iv rates getting worse return higher R-oov rates.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	tackle problem confidence measure approach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	3.2 Effect confidence measure.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	slot, numbers top character-based approach numbers bottom subword-based.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	60 7 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	68 2 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	77 5 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	67 4 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	95 2 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	9 4 3 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	96 4 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	95 0 u r 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	95 1 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	9 5 1 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	97 1 0.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	proves proposed word-based IOB tagging effective.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	IOB tagging approach adopted work new idea.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	main contribution extend IOB tagging approach character-based subword-based.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	proved new approach enhanced word segmentation significantly.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	achieved highest F-scores CITYU, PKU MSR corpora.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	think proposed subword- based tagging played important role good results.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	could yield better results shown Table 4 using information.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	approaches produced wrong segmentations labeling inconsistency.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Another advantage word-based IOB tagging character-based speed.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	subword-based approach faster fewer words characters labeled.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	found speed training test.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	work used delicately.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.  (2006).	authors appreciate reviewers’ effort good advice improving paper.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	found far existing implementations using character-based IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	1
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	give detailed description approach Section 2.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	∗ second author affiliated NTT.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	OOV recognition important word segmentation, higher IV rate also desired.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	work propose confidence measure approach lessen weakness.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	approach change R-oovs R-ivs find optimal tradeoff.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	approach described Section 2.2.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Section 3 presents experimental results.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Section 5 provides concluding remarks.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	word segmentation process illustrated Fig.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	1.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	example exhibiting step’s results also given figure.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Since dictionary-based approach well-known method, skip technical descriptions.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	use advantage confidence measure approach.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	2.1 Subword-based IOB tagging using CRFs.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	several steps train subword-based IOB tag- ger.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	chose single characters top multi- character words lexicon subset IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	subset consists Chinese characters only, character-based IOB tagger.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	regard words subset subwords IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	character-based IOB tagger, one possibility re-segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	However, multiple choices subword-based IOB tagger.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	course, backward maximal match (BMM) approaches also applicable.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	order overcome overfitting, gaussian prior imposed training.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	subscripts position indicators.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	defined cutoff value feature type selected features occurrence counts cutoff.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	forward-backward algorithm used training viterbi algorithm used decoding.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	2.2 Confidence-dependent word segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	However, neither perfect.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	section introduce confidence measure approach combine two results.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	found value 0.7 α, empirically.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Eq. 2 results IOB tagging reevaluated.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	confidence measure threshold, t, defined making decision based value.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	new OOV thus created.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Section 3.2 present experimental segmentation results confidence measure approach.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	detailed info.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	corpora scores, refer (Emerson, 2005).	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	dictionary-based approach, extracted word list training data vocabulary.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Table 1 shows performance dictionary-based segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Since single-character words present test data training data, R-oov rates zero experiment.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	fact, OOV recognition.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Hence, approach produced lower F-scores.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	However, R-ivs high.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	3.1 Effects Character-based the.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	character-based tagging, used Chinese characters.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	67 8 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	70 0 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	78 3 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	71 0 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	upper numbers character- based lower ones, subword-based.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	using FMM, labeled “IOB” tags CRFs.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	F-score changes PKU corpora, recall rates improved.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	However, R-iv rates getting worse return higher R-oov rates.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	tackle problem confidence measure approach.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	3.2 Effect confidence measure.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	slot, numbers top character-based approach numbers bottom subword-based.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	60 7 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	68 2 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	77 5 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	67 4 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	95 2 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	9 4 3 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	96 4 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	95 0 u r 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	95 1 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	9 5 1 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	97 1 0.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	proves proposed word-based IOB tagging effective.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	IOB tagging approach adopted work new idea.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	main contribution extend IOB tagging approach character-based subword-based.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	proved new approach enhanced word segmentation significantly.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	achieved highest F-scores CITYU, PKU MSR corpora.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	think proposed subword- based tagging played important role good results.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	could yield better results shown Table 4 using information.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	approaches produced wrong segmentations labeling inconsistency.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Another advantage word-based IOB tagging character-based speed.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	subword-based approach faster fewer words characters labeled.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	found speed training test.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	work used delicately.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)	authors appreciate reviewers’ effort good advice improving paper.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	found far existing implementations using character-based IOB tagging.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	give detailed description approach Section 2.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	∗ second author affiliated NTT.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	OOV recognition important word segmentation, higher IV rate also desired.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	work propose confidence measure approach lessen weakness.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	approach change R-oovs R-ivs find optimal tradeoff.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	approach described Section 2.2.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Section 3 presents experimental results.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Section 5 provides concluding remarks.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	word segmentation process illustrated Fig.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	1.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	example exhibiting step’s results also given figure.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Since dictionary-based approach well-known method, skip technical descriptions.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	use advantage confidence measure approach.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	2.1 Subword-based IOB tagging using CRFs.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	several steps train subword-based IOB tag- ger.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	chose single characters top multi- character words lexicon subset IOB tagging.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	subset consists Chinese characters only, character-based IOB tagger.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	regard words subset subwords IOB tagging.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	character-based IOB tagger, one possibility re-segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	However, multiple choices subword-based IOB tagger.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	course, backward maximal match (BMM) approaches also applicable.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	order overcome overfitting, gaussian prior imposed training.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	subscripts position indicators.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	defined cutoff value feature type selected features occurrence counts cutoff.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	forward-backward algorithm used training viterbi algorithm used decoding.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	2.2 Confidence-dependent word segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	However, neither perfect.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	section introduce confidence measure approach combine two results.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	found value 0.7 α, empirically.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Eq. 2 results IOB tagging reevaluated.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	confidence measure threshold, t, defined making decision based value.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	new OOV thus created.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Section 3.2 present experimental segmentation results confidence measure approach.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	detailed info.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	corpora scores, refer (Emerson, 2005).	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	dictionary-based approach, extracted word list training data vocabulary.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Table 1 shows performance dictionary-based segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Since single-character words present test data training data, R-oov rates zero experiment.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	fact, OOV recognition.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Hence, approach produced lower F-scores.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	However, R-ivs high.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	3.1 Effects Character-based the.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	character-based tagging, used Chinese characters.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	67 8 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	70 0 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	78 3 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	71 0 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	upper numbers character- based lower ones, subword-based.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	using FMM, labeled “IOB” tags CRFs.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	F-score changes PKU corpora, recall rates improved.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	However, R-iv rates getting worse return higher R-oov rates.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	tackle problem confidence measure approach.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	3.2 Effect confidence measure.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	slot, numbers top character-based approach numbers bottom subword-based.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	60 7 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	68 2 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	77 5 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	67 4 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	95 2 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	9 4 3 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	96 4 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	95 0 u r 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	95 1 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	9 5 1 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	97 1 0.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	proves proposed word-based IOB tagging effective.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	IOB tagging approach adopted work new idea.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	main contribution extend IOB tagging approach character-based subword-based.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	proved new approach enhanced word segmentation significantly.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	achieved highest F-scores CITYU, PKU MSR corpora.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	think proposed subword- based tagging played important role good results.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	could yield better results shown Table 4 using information.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	approaches produced wrong segmentations labeling inconsistency.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Another advantage word-based IOB tagging character-based speed.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	subword-based approach faster fewer words characters labeled.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	found speed training test.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	work used delicately.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.	authors appreciate reviewers’ effort good advice improving paper.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	found far existing implementations using character-based IOB tagging.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	give detailed description approach Section 2.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	∗ second author affiliated NTT.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	OOV recognition important word segmentation, higher IV rate also desired.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	work propose confidence measure approach lessen weakness.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	approach change R-oovs R-ivs find optimal tradeoff.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	approach described Section 2.2.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Section 3 presents experimental results.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Section 5 provides concluding remarks.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	word segmentation process illustrated Fig.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	1.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	example exhibiting step’s results also given figure.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Since dictionary-based approach well-known method, skip technical descriptions.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	use advantage confidence measure approach.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	2.1 Subword-based IOB tagging using CRFs.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	several steps train subword-based IOB tag- ger.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	chose single characters top multi- character words lexicon subset IOB tagging.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	subset consists Chinese characters only, character-based IOB tagger.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	regard words subset subwords IOB tagging.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	character-based IOB tagger, one possibility re-segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	However, multiple choices subword-based IOB tagger.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	course, backward maximal match (BMM) approaches also applicable.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	order overcome overfitting, gaussian prior imposed training.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	subscripts position indicators.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	defined cutoff value feature type selected features occurrence counts cutoff.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	forward-backward algorithm used training viterbi algorithm used decoding.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	2.2 Confidence-dependent word segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	However, neither perfect.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	section introduce confidence measure approach combine two results.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	found value 0.7 α, empirically.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Eq. 2 results IOB tagging reevaluated.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	confidence measure threshold, t, defined making decision based value.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	new OOV thus created.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Section 3.2 present experimental segmentation results confidence measure approach.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	detailed info.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	corpora scores, refer (Emerson, 2005).	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	dictionary-based approach, extracted word list training data vocabulary.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Table 1 shows performance dictionary-based segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Since single-character words present test data training data, R-oov rates zero experiment.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	fact, OOV recognition.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Hence, approach produced lower F-scores.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	However, R-ivs high.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	3.1 Effects Character-based the.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	character-based tagging, used Chinese characters.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	67 8 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	70 0 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	78 3 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	71 0 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	upper numbers character- based lower ones, subword-based.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	using FMM, labeled “IOB” tags CRFs.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	F-score changes PKU corpora, recall rates improved.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	However, R-iv rates getting worse return higher R-oov rates.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	tackle problem confidence measure approach.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	3.2 Effect confidence measure.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	slot, numbers top character-based approach numbers bottom subword-based.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	60 7 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	68 2 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	77 5 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	67 4 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	95 2 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	9 4 3 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	96 4 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	95 0 u r 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	95 1 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	9 5 1 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	97 1 0.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	proves proposed word-based IOB tagging effective.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	IOB tagging approach adopted work new idea.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	main contribution extend IOB tagging approach character-based subword-based.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	proved new approach enhanced word segmentation significantly.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	achieved highest F-scores CITYU, PKU MSR corpora.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	think proposed subword- based tagging played important role good results.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	could yield better results shown Table 4 using information.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	approaches produced wrong segmentations labeling inconsistency.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Another advantage word-based IOB tagging character-based speed.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	subword-based approach faster fewer words characters labeled.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	found speed training test.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	work used delicately.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Part of the work using this tool was described by (Zhang et al., 2006).  The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.	authors appreciate reviewers’ effort good advice improving paper.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	found far existing implementations using character-based IOB tagging.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	give detailed description approach Section 2.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	∗ second author affiliated NTT.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	OOV recognition important word segmentation, higher IV rate also desired.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	work propose confidence measure approach lessen weakness.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	approach change R-oovs R-ivs find optimal tradeoff.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	approach described Section 2.2.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Section 3 presents experimental results.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Section 5 provides concluding remarks.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	word segmentation process illustrated Fig.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	1.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	example exhibiting step’s results also given figure.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Since dictionary-based approach well-known method, skip technical descriptions.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	use advantage confidence measure approach.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	2.1 Subword-based IOB tagging using CRFs.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	several steps train subword-based IOB tag- ger.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	chose single characters top multi- character words lexicon subset IOB tagging.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	subset consists Chinese characters only, character-based IOB tagger.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	regard words subset subwords IOB tagging.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	character-based IOB tagger, one possibility re-segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	However, multiple choices subword-based IOB tagger.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	course, backward maximal match (BMM) approaches also applicable.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	order overcome overfitting, gaussian prior imposed training.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	subscripts position indicators.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	defined cutoff value feature type selected features occurrence counts cutoff.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	forward-backward algorithm used training viterbi algorithm used decoding.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	2.2 Confidence-dependent word segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	However, neither perfect.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	section introduce confidence measure approach combine two results.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	found value 0.7 α, empirically.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Eq. 2 results IOB tagging reevaluated.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	confidence measure threshold, t, defined making decision based value.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	new OOV thus created.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Section 3.2 present experimental segmentation results confidence measure approach.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	detailed info.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	corpora scores, refer (Emerson, 2005).	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	dictionary-based approach, extracted word list training data vocabulary.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Table 1 shows performance dictionary-based segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Since single-character words present test data training data, R-oov rates zero experiment.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	fact, OOV recognition.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Hence, approach produced lower F-scores.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	However, R-ivs high.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	3.1 Effects Character-based the.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	character-based tagging, used Chinese characters.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	67 8 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	70 0 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	78 3 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	71 0 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	upper numbers character- based lower ones, subword-based.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	using FMM, labeled “IOB” tags CRFs.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	F-score changes PKU corpora, recall rates improved.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	However, R-iv rates getting worse return higher R-oov rates.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	tackle problem confidence measure approach.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	3.2 Effect confidence measure.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	slot, numbers top character-based approach numbers bottom subword-based.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	60 7 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	68 2 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	77 5 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	67 4 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	95 2 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	9 4 3 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	96 4 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	95 0 u r 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	95 1 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	9 5 1 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	97 1 0.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	proves proposed word-based IOB tagging effective.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	IOB tagging approach adopted work new idea.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	main contribution extend IOB tagging approach character-based subword-based.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	proved new approach enhanced word segmentation significantly.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	achieved highest F-scores CITYU, PKU MSR corpora.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	think proposed subword- based tagging played important role good results.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	could yield better results shown Table 4 using information.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	approaches produced wrong segmentations labeling inconsistency.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Another advantage word-based IOB tagging character-based speed.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	subword-based approach faster fewer words characters labeled.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	found speed training test.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	work used delicately.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.	authors appreciate reviewers’ effort good advice improving paper.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	found far existing implementations using character-based IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	1
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	give detailed description approach Section 2.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	∗ second author affiliated NTT.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	OOV recognition important word segmentation, higher IV rate also desired.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	work propose confidence measure approach lessen weakness.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	approach change R-oovs R-ivs find optimal tradeoff.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	approach described Section 2.2.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Section 3 presents experimental results.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Section 5 provides concluding remarks.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	word segmentation process illustrated Fig.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	1.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	example exhibiting step’s results also given figure.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Since dictionary-based approach well-known method, skip technical descriptions.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	use advantage confidence measure approach.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	2.1 Subword-based IOB tagging using CRFs.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	several steps train subword-based IOB tag- ger.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	chose single characters top multi- character words lexicon subset IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	subset consists Chinese characters only, character-based IOB tagger.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	regard words subset subwords IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	character-based IOB tagger, one possibility re-segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	However, multiple choices subword-based IOB tagger.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	course, backward maximal match (BMM) approaches also applicable.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	order overcome overfitting, gaussian prior imposed training.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	subscripts position indicators.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	defined cutoff value feature type selected features occurrence counts cutoff.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	forward-backward algorithm used training viterbi algorithm used decoding.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	2.2 Confidence-dependent word segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	However, neither perfect.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	section introduce confidence measure approach combine two results.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	found value 0.7 α, empirically.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Eq. 2 results IOB tagging reevaluated.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	confidence measure threshold, t, defined making decision based value.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	new OOV thus created.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Section 3.2 present experimental segmentation results confidence measure approach.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	detailed info.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	corpora scores, refer (Emerson, 2005).	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	dictionary-based approach, extracted word list training data vocabulary.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Table 1 shows performance dictionary-based segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Since single-character words present test data training data, R-oov rates zero experiment.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	fact, OOV recognition.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Hence, approach produced lower F-scores.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	However, R-ivs high.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	3.1 Effects Character-based the.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	character-based tagging, used Chinese characters.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	67 8 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	70 0 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	78 3 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	71 0 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	upper numbers character- based lower ones, subword-based.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	using FMM, labeled “IOB” tags CRFs.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	F-score changes PKU corpora, recall rates improved.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	However, R-iv rates getting worse return higher R-oov rates.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	tackle problem confidence measure approach.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	3.2 Effect confidence measure.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	slot, numbers top character-based approach numbers bottom subword-based.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	60 7 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	68 2 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	77 5 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	67 4 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	95 2 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	9 4 3 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	96 4 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	95 0 u r 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	95 1 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	9 5 1 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	97 1 0.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	proves proposed word-based IOB tagging effective.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	IOB tagging approach adopted work new idea.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	main contribution extend IOB tagging approach character-based subword-based.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	proved new approach enhanced word segmentation significantly.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	achieved highest F-scores CITYU, PKU MSR corpora.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	think proposed subword- based tagging played important role good results.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	could yield better results shown Table 4 using information.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	approaches produced wrong segmentations labeling inconsistency.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Another advantage word-based IOB tagging character-based speed.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	subword-based approach faster fewer words characters labeled.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	found speed training test.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	work used delicately.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.	authors appreciate reviewers’ effort good advice improving paper.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	found far existing implementations using character-based IOB tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	give detailed description approach Section 2.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	∗ second author affiliated NTT.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	OOV recognition important word segmentation, higher IV rate also desired.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	work propose confidence measure approach lessen weakness.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	approach change R-oovs R-ivs find optimal tradeoff.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	approach described Section 2.2.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Section 3 presents experimental results.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Section 5 provides concluding remarks.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	word segmentation process illustrated Fig.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	1.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	example exhibiting step’s results also given figure.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Since dictionary-based approach well-known method, skip technical descriptions.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	However, keep mind dictionary-based approach produce higher R-iv rate.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	use advantage confidence measure approach.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	2.1 Subword-based IOB tagging using CRFs.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	several steps train subword-based IOB tag- ger.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	chose single characters top multi- character words lexicon subset IOB tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	subset consists Chinese characters only, character-based IOB tagger.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	regard words subset subwords IOB tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	character-based IOB tagger, one possibility re-segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	However, multiple choices subword-based IOB tagger.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	course, backward maximal match (BMM) approaches also applicable.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	order overcome overfitting, gaussian prior imposed training.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	subscripts position indicators.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	defined cutoff value feature type selected features occurrence counts cutoff.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	forward-backward algorithm used training viterbi algorithm used decoding.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	2.2 Confidence-dependent word segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	However, neither perfect.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	section introduce confidence measure approach combine two results.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	found value 0.7 α, empirically.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Eq. 2 results IOB tagging reevaluated.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	confidence measure threshold, t, defined making decision based value.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	new OOV thus created.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Section 3.2 present experimental segmentation results confidence measure approach.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	detailed info.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	corpora scores, refer (Emerson, 2005).	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	dictionary-based approach, extracted word list training data vocabulary.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Table 1 shows performance dictionary-based segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Since single-character words present test data training data, R-oov rates zero experiment.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	fact, OOV recognition.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Hence, approach produced lower F-scores.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	However, R-ivs high.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	3.1 Effects Character-based the.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	character-based tagging, used Chinese characters.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	67 8 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	70 0 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	78 3 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	71 0 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	upper numbers character- based lower ones, subword-based.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	using FMM, labeled “IOB” tags CRFs.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	F-score changes PKU corpora, recall rates improved.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	However, R-iv rates getting worse return higher R-oov rates.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	tackle problem confidence measure approach.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	3.2 Effect confidence measure.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	slot, numbers top character-based approach numbers bottom subword-based.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	60 7 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	68 2 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	77 5 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	67 4 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	95 2 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	9 4 3 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	96 4 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	95 0 u r 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	95 1 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	9 5 1 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	97 1 0.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	proves proposed word-based IOB tagging effective.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	IOB tagging approach adopted work new idea.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	main contribution extend IOB tagging approach character-based subword-based.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	proved new approach enhanced word segmentation significantly.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	achieved highest F-scores CITYU, PKU MSR corpora.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	think proposed subword- based tagging played important role good results.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	could yield better results shown Table 4 using information.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	example, inconsistent errors foreign names fixed alphabetical characters known.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	approaches produced wrong segmentations labeling inconsistency.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Another advantage word-based IOB tagging character-based speed.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	subword-based approach faster fewer words characters labeled.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	found speed training test.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	work used delicately.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	also successfully employed confidence measure make confidence-dependent word segmentation.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).	authors appreciate reviewers’ effort good advice improving paper.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	found far existing implementations using character-based IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	1
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	give detailed description approach Section 2.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	∗ second author affiliated NTT.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	OOV recognition important word segmentation, higher IV rate also desired.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	work propose confidence measure approach lessen weakness.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	approach change R-oovs R-ivs find optimal tradeoff.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	approach described Section 2.2.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Section 3 presents experimental results.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Section 5 provides concluding remarks.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	word segmentation process illustrated Fig.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	1.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	example exhibiting step’s results also given figure.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Since dictionary-based approach well-known method, skip technical descriptions.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	use advantage confidence measure approach.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	2.1 Subword-based IOB tagging using CRFs.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	several steps train subword-based IOB tag- ger.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	chose single characters top multi- character words lexicon subset IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	subset consists Chinese characters only, character-based IOB tagger.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	regard words subset subwords IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	character-based IOB tagger, one possibility re-segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	However, multiple choices subword-based IOB tagger.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	course, backward maximal match (BMM) approaches also applicable.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	order overcome overfitting, gaussian prior imposed training.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	subscripts position indicators.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	defined cutoff value feature type selected features occurrence counts cutoff.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	forward-backward algorithm used training viterbi algorithm used decoding.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	2.2 Confidence-dependent word segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	However, neither perfect.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	section introduce confidence measure approach combine two results.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	found value 0.7 α, empirically.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Eq. 2 results IOB tagging reevaluated.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	confidence measure threshold, t, defined making decision based value.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	new OOV thus created.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Section 3.2 present experimental segmentation results confidence measure approach.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	detailed info.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	corpora scores, refer (Emerson, 2005).	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	dictionary-based approach, extracted word list training data vocabulary.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Table 1 shows performance dictionary-based segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Since single-character words present test data training data, R-oov rates zero experiment.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	fact, OOV recognition.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Hence, approach produced lower F-scores.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	However, R-ivs high.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	3.1 Effects Character-based the.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	character-based tagging, used Chinese characters.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	67 8 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	70 0 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	78 3 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	71 0 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	upper numbers character- based lower ones, subword-based.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	using FMM, labeled “IOB” tags CRFs.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	F-score changes PKU corpora, recall rates improved.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	However, R-iv rates getting worse return higher R-oov rates.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	tackle problem confidence measure approach.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	3.2 Effect confidence measure.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	slot, numbers top character-based approach numbers bottom subword-based.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	60 7 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	68 2 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	77 5 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	67 4 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	95 2 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	9 4 3 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	96 4 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	95 0 u r 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	95 1 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	9 5 1 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	97 1 0.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	proves proposed word-based IOB tagging effective.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	IOB tagging approach adopted work new idea.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	main contribution extend IOB tagging approach character-based subword-based.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	proved new approach enhanced word segmentation significantly.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	achieved highest F-scores CITYU, PKU MSR corpora.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	think proposed subword- based tagging played important role good results.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	could yield better results shown Table 4 using information.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	approaches produced wrong segmentations labeling inconsistency.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Another advantage word-based IOB tagging character-based speed.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	subword-based approach faster fewer words characters labeled.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	found speed training test.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	work used delicately.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).	authors appreciate reviewers’ effort good advice improving paper.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	found far existing implementations using character-based IOB tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	1
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	give detailed description approach Section 2.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	∗ second author affiliated NTT.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	OOV recognition important word segmentation, higher IV rate also desired.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	work propose confidence measure approach lessen weakness.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	approach change R-oovs R-ivs find optimal tradeoff.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	approach described Section 2.2.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Section 3 presents experimental results.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Section 5 provides concluding remarks.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	word segmentation process illustrated Fig.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	1.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	example exhibiting step’s results also given figure.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Since dictionary-based approach well-known method, skip technical descriptions.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	However, keep mind dictionary-based approach produce higher R-iv rate.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	use advantage confidence measure approach.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	2.1 Subword-based IOB tagging using CRFs.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	several steps train subword-based IOB tag- ger.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	chose single characters top multi- character words lexicon subset IOB tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	subset consists Chinese characters only, character-based IOB tagger.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	regard words subset subwords IOB tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	character-based IOB tagger, one possibility re-segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	However, multiple choices subword-based IOB tagger.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	course, backward maximal match (BMM) approaches also applicable.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	order overcome overfitting, gaussian prior imposed training.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	subscripts position indicators.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	defined cutoff value feature type selected features occurrence counts cutoff.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	forward-backward algorithm used training viterbi algorithm used decoding.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	2.2 Confidence-dependent word segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	However, neither perfect.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	section introduce confidence measure approach combine two results.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	found value 0.7 α, empirically.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Eq. 2 results IOB tagging reevaluated.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	confidence measure threshold, t, defined making decision based value.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	new OOV thus created.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Section 3.2 present experimental segmentation results confidence measure approach.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	detailed info.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	corpora scores, refer (Emerson, 2005).	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	dictionary-based approach, extracted word list training data vocabulary.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Table 1 shows performance dictionary-based segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Since single-character words present test data training data, R-oov rates zero experiment.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	fact, OOV recognition.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Hence, approach produced lower F-scores.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	However, R-ivs high.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	3.1 Effects Character-based the.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	character-based tagging, used Chinese characters.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	67 8 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	70 0 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	78 3 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	71 0 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	upper numbers character- based lower ones, subword-based.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	using FMM, labeled “IOB” tags CRFs.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	F-score changes PKU corpora, recall rates improved.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	However, R-iv rates getting worse return higher R-oov rates.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	tackle problem confidence measure approach.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	3.2 Effect confidence measure.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	slot, numbers top character-based approach numbers bottom subword-based.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	60 7 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	68 2 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	77 5 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	67 4 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	95 2 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	9 4 3 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	96 4 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	95 0 u r 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	95 1 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	9 5 1 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	97 1 0.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	proves proposed word-based IOB tagging effective.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	IOB tagging approach adopted work new idea.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	main contribution extend IOB tagging approach character-based subword-based.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	proved new approach enhanced word segmentation significantly.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	achieved highest F-scores CITYU, PKU MSR corpora.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	think proposed subword- based tagging played important role good results.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	could yield better results shown Table 4 using information.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	example, inconsistent errors foreign names fixed alphabetical characters known.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	approaches produced wrong segmentations labeling inconsistency.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Another advantage word-based IOB tagging character-based speed.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	subword-based approach faster fewer words characters labeled.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	found speed training test.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	work used delicately.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	also successfully employed confidence measure make confidence-dependent word segmentation.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)	authors appreciate reviewers’ effort good advice improving paper.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Subword-based Tagging Conditional Random Fields Chinese Word Segmentation	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	proposed two approaches improve Chinese word segmentation: subword-based tagging confidence measure approach.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	found former achieved better performance existing character-based tagging, latter improved segmentation combining former dictionary-based segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	addition, latter used balance out-of-vocabulary rates in-vocabulary rates.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	techniques achieved higher F-scores CITYU, PKU MSR corpora best results Sighan Bakeoff 2005.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	character-based “IOB” tagging approach widely used Chinese word segmentation recently (Xue Shen, 2003; Peng McCallum, 2004; Tseng et al., 2005).	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	scheme, character word labeled ‘B’ first character multiple-character word, ‘O’ character functions independent word, ‘I’ otherwise.” example, ” (whole) (Beijing city)” labeled ” (whole)/O (north)/B (capital)/I (city)/I”.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	found far existing implementations using character-based IOB tagging.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	work propose subword-based IOB tagging, assigns tags predefined lexicon subset consisting frequent multiple-character words addition single Chinese characters.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Chinese characters used, subword-based IOB tagging downgraded character-based one.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Taking example mentioned above, “ (whole) (Beijing city)” labeled ” (whole)/O (Beijing)/B (city)/I” subword-based tagging, ” (Beijing)/B” labeled one unit.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	give detailed description approach Section 2.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	∗ second author affiliated NTT.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	addition, found clear weakness IOB tagging approach: yields low in-vocabulary (IV) rate (R-iv) return higher out-of-vocabulary (OOV) rate (R-oov).	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	results closed test Bakeoff 2005 (Emerson, 2005), work (Tseng et al., 2005), using conditional random fields (CRF) IOB tagging, yielded high R-oovs four corpora used, R-iv rates lower.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	OOV recognition important word segmentation, higher IV rate also desired.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	work propose confidence measure approach lessen weakness.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	approach change R-oovs R-ivs find optimal tradeoff.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	approach described Section 2.2.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	followings, illustrate word segmentation process Section 2, subword-based tagging implemented CRFs method.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Section 3 presents experimental results.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Section 4 describes current state- of-the-art methods Chinese word segmentation, results compared.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Section 5 provides concluding remarks.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	word segmentation process illustrated Fig.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	1.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	composed three parts: dictionary-based N-gram word segmentation segmenting IV words, subword- based tagging CRF recognizing OOVs, confidence-dependent word segmentation used merging results dictionary-based IOB tagging.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	example exhibiting step’s results also given figure.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Since dictionary-based approach well-known method, skip technical descriptions.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	However, keep mind dictionary-based approach produce higher R-iv rate.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	use advantage confidence measure approach.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	2.1 Subword-based IOB tagging using CRFs.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	several steps train subword-based IOB tag- ger.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	First, extracted word list training data sorted decreasing order counts training 193 Proceedings Human Language Technology Conference North American Chapter ACL, pages 193–196, New York, June 2006.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Qc 2006 Association Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline word segmentation process data.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	chose single characters top multi- character words lexicon subset IOB tagging.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	subset consists Chinese characters only, character-based IOB tagger.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	regard words subset subwords IOB tagging.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Second, re-segmented words training data subwords belonging subset, assigned IOB tags them.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	character-based IOB tagger, one possibility re-segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	However, multiple choices subword-based IOB tagger.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	example, “ (Beijing-city)” segmented “ (Beijing-city)/O,” “ (Beijing)/B (city)/I,” ” (north)/B (capital)/I (city)/I.” work used forward maximal match (FMM) disambiguation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	course, backward maximal match (BMM) approaches also applicable.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	conduct comparative experiments trivial differences approaches may result significant consequences subword-based ap proach.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	third step, used CRFs approach train IOB tagger (Lafferty et al., 2001) training data.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	downloaded used package “CRF++” site “http://www.chasen.org/˜taku/software.” According CRFs, probability IOB tag sequence, = t0 t1 · · · tM , given word sequence, W = w0 w1 · · · wM , defined p(T |W ) = current observation ti simultaneously; gk (ti , W ), unigram feature functions trigger current observation ti . λk µk model parameters corresponding feature functions fk gk respectively.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	model parameters trained maximizing log-likelihood training data using L-BFGS gradient descent optimization method.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	order overcome overfitting, gaussian prior imposed training.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	types unigram features used experiments included following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 w stands word.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	subscripts position indicators.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	0 means current word; −1, −2, first second word left; 1, 2, first second word right.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	bigram features, used previous current observations, t−1 t0 . feature selection, simply used absolute counts feature training data.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	defined cutoff value feature type selected features occurrence counts cutoff.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	forward-backward algorithm used training viterbi algorithm used decoding.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	2.2 Confidence-dependent word segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	moving step Figure 1, produced two segmentation results: one dictionary-based approach one IOB tagging.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	However, neither perfect.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	dictionary-based segmentation produced results higher R-ivs lower R-oovs IOB tagging yielded contrary results.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	section introduce confidence measure approach combine two results.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	define confidence measure, C M(tiob |w), measure confidence results produced IOB tagging using results dictionary-based segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	confidence measure comes two sources: IOB tagging dictionary- based word segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	calculation defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) tiob word w’s IOB tag assigned IOB tagging; tw , prior IOB tag determined results dictionary-based segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	dictionary- based word segmentation, words re-segmented subwords FMM fed IOB tagging.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	subword given prior IOB tag, tw . C Miob (t|w),   confidence probability derived process IOB tag exp )' )' λk fk (ti−1 , ti , W ) + )' µk gk (ti , W ) /Z,  i=1  k k   (1) ging, defined Z = )' =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) =t 0 t1 ··· tM P ( | W ) call fk (ti−1 , ti , W ) bigram feature functions features trigger previous observation ti−1 numerator sum observation sequences word wi labeled t. δ(tw , tiob )ng denotes contribution dictionary- based segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Kronecker delta function defined δ(tw , tiob )ng = { 1 tw = tiob 0 otherwise Eq. 2, α weighting IOB tagging dictionary-based word segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	found value 0.7 α, empirically.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Eq. 2 results IOB tagging reevaluated.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	confidence measure threshold, t, defined making decision based value.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	value lower t, IOB tag rejected dictionary-based segmentation used; otherwise, IOB tagging segmentation used.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	new OOV thus created.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	two extreme cases, = 0 case IOB tagging = 1 dictionary-based approach.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	real application, satisfactory tradeoff R- ivs R-oovs could find tuning confidence threshold.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Section 3.2 present experimental segmentation results confidence measure approach.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	used data provided Sighan Bakeoff 2005 test approaches described previous sections.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	data contain four corpora different sources: Academia Sinica (AS), City University Hong Kong (CITYU), Peking University (PKU) Microsoft Research Beijing (MSR).	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Since work evaluate proposed subword-based IOB tagging, carried closed test only.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Five metrics used evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) IV rate(R-iv).	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	detailed info.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	corpora scores, refer (Emerson, 2005).	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	dictionary-based approach, extracted word list training data vocabulary.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Tri- gram LMs generated using SRI LM toolkit disambiguation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Table 1 shows performance dictionary-based segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Since single-character words present test data training data, R-oov rates zero experiment.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	fact, OOV recognition.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Hence, approach produced lower F-scores.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	However, R-ivs high.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	3.1 Effects Character-based the.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	subword-based tagger main difference character-based word-based contents lexicon subset used re-segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	character-based tagging, used Chinese characters.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	subword-based tagging, added another 2000 frequent multiple- character words lexicons tagging.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	segmentation results dictionary-based re-segmented Table 1: segmentation results dictionary- based approach closed test Bakeoff 2005, low R-oov rates due OOV recognition applied.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	R P FR oo vR iv 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	67 8 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	70 0 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	78 3 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	75 4 0.9 49 0.9 55 R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	71 0 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	71 6 0.9 64 0.9 72 Table 2: Segmentation results pure subword-based IOB tagging.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	upper numbers character- based lower ones, subword-based.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	using FMM, labeled “IOB” tags CRFs.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	segmentation results using CRF tagging shown Table 2, upper numbers slot produced character-based approach lower numbers subword-based.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	found proposed subword-based approaches effective CITYU MSR corpora, raising F-scores 0.941 0.946 CITYU corpus, 0.959 0.964 MSR corpus.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	F-score changes PKU corpora, recall rates improved.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Comparing Table 1 2, found CRF-modeled IOB tagging yielded better segmentation dictionary- based approach.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	However, R-iv rates getting worse return higher R-oov rates.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	tackle problem confidence measure approach.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	3.2 Effect confidence measure.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	section 2.2, proposed confidence measure approach reevaluate results IOB tagging combinations results dictionary-based segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	effect confidence measure shown Table 3, used α = 0.7 confidence threshold = 0.8.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	slot, numbers top character-based approach numbers bottom subword-based.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	found results Table 3 better Table 2 Table 1, prove using confidence measure approach achieved best performance dictionary-based segmentation IOB tagging approach.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	act confidence measure made tradeoff R-ivs R- oovs, yielding higher R-oovs Table 1 higher R R P FR oo vR iv 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	60 7 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	68 2 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	77 5 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	74 8 0.9 52 0.9 59 R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	67 4 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	71 2 0.9 67 0.9 76 Table 3: Effects combination using confidence measure.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	upper numbers lower numbers character-based subword-based, respectively CI U SR P K U Ba ke st 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	95 2 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	9 4 3 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	96 4 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	95 0 u r 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	95 1 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	9 5 1 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	97 1 0.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	95 1 Table 4: Comparison results best ones Sighan Bakeoff 2005 terms F-score ivs Table 2.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Even use confidence measure, word- based IOB tagging still outperformed character-based IOB tagging.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	proves proposed word-based IOB tagging effective.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	IOB tagging approach adopted work new idea.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	first used Chinese word segmentation (Xue Shen, 2003), maximum entropy methods used.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Later, approach implemented CRF-based method (Peng McCallum, 2004), proved achieve better results maximum entropy approach solve label bias problem (Lafferty et al., 2001).	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	main contribution extend IOB tagging approach character-based subword-based.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	proved new approach enhanced word segmentation significantly.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	results listed together best results Bakeoff 2005 Table 4 terms F-scores.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	achieved highest F-scores CITYU, PKU MSR corpora.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	think proposed subword- based tagging played important role good results.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Since closed test, information Arabic Chinese number alphabetical letters cannot used.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	could yield better results shown Table 4 using information.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	example, inconsistent errors foreign names fixed alphabetical characters known.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	corpus, “Adam Smith” two words training become one- word test, “AdamSmith”.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	approaches produced wrong segmentations labeling inconsistency.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Another advantage word-based IOB tagging character-based speed.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	subword-based approach faster fewer words characters labeled.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	found speed training test.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	idea using confidence measure appeared (Peng McCallum, 2004), used recognize OOVs.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	work used delicately.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	way confidence measure combined results dictionary-based IOB-tagging-based result, could achieve optimal performance.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	work, proposed subword-based IOB tagging method Chinese word segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	Using CRFs approaches, prove outperformed character- based method using CRF approaches.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	also successfully employed confidence measure make confidence-dependent word segmentation.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	approach effective performing desired segmentation based users’ requirements R-oov R-iv.	0
Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.  This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.  (2006) were proposed.	authors appreciate reviewers’ effort good advice improving paper.	0
