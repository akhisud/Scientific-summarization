multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	conclude brief discussion section 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	use end-user ter minology source target languages.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	plates represent replicates.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	2.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	3.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	e generated	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	4.1 Variational Approximation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Essentially, Eqs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	First: Laplace smoothing.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Second: interpolation smoothing.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(13) Eqn.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	contains 14,769 alignment-links.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	test used unseen test data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.1 Model Settings.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	$) |K ore an) 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	06 12 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	21 38 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	� )|K ore an) 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	83 79 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	61 16 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Choosing number topics model selection problem.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.2 Variational Inference.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	�).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.4 Evaluating Word.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	45 8 15 .7 0 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	82 2 17 .7 0 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	92 6 18 .2 5 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	see improvement alignment quality.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.6 Evaluating Translations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5 9 19 .1 9 7.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	7 7 21 .9 9 7.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	8 3 23 .1 8 7.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	experimental conditions similar Table.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	4.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	conclude brief discussion section 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	use end-user ter minology source target languages.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	plates represent replicates.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	2.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	3.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	e generated	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	4.1 Variational Approximation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Essentially, Eqs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	First: Laplace smoothing.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Second: interpolation smoothing.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(13) Eqn.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	contains 14,769 alignment-links.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	test used unseen test data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.1 Model Settings.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	$) |K ore an) 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	06 12 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	21 38 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	� )|K ore an) 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	83 79 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	61 16 0.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Choosing number topics model selection problem.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.2 Variational Inference.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	�).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.4 Evaluating Word.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	45 8 15 .7 0 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	82 2 17 .7 0 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	92 6 18 .2 5 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	see improvement alignment quality.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5.6 Evaluating Translations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	5 9 19 .1 9 7.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	7 7 21 .9 9 7.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	8 3 23 .1 8 7.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	1.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	experimental conditions similar Table.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	4.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
multilingual topic models require parallel text, either document (Ni et al., 2009; Mimno et al., 2009) word-level (Kim Khudanpur, 2004; Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	models enable word- alignment process leverage topical contents document-pairs.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Efficient variational approximation algorithms designed inference parameter estimation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	1
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	topical information, translation models expected sharper word-alignment process less ambiguous.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	approaches expensive, emphasize stochastic translation aspects.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	conclude brief discussion section 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	2.1 Baseline: IBM Model-1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation lexicon p(f |e) key component generative process.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	use end-user ter minology source target languages.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	first BiTAM model, assume topics sampled sentence-level.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	document- pair represented random mixture latent topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	node graph represents random variable, hexagon denotes parameter.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Un-shaded nodes hidden variables.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	plates represent replicates.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	generative process, document-pair (Fd, Ed), summarized below: 1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Sample sentence-number N Poisson(γ)..	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	2.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Sample topic-weight vector θd Dirichlet(α)..	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	3.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	sentence-pair (fn , en ) dtth doc-pair ,.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(6) contains one word: “Null”, alignment link longer hidden variable.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	3.2 BiTAM2: Monolingual Admixture.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	general, monolingual model English also rich topic-mixture.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	e generated	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	variational inference used approximate true posteriors hidden variables.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	4.1 Variational Approximation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	topics inferred directly observed bilingual data, result, improve alignment.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	3.3 BiTAM3: Word-level Admixture.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Similar IBM models, “Null” word used source words translation counterparts target language.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Essentially, Eqs.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(810) constitute E-step, posterior estimations latent variables obtained.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	4.2 Data Sparseness Smoothing.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	reduce data sparsity problem, introduce two remedies models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	First: Laplace smoothing.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Second: interpolation smoothing.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(13) Eqn.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	4.3 Retrieving Word Alignments.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(14) i∈[1,Idn ] icon’s strength.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	evaluate BiTAM models word alignment accuracy translation quality.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Table 1: Training Test Data Statistics Tra #D oc.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	contains 14,769 alignment-links.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	evaluate translation quality, TIDES’02 Eval.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	test used development set, TIDES’03 Eval.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	test used unseen test data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.1 Model Settings.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	First, explore effects Null word smoothing strategies.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	pics Le xic ons pic1 pic2 pic3 Co oc.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	$) |K ore an) 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	06 12 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	21 38 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	� )|K ore an) 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	83 79 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	61 16 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	two candidate translations may fade learned translation lexicons.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	33 functional words removed highlight main content topic.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Laplace smoothing chosen emphasize BiTAM’s strength.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Without smoothing, F- measure drops quickly two topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	following experiments, use Null word Laplace smoothing BiTAM models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Choosing number topics model selection problem.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	overall computation complexity BiTAM linear number hidden topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.2 Variational Inference.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Better initialization B help avoid local optimal shown § 5.5.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	learned B α fixed, variational parameters computed Eqn.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTam Null Laplace Smoothing Var.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.3 Topic-Specific Translation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	$); South Korean occurs often economics translated “HanGuo”(li!	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	�).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTAMs discriminate two considering topics context.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	values relatively sharper, clearly favors one candidates.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.4 Evaluating Word.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Alignments evaluate word alignment accuracies various settings.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Additional heuristics applied improve accuracies.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	45 8 15 .7 0 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	82 2 17 .7 0 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	92 6 18 .2 5 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	93 7 6.954 17 .93 18.14 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	90 4 6.976 18 .13 18.05 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	close look three BiTAMs yield significant difference.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.5 Boosting BiTAM Models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	boosted alignments denoted BUDA BBDA Table.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5, corresponding uni-direction bi-direction alignments, respectively.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	see improvement alignment quality.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.6 Evaluating Translations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	trigram language model built using 180 million English words.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Shown Table 4 results small- data track; large-data track results Table 5.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Similar improvements observed large-data track (see Table 5).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5 9 19 .1 9 7.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	7 7 21 .9 9 7.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	8 3 23 .1 8 7.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	experimental conditions similar Table.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	4.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	proposed models significantly improve alignment accuracy lead better translation qualities.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	models enable word- alignment process leverage topical contents document-pairs.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Efficient variational approximation algorithms designed inference parameter estimation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	1
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	topical information, translation models expected sharper word-alignment process less ambiguous.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	approaches expensive, emphasize stochastic translation aspects.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	conclude brief discussion section 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	2.1 Baseline: IBM Model-1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation lexicon p(f |e) key component generative process.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	use end-user ter minology source target languages.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	first BiTAM model, assume topics sampled sentence-level.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	document- pair represented random mixture latent topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	node graph represents random variable, hexagon denotes parameter.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Un-shaded nodes hidden variables.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	plates represent replicates.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	generative process, document-pair (Fd, Ed), summarized below: 1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Sample sentence-number N Poisson(γ)..	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	2.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Sample topic-weight vector θd Dirichlet(α)..	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	3.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	sentence-pair (fn , en ) dtth doc-pair ,.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(6) contains one word: “Null”, alignment link longer hidden variable.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	3.2 BiTAM2: Monolingual Admixture.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	general, monolingual model English also rich topic-mixture.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	e generated	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	variational inference used approximate true posteriors hidden variables.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	4.1 Variational Approximation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	topics inferred directly observed bilingual data, result, improve alignment.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	3.3 BiTAM3: Word-level Admixture.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Similar IBM models, “Null” word used source words translation counterparts target language.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Essentially, Eqs.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(810) constitute E-step, posterior estimations latent variables obtained.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	4.2 Data Sparseness Smoothing.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	reduce data sparsity problem, introduce two remedies models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	First: Laplace smoothing.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Second: interpolation smoothing.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(13) Eqn.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	4.3 Retrieving Word Alignments.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(14) i∈[1,Idn ] icon’s strength.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	evaluate BiTAM models word alignment accuracy translation quality.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Table 1: Training Test Data Statistics Tra #D oc.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	contains 14,769 alignment-links.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	evaluate translation quality, TIDES’02 Eval.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	test used development set, TIDES’03 Eval.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	test used unseen test data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.1 Model Settings.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	First, explore effects Null word smoothing strategies.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	pics Le xic ons pic1 pic2 pic3 Co oc.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	$) |K ore an) 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	06 12 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	21 38 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	� )|K ore an) 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	83 79 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	61 16 0.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	two candidate translations may fade learned translation lexicons.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	33 functional words removed highlight main content topic.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Laplace smoothing chosen emphasize BiTAM’s strength.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Without smoothing, F- measure drops quickly two topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	following experiments, use Null word Laplace smoothing BiTAM models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Choosing number topics model selection problem.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	overall computation complexity BiTAM linear number hidden topics.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.2 Variational Inference.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Better initialization B help avoid local optimal shown § 5.5.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	learned B α fixed, variational parameters computed Eqn.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTam Null Laplace Smoothing Var.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.3 Topic-Specific Translation.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	$); South Korean occurs often economics translated “HanGuo”(li!	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	�).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTAMs discriminate two considering topics context.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	values relatively sharper, clearly favors one candidates.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.4 Evaluating Word.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Alignments evaluate word alignment accuracies various settings.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Additional heuristics applied improve accuracies.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	45 8 15 .7 0 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	82 2 17 .7 0 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	92 6 18 .2 5 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	93 7 6.954 17 .93 18.14 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	90 4 6.976 18 .13 18.05 6.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	close look three BiTAMs yield significant difference.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.5 Boosting BiTAM Models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	boosted alignments denoted BUDA BBDA Table.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5, corresponding uni-direction bi-direction alignments, respectively.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	see improvement alignment quality.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5.6 Evaluating Translations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	trigram language model built using 180 million English words.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Shown Table 4 results small- data track; large-data track results Table 5.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Similar improvements observed large-data track (see Table 5).	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	5 9 19 .1 9 7.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	7 7 21 .9 9 7.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	8 3 23 .1 8 7.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	1.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	experimental conditions similar Table.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	4.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	proposed models significantly improve alignment accuracy lead better translation qualities.	0
"studies document-level SMT.</S><S sid =""49"" ssid = ""2"">Representative work includes Zhao et al.</S><S sid =""50"" ssid = ""3"">(2006), Tam et al.</S><S sid =""51"" ssid = ""4"">(2007), Carpuat (2009)."	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	models enable word- alignment process leverage topical contents document-pairs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Efficient variational approximation algorithms designed inference parameter estimation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	topical information, translation models expected sharper word-alignment process less ambiguous.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	approaches expensive, emphasize stochastic translation aspects.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	conclude brief discussion section 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	2.1 Baseline: IBM Model-1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation lexicon p(f |e) key component generative process.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	use end-user ter minology source target languages.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	first BiTAM model, assume topics sampled sentence-level.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	document- pair represented random mixture latent topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	node graph represents random variable, hexagon denotes parameter.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Un-shaded nodes hidden variables.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	plates represent replicates.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	generative process, document-pair (Fd, Ed), summarized below: 1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Sample sentence-number N Poisson(γ)..	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	2.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Sample topic-weight vector θd Dirichlet(α)..	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	3.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	sentence-pair (fn , en ) dtth doc-pair ,.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(6) contains one word: “Null”, alignment link longer hidden variable.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	3.2 BiTAM2: Monolingual Admixture.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	general, monolingual model English also rich topic-mixture.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	e generated	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	variational inference used approximate true posteriors hidden variables.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	4.1 Variational Approximation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	topics inferred directly observed bilingual data, result, improve alignment.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	3.3 BiTAM3: Word-level Admixture.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Similar IBM models, “Null” word used source words translation counterparts target language.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Essentially, Eqs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(810) constitute E-step, posterior estimations latent variables obtained.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	4.2 Data Sparseness Smoothing.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	reduce data sparsity problem, introduce two remedies models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	First: Laplace smoothing.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Second: interpolation smoothing.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(13) Eqn.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	4.3 Retrieving Word Alignments.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(14) i∈[1,Idn ] icon’s strength.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	evaluate BiTAM models word alignment accuracy translation quality.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Table 1: Training Test Data Statistics Tra #D oc.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	contains 14,769 alignment-links.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	evaluate translation quality, TIDES’02 Eval.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	test used development set, TIDES’03 Eval.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	test used unseen test data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.1 Model Settings.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	First, explore effects Null word smoothing strategies.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	pics Le xic ons pic1 pic2 pic3 Co oc.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	$) |K ore an) 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	06 12 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	21 38 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	� )|K ore an) 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	83 79 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	61 16 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	two candidate translations may fade learned translation lexicons.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	33 functional words removed highlight main content topic.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Laplace smoothing chosen emphasize BiTAM’s strength.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Without smoothing, F- measure drops quickly two topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	following experiments, use Null word Laplace smoothing BiTAM models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Choosing number topics model selection problem.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	overall computation complexity BiTAM linear number hidden topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.2 Variational Inference.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Better initialization B help avoid local optimal shown § 5.5.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	learned B α fixed, variational parameters computed Eqn.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTam Null Laplace Smoothing Var.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.3 Topic-Specific Translation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	$); South Korean occurs often economics translated “HanGuo”(li!	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	�).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTAMs discriminate two considering topics context.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	values relatively sharper, clearly favors one candidates.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.4 Evaluating Word.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Alignments evaluate word alignment accuracies various settings.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Additional heuristics applied improve accuracies.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	45 8 15 .7 0 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	82 2 17 .7 0 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	92 6 18 .2 5 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	93 7 6.954 17 .93 18.14 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	90 4 6.976 18 .13 18.05 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	close look three BiTAMs yield significant difference.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.5 Boosting BiTAM Models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	boosted alignments denoted BUDA BBDA Table.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5, corresponding uni-direction bi-direction alignments, respectively.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	see improvement alignment quality.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.6 Evaluating Translations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	trigram language model built using 180 million English words.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Shown Table 4 results small- data track; large-data track results Table 5.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Similar improvements observed large-data track (see Table 5).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5 9 19 .1 9 7.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	7 7 21 .9 9 7.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	8 3 23 .1 8 7.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	experimental conditions similar Table.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	4.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	proposed models significantly improve alignment accuracy lead better translation qualities.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	models enable word- alignment process leverage topical contents document-pairs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Efficient variational approximation algorithms designed inference parameter estimation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	topical information, translation models expected sharper word-alignment process less ambiguous.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	approaches expensive, emphasize stochastic translation aspects.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	conclude brief discussion section 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	2.1 Baseline: IBM Model-1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation lexicon p(f |e) key component generative process.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	use end-user ter minology source target languages.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	first BiTAM model, assume topics sampled sentence-level.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	document- pair represented random mixture latent topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	node graph represents random variable, hexagon denotes parameter.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Un-shaded nodes hidden variables.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	plates represent replicates.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	generative process, document-pair (Fd, Ed), summarized below: 1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Sample sentence-number N Poisson(γ)..	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	2.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Sample topic-weight vector θd Dirichlet(α)..	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	3.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	sentence-pair (fn , en ) dtth doc-pair ,.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(6) contains one word: “Null”, alignment link longer hidden variable.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	3.2 BiTAM2: Monolingual Admixture.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	general, monolingual model English also rich topic-mixture.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	e generated	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	variational inference used approximate true posteriors hidden variables.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	4.1 Variational Approximation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	topics inferred directly observed bilingual data, result, improve alignment.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	3.3 BiTAM3: Word-level Admixture.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Similar IBM models, “Null” word used source words translation counterparts target language.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Essentially, Eqs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(810) constitute E-step, posterior estimations latent variables obtained.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	4.2 Data Sparseness Smoothing.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	reduce data sparsity problem, introduce two remedies models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	First: Laplace smoothing.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Second: interpolation smoothing.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(13) Eqn.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	4.3 Retrieving Word Alignments.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(14) i∈[1,Idn ] icon’s strength.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	evaluate BiTAM models word alignment accuracy translation quality.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Table 1: Training Test Data Statistics Tra #D oc.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	contains 14,769 alignment-links.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	evaluate translation quality, TIDES’02 Eval.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	test used development set, TIDES’03 Eval.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	test used unseen test data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.1 Model Settings.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	First, explore effects Null word smoothing strategies.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	pics Le xic ons pic1 pic2 pic3 Co oc.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	$) |K ore an) 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	06 12 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	21 38 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	� )|K ore an) 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	83 79 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	61 16 0.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	two candidate translations may fade learned translation lexicons.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	33 functional words removed highlight main content topic.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Laplace smoothing chosen emphasize BiTAM’s strength.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Without smoothing, F- measure drops quickly two topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	following experiments, use Null word Laplace smoothing BiTAM models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Choosing number topics model selection problem.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	overall computation complexity BiTAM linear number hidden topics.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.2 Variational Inference.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Better initialization B help avoid local optimal shown § 5.5.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	learned B α fixed, variational parameters computed Eqn.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTam Null Laplace Smoothing Var.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.3 Topic-Specific Translation.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	$); South Korean occurs often economics translated “HanGuo”(li!	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	�).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTAMs discriminate two considering topics context.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	values relatively sharper, clearly favors one candidates.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.4 Evaluating Word.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Alignments evaluate word alignment accuracies various settings.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Additional heuristics applied improve accuracies.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	45 8 15 .7 0 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	82 2 17 .7 0 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	92 6 18 .2 5 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	93 7 6.954 17 .93 18.14 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	90 4 6.976 18 .13 18.05 6.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	close look three BiTAMs yield significant difference.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.5 Boosting BiTAM Models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	boosted alignments denoted BUDA BBDA Table.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5, corresponding uni-direction bi-direction alignments, respectively.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	see improvement alignment quality.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5.6 Evaluating Translations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	trigram language model built using 180 million English words.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Shown Table 4 results small- data track; large-data track results Table 5.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Similar improvements observed large-data track (see Table 5).	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	5 9 19 .1 9 7.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	7 7 21 .9 9 7.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	8 3 23 .1 8 7.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	1.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	experimental conditions similar Table.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	4.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	proposed models significantly improve alignment accuracy lead better translation qualities.	0
"Zhao et al.</S><S sid =""53"" ssid = ""6"">(2006) assumed parallel sentence pairs within document pair constitute mixture hidden topics word pair follows topic-specific bilingual translation model.</S><S sid =""54"" ssid = ""7"">It shows performance word alignment improved help document-level information, indirectly improves quality SMT."	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	conclude brief discussion section 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	use end-user ter minology source target languages.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	plates represent replicates.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	2.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	3.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	e generated	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	4.1 Variational Approximation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Essentially, Eqs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	First: Laplace smoothing.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Second: interpolation smoothing.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(13) Eqn.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	contains 14,769 alignment-links.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	test used unseen test data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.1 Model Settings.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	$) |K ore an) 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	06 12 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	21 38 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	� )|K ore an) 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	83 79 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	61 16 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Choosing number topics model selection problem.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.2 Variational Inference.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	�).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.4 Evaluating Word.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	45 8 15 .7 0 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	82 2 17 .7 0 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	92 6 18 .2 5 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	see improvement alignment quality.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.6 Evaluating Translations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5 9 19 .1 9 7.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	7 7 21 .9 9 7.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	8 3 23 .1 8 7.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	experimental conditions similar Table.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	4.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	conclude brief discussion section 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	use end-user ter minology source target languages.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	plates represent replicates.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	2.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	3.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	e generated	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	4.1 Variational Approximation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Essentially, Eqs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	First: Laplace smoothing.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Second: interpolation smoothing.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(13) Eqn.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	contains 14,769 alignment-links.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	test used unseen test data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.1 Model Settings.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	$) |K ore an) 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	06 12 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	21 38 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	� )|K ore an) 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	83 79 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	61 16 0.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Choosing number topics model selection problem.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.2 Variational Inference.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	�).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.4 Evaluating Word.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	45 8 15 .7 0 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	82 2 17 .7 0 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	92 6 18 .2 5 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	see improvement alignment quality.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5.6 Evaluating Translations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	5 9 19 .1 9 7.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	7 7 21 .9 9 7.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	8 3 23 .1 8 7.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	1.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	experimental conditions similar Table.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	4.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
methods ensure bilingual embeddings retain els (Peirsman PadoÂ´ , 2010; Sumita, 2000), translational equivalence distribu unsupervised algorithms LDA LSA (BoydGraber Resnik, 2010; Tam et al., 2007; Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	models enable word- alignment process leverage topical contents document-pairs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Efficient variational approximation algorithms designed inference parameter estimation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	approaches expensive, emphasize stochastic translation aspects.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	conclude brief discussion section 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	2.1 Baseline: IBM Model-1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation lexicon p(f |e) key component generative process.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	use end-user ter minology source target languages.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	first BiTAM model, assume topics sampled sentence-level.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	document- pair represented random mixture latent topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	node graph represents random variable, hexagon denotes parameter.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Un-shaded nodes hidden variables.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	plates represent replicates.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Sample sentence-number N Poisson(γ)..	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	2.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Sample topic-weight vector θd Dirichlet(α)..	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	3.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	sentence-pair (fn , en ) dtth doc-pair ,.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	3.2 BiTAM2: Monolingual Admixture.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	general, monolingual model English also rich topic-mixture.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	e generated	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	variational inference used approximate true posteriors hidden variables.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	4.1 Variational Approximation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	topics inferred directly observed bilingual data, result, improve alignment.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	3.3 BiTAM3: Word-level Admixture.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Essentially, Eqs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(810) constitute E-step, posterior estimations latent variables obtained.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	4.2 Data Sparseness Smoothing.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	reduce data sparsity problem, introduce two remedies models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	First: Laplace smoothing.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Second: interpolation smoothing.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(13) Eqn.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	4.3 Retrieving Word Alignments.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(14) i∈[1,Idn ] icon’s strength.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	evaluate BiTAM models word alignment accuracy translation quality.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Table 1: Training Test Data Statistics Tra #D oc.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	contains 14,769 alignment-links.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	evaluate translation quality, TIDES’02 Eval.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	test used development set, TIDES’03 Eval.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	test used unseen test data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.1 Model Settings.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	First, explore effects Null word smoothing strategies.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	$) |K ore an) 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	06 12 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	21 38 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	� )|K ore an) 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	83 79 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	61 16 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	two candidate translations may fade learned translation lexicons.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	33 functional words removed highlight main content topic.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Without smoothing, F- measure drops quickly two topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	following experiments, use Null word Laplace smoothing BiTAM models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Choosing number topics model selection problem.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	overall computation complexity BiTAM linear number hidden topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.2 Variational Inference.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Better initialization B help avoid local optimal shown § 5.5.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	learned B α fixed, variational parameters computed Eqn.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTam Null Laplace Smoothing Var.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.3 Topic-Specific Translation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	$); South Korean occurs often economics translated “HanGuo”(li!	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	�).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTAMs discriminate two considering topics context.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	values relatively sharper, clearly favors one candidates.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.4 Evaluating Word.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Alignments evaluate word alignment accuracies various settings.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Additional heuristics applied improve accuracies.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	45 8 15 .7 0 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	82 2 17 .7 0 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	92 6 18 .2 5 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	93 7 6.954 17 .93 18.14 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	90 4 6.976 18 .13 18.05 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	close look three BiTAMs yield significant difference.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.5 Boosting BiTAM Models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	boosted alignments denoted BUDA BBDA Table.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5, corresponding uni-direction bi-direction alignments, respectively.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	see improvement alignment quality.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.6 Evaluating Translations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	trigram language model built using 180 million English words.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Shown Table 4 results small- data track; large-data track results Table 5.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Similar improvements observed large-data track (see Table 5).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5 9 19 .1 9 7.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	7 7 21 .9 9 7.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	8 3 23 .1 8 7.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	experimental conditions similar Table.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	4.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	models enable word- alignment process leverage topical contents document-pairs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Efficient variational approximation algorithms designed inference parameter estimation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	approaches expensive, emphasize stochastic translation aspects.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	conclude brief discussion section 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	2.1 Baseline: IBM Model-1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation lexicon p(f |e) key component generative process.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	use end-user ter minology source target languages.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	first BiTAM model, assume topics sampled sentence-level.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	document- pair represented random mixture latent topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	node graph represents random variable, hexagon denotes parameter.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Un-shaded nodes hidden variables.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	plates represent replicates.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Sample sentence-number N Poisson(γ)..	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	2.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Sample topic-weight vector θd Dirichlet(α)..	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	3.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	sentence-pair (fn , en ) dtth doc-pair ,.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	3.2 BiTAM2: Monolingual Admixture.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	general, monolingual model English also rich topic-mixture.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	e generated	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	variational inference used approximate true posteriors hidden variables.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	4.1 Variational Approximation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	topics inferred directly observed bilingual data, result, improve alignment.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	3.3 BiTAM3: Word-level Admixture.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Essentially, Eqs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(810) constitute E-step, posterior estimations latent variables obtained.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	4.2 Data Sparseness Smoothing.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	reduce data sparsity problem, introduce two remedies models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	First: Laplace smoothing.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Second: interpolation smoothing.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(13) Eqn.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	4.3 Retrieving Word Alignments.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(14) i∈[1,Idn ] icon’s strength.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	evaluate BiTAM models word alignment accuracy translation quality.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Table 1: Training Test Data Statistics Tra #D oc.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	contains 14,769 alignment-links.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	evaluate translation quality, TIDES’02 Eval.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	test used development set, TIDES’03 Eval.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	test used unseen test data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.1 Model Settings.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	First, explore effects Null word smoothing strategies.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	$) |K ore an) 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	06 12 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	21 38 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	� )|K ore an) 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	83 79 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	61 16 0.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	two candidate translations may fade learned translation lexicons.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	33 functional words removed highlight main content topic.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Without smoothing, F- measure drops quickly two topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	following experiments, use Null word Laplace smoothing BiTAM models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Choosing number topics model selection problem.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	overall computation complexity BiTAM linear number hidden topics.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.2 Variational Inference.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Better initialization B help avoid local optimal shown § 5.5.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	learned B α fixed, variational parameters computed Eqn.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTam Null Laplace Smoothing Var.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.3 Topic-Specific Translation.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	$); South Korean occurs often economics translated “HanGuo”(li!	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	�).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTAMs discriminate two considering topics context.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	values relatively sharper, clearly favors one candidates.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.4 Evaluating Word.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Alignments evaluate word alignment accuracies various settings.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Additional heuristics applied improve accuracies.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	45 8 15 .7 0 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	82 2 17 .7 0 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	92 6 18 .2 5 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	93 7 6.954 17 .93 18.14 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	90 4 6.976 18 .13 18.05 6.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	close look three BiTAMs yield significant difference.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.5 Boosting BiTAM Models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	boosted alignments denoted BUDA BBDA Table.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5, corresponding uni-direction bi-direction alignments, respectively.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	see improvement alignment quality.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5.6 Evaluating Translations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	trigram language model built using 180 million English words.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Shown Table 4 results small- data track; large-data track results Table 5.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Similar improvements observed large-data track (see Table 5).	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	5 9 19 .1 9 7.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	7 7 21 .9 9 7.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	8 3 23 .1 8 7.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	1.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	experimental conditions similar Table.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	4.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Related work includes Bilingual Topic Admixture Model (BiTAM) word alignment proposed (Zhao Xing, 2006).</S>	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	models enable word- alignment process leverage topical contents document-pairs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Efficient variational approximation algorithms designed inference parameter estimation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	approaches expensive, emphasize stochastic translation aspects.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	conclude brief discussion section 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	2.1 Baseline: IBM Model-1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation lexicon p(f |e) key component generative process.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	use end-user ter minology source target languages.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	first BiTAM model, assume topics sampled sentence-level.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	document- pair represented random mixture latent topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	node graph represents random variable, hexagon denotes parameter.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Un-shaded nodes hidden variables.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	plates represent replicates.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Sample sentence-number N Poisson(γ)..	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	2.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Sample topic-weight vector θd Dirichlet(α)..	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	3.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	sentence-pair (fn , en ) dtth doc-pair ,.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	3.2 BiTAM2: Monolingual Admixture.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	general, monolingual model English also rich topic-mixture.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	e generated	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	variational inference used approximate true posteriors hidden variables.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	4.1 Variational Approximation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	topics inferred directly observed bilingual data, result, improve alignment.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	3.3 BiTAM3: Word-level Admixture.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Essentially, Eqs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(810) constitute E-step, posterior estimations latent variables obtained.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	4.2 Data Sparseness Smoothing.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	reduce data sparsity problem, introduce two remedies models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	First: Laplace smoothing.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Second: interpolation smoothing.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(13) Eqn.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	4.3 Retrieving Word Alignments.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(14) i∈[1,Idn ] icon’s strength.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	evaluate BiTAM models word alignment accuracy translation quality.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Table 1: Training Test Data Statistics Tra #D oc.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	contains 14,769 alignment-links.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	evaluate translation quality, TIDES’02 Eval.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	test used development set, TIDES’03 Eval.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	test used unseen test data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.1 Model Settings.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	First, explore effects Null word smoothing strategies.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	$) |K ore an) 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	06 12 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	21 38 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	� )|K ore an) 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	83 79 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	61 16 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	two candidate translations may fade learned translation lexicons.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	33 functional words removed highlight main content topic.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Without smoothing, F- measure drops quickly two topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	following experiments, use Null word Laplace smoothing BiTAM models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Choosing number topics model selection problem.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	overall computation complexity BiTAM linear number hidden topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.2 Variational Inference.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Better initialization B help avoid local optimal shown § 5.5.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	learned B α fixed, variational parameters computed Eqn.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTam Null Laplace Smoothing Var.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.3 Topic-Specific Translation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	$); South Korean occurs often economics translated “HanGuo”(li!	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	�).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTAMs discriminate two considering topics context.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	values relatively sharper, clearly favors one candidates.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.4 Evaluating Word.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Alignments evaluate word alignment accuracies various settings.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Additional heuristics applied improve accuracies.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	45 8 15 .7 0 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	82 2 17 .7 0 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	92 6 18 .2 5 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	93 7 6.954 17 .93 18.14 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	90 4 6.976 18 .13 18.05 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	close look three BiTAMs yield significant difference.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.5 Boosting BiTAM Models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	boosted alignments denoted BUDA BBDA Table.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5, corresponding uni-direction bi-direction alignments, respectively.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	see improvement alignment quality.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.6 Evaluating Translations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	trigram language model built using 180 million English words.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Shown Table 4 results small- data track; large-data track results Table 5.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Similar improvements observed large-data track (see Table 5).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5 9 19 .1 9 7.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	7 7 21 .9 9 7.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	8 3 23 .1 8 7.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	experimental conditions similar Table.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	4.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	models enable word- alignment process leverage topical contents document-pairs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Efficient variational approximation algorithms designed inference parameter estimation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	approaches expensive, emphasize stochastic translation aspects.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	conclude brief discussion section 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	2.1 Baseline: IBM Model-1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation lexicon p(f |e) key component generative process.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	use end-user ter minology source target languages.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	first BiTAM model, assume topics sampled sentence-level.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	document- pair represented random mixture latent topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	node graph represents random variable, hexagon denotes parameter.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Un-shaded nodes hidden variables.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	plates represent replicates.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Sample sentence-number N Poisson(γ)..	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	2.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Sample topic-weight vector θd Dirichlet(α)..	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	3.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	sentence-pair (fn , en ) dtth doc-pair ,.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	3.2 BiTAM2: Monolingual Admixture.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	general, monolingual model English also rich topic-mixture.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	e generated	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	variational inference used approximate true posteriors hidden variables.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	4.1 Variational Approximation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	topics inferred directly observed bilingual data, result, improve alignment.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	3.3 BiTAM3: Word-level Admixture.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Essentially, Eqs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(810) constitute E-step, posterior estimations latent variables obtained.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	4.2 Data Sparseness Smoothing.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	reduce data sparsity problem, introduce two remedies models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	First: Laplace smoothing.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Second: interpolation smoothing.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(13) Eqn.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	4.3 Retrieving Word Alignments.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(14) i∈[1,Idn ] icon’s strength.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	evaluate BiTAM models word alignment accuracy translation quality.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Table 1: Training Test Data Statistics Tra #D oc.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	contains 14,769 alignment-links.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	evaluate translation quality, TIDES’02 Eval.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	test used development set, TIDES’03 Eval.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	test used unseen test data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.1 Model Settings.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	First, explore effects Null word smoothing strategies.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	$) |K ore an) 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	06 12 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	21 38 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	� )|K ore an) 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	83 79 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	61 16 0.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	two candidate translations may fade learned translation lexicons.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	33 functional words removed highlight main content topic.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Without smoothing, F- measure drops quickly two topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	following experiments, use Null word Laplace smoothing BiTAM models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Choosing number topics model selection problem.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	overall computation complexity BiTAM linear number hidden topics.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.2 Variational Inference.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Better initialization B help avoid local optimal shown § 5.5.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	learned B α fixed, variational parameters computed Eqn.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTam Null Laplace Smoothing Var.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.3 Topic-Specific Translation.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	$); South Korean occurs often economics translated “HanGuo”(li!	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	�).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTAMs discriminate two considering topics context.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	values relatively sharper, clearly favors one candidates.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.4 Evaluating Word.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Alignments evaluate word alignment accuracies various settings.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Additional heuristics applied improve accuracies.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	45 8 15 .7 0 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	82 2 17 .7 0 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	92 6 18 .2 5 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	93 7 6.954 17 .93 18.14 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	90 4 6.976 18 .13 18.05 6.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	close look three BiTAMs yield significant difference.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.5 Boosting BiTAM Models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	boosted alignments denoted BUDA BBDA Table.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5, corresponding uni-direction bi-direction alignments, respectively.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	see improvement alignment quality.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5.6 Evaluating Translations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	trigram language model built using 180 million English words.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Shown Table 4 results small- data track; large-data track results Table 5.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Similar improvements observed large-data track (see Table 5).	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	5 9 19 .1 9 7.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	7 7 21 .9 9 7.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	8 3 23 .1 8 7.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	1.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	experimental conditions similar Table.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	4.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
previous work multilingual topic models assume documents multiple languages aligned either document level, sentence level time stamps (Mimno et al., 2009; Zhao Xing, 2006; Kim Khudanpur, 2004; Ni et al., 2009; Wang et al., 2007).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	conclude brief discussion section 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	use end-user ter minology source target languages.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	plates represent replicates.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	2.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	3.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	e generated	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	4.1 Variational Approximation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Essentially, Eqs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	First: Laplace smoothing.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Second: interpolation smoothing.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(13) Eqn.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	contains 14,769 alignment-links.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	test used unseen test data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.1 Model Settings.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	$) |K ore an) 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	06 12 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	21 38 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	� )|K ore an) 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	83 79 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	61 16 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Choosing number topics model selection problem.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.2 Variational Inference.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	�).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.4 Evaluating Word.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	45 8 15 .7 0 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	82 2 17 .7 0 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	92 6 18 .2 5 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	see improvement alignment quality.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.6 Evaluating Translations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5 9 19 .1 9 7.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	7 7 21 .9 9 7.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	8 3 23 .1 8 7.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	experimental conditions similar Table.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	4.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	conclude brief discussion section 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	use end-user ter minology source target languages.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	plates represent replicates.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	2.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	3.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	e generated	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	4.1 Variational Approximation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Essentially, Eqs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	First: Laplace smoothing.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Second: interpolation smoothing.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(13) Eqn.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	contains 14,769 alignment-links.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	test used unseen test data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.1 Model Settings.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	$) |K ore an) 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	06 12 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	21 38 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	� )|K ore an) 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	83 79 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	61 16 0.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Choosing number topics model selection problem.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.2 Variational Inference.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	�).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.4 Evaluating Word.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	45 8 15 .7 0 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	82 2 17 .7 0 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	92 6 18 .2 5 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	see improvement alignment quality.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5.6 Evaluating Translations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	5 9 19 .1 9 7.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	7 7 21 .9 9 7.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	8 3 23 .1 8 7.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	1.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	experimental conditions similar Table.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	4.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
adopted K = 3 topics, following setting (Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	conclude brief discussion section 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	use end-user ter minology source target languages.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	plates represent replicates.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	2.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	3.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	e generated	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	4.1 Variational Approximation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Essentially, Eqs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	First: Laplace smoothing.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Second: interpolation smoothing.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(13) Eqn.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	contains 14,769 alignment-links.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	test used unseen test data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.1 Model Settings.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	$) |K ore an) 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	06 12 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	21 38 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	� )|K ore an) 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	83 79 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	61 16 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Choosing number topics model selection problem.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.2 Variational Inference.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	�).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.4 Evaluating Word.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	45 8 15 .7 0 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	82 2 17 .7 0 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	92 6 18 .2 5 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	see improvement alignment quality.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.6 Evaluating Translations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5 9 19 .1 9 7.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	7 7 21 .9 9 7.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	8 3 23 .1 8 7.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	experimental conditions similar Table.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	4.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	conclude brief discussion section 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	use end-user ter minology source target languages.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	plates represent replicates.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	2.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	3.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	e generated	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	4.1 Variational Approximation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Essentially, Eqs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	First: Laplace smoothing.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Second: interpolation smoothing.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(13) Eqn.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	contains 14,769 alignment-links.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	test used unseen test data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.1 Model Settings.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	$) |K ore an) 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	06 12 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	21 38 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	� )|K ore an) 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	83 79 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	61 16 0.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Choosing number topics model selection problem.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.2 Variational Inference.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	�).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.4 Evaluating Word.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	45 8 15 .7 0 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	82 2 17 .7 0 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	92 6 18 .2 5 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	see improvement alignment quality.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5.6 Evaluating Translations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	5 9 19 .1 9 7.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	7 7 21 .9 9 7.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	8 3 23 .1 8 7.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	1.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	experimental conditions similar Table.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	4.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
alignment results directions refined ‘GROW’ heuristics yield high precision high recall accordance previous work (Och Ney, 2003; Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	models enable word- alignment process leverage topical contents document-pairs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Efficient variational approximation algorithms designed inference parameter estimation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	approaches expensive, emphasize stochastic translation aspects.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	conclude brief discussion section 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	2.1 Baseline: IBM Model-1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation lexicon p(f |e) key component generative process.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	use end-user ter minology source target languages.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	first BiTAM model, assume topics sampled sentence-level.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	document- pair represented random mixture latent topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	node graph represents random variable, hexagon denotes parameter.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Un-shaded nodes hidden variables.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	plates represent replicates.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Sample sentence-number N Poisson(γ)..	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	2.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Sample topic-weight vector θd Dirichlet(α)..	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	3.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	sentence-pair (fn , en ) dtth doc-pair ,.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	3.2 BiTAM2: Monolingual Admixture.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	general, monolingual model English also rich topic-mixture.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	e generated	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	variational inference used approximate true posteriors hidden variables.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	4.1 Variational Approximation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	topics inferred directly observed bilingual data, result, improve alignment.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	3.3 BiTAM3: Word-level Admixture.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Essentially, Eqs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(810) constitute E-step, posterior estimations latent variables obtained.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	4.2 Data Sparseness Smoothing.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	reduce data sparsity problem, introduce two remedies models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	First: Laplace smoothing.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Second: interpolation smoothing.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(13) Eqn.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	4.3 Retrieving Word Alignments.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(14) i∈[1,Idn ] icon’s strength.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	evaluate BiTAM models word alignment accuracy translation quality.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Table 1: Training Test Data Statistics Tra #D oc.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	contains 14,769 alignment-links.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	evaluate translation quality, TIDES’02 Eval.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	test used development set, TIDES’03 Eval.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	test used unseen test data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.1 Model Settings.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	First, explore effects Null word smoothing strategies.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	$) |K ore an) 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	06 12 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	21 38 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	� )|K ore an) 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	83 79 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	61 16 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	two candidate translations may fade learned translation lexicons.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	33 functional words removed highlight main content topic.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Without smoothing, F- measure drops quickly two topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	following experiments, use Null word Laplace smoothing BiTAM models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Choosing number topics model selection problem.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	overall computation complexity BiTAM linear number hidden topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.2 Variational Inference.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Better initialization B help avoid local optimal shown § 5.5.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	learned B α fixed, variational parameters computed Eqn.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTam Null Laplace Smoothing Var.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.3 Topic-Specific Translation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	$); South Korean occurs often economics translated “HanGuo”(li!	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	�).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTAMs discriminate two considering topics context.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	values relatively sharper, clearly favors one candidates.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.4 Evaluating Word.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Alignments evaluate word alignment accuracies various settings.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Additional heuristics applied improve accuracies.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	45 8 15 .7 0 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	82 2 17 .7 0 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	92 6 18 .2 5 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	93 7 6.954 17 .93 18.14 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	90 4 6.976 18 .13 18.05 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	close look three BiTAMs yield significant difference.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.5 Boosting BiTAM Models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	boosted alignments denoted BUDA BBDA Table.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5, corresponding uni-direction bi-direction alignments, respectively.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	see improvement alignment quality.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.6 Evaluating Translations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	trigram language model built using 180 million English words.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Shown Table 4 results small- data track; large-data track results Table 5.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Similar improvements observed large-data track (see Table 5).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5 9 19 .1 9 7.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	7 7 21 .9 9 7.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	8 3 23 .1 8 7.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	experimental conditions similar Table.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	4.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	models enable word- alignment process leverage topical contents document-pairs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Efficient variational approximation algorithms designed inference parameter estimation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	approaches expensive, emphasize stochastic translation aspects.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	conclude brief discussion section 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	2.1 Baseline: IBM Model-1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation lexicon p(f |e) key component generative process.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	use end-user ter minology source target languages.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	first BiTAM model, assume topics sampled sentence-level.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	document- pair represented random mixture latent topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	node graph represents random variable, hexagon denotes parameter.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Un-shaded nodes hidden variables.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	plates represent replicates.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Sample sentence-number N Poisson(γ)..	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	2.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Sample topic-weight vector θd Dirichlet(α)..	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	3.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	sentence-pair (fn , en ) dtth doc-pair ,.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	3.2 BiTAM2: Monolingual Admixture.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	general, monolingual model English also rich topic-mixture.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	e generated	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	variational inference used approximate true posteriors hidden variables.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	4.1 Variational Approximation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	topics inferred directly observed bilingual data, result, improve alignment.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	3.3 BiTAM3: Word-level Admixture.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Essentially, Eqs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(810) constitute E-step, posterior estimations latent variables obtained.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	4.2 Data Sparseness Smoothing.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	reduce data sparsity problem, introduce two remedies models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	First: Laplace smoothing.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Second: interpolation smoothing.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(13) Eqn.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	4.3 Retrieving Word Alignments.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(14) i∈[1,Idn ] icon’s strength.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	evaluate BiTAM models word alignment accuracy translation quality.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Table 1: Training Test Data Statistics Tra #D oc.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	contains 14,769 alignment-links.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	evaluate translation quality, TIDES’02 Eval.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	test used development set, TIDES’03 Eval.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	test used unseen test data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.1 Model Settings.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	First, explore effects Null word smoothing strategies.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	$) |K ore an) 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	06 12 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	21 38 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	� )|K ore an) 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	83 79 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	61 16 0.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	two candidate translations may fade learned translation lexicons.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	33 functional words removed highlight main content topic.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Without smoothing, F- measure drops quickly two topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	following experiments, use Null word Laplace smoothing BiTAM models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Choosing number topics model selection problem.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	overall computation complexity BiTAM linear number hidden topics.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.2 Variational Inference.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Better initialization B help avoid local optimal shown § 5.5.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	learned B α fixed, variational parameters computed Eqn.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTam Null Laplace Smoothing Var.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.3 Topic-Specific Translation.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	$); South Korean occurs often economics translated “HanGuo”(li!	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	�).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTAMs discriminate two considering topics context.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	values relatively sharper, clearly favors one candidates.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.4 Evaluating Word.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Alignments evaluate word alignment accuracies various settings.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Additional heuristics applied improve accuracies.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	45 8 15 .7 0 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	82 2 17 .7 0 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	92 6 18 .2 5 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	93 7 6.954 17 .93 18.14 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	90 4 6.976 18 .13 18.05 6.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	close look three BiTAMs yield significant difference.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.5 Boosting BiTAM Models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	boosted alignments denoted BUDA BBDA Table.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5, corresponding uni-direction bi-direction alignments, respectively.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	see improvement alignment quality.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5.6 Evaluating Translations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	trigram language model built using 180 million English words.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Shown Table 4 results small- data track; large-data track results Table 5.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Similar improvements observed large-data track (see Table 5).	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	5 9 19 .1 9 7.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	7 7 21 .9 9 7.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	8 3 23 .1 8 7.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	1.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	experimental conditions similar Table.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	4.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Zhao Xing (2006) note parameter estimation (for use variational EM) suffers data sparsity use symmetric Dirichlet priors, find MAP solution.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	models enable word- alignment process leverage topical contents document-pairs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Efficient variational approximation algorithms designed inference parameter estimation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	approaches expensive, emphasize stochastic translation aspects.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	conclude brief discussion section 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	2.1 Baseline: IBM Model-1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation lexicon p(f |e) key component generative process.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	use end-user ter minology source target languages.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	first BiTAM model, assume topics sampled sentence-level.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	document- pair represented random mixture latent topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	node graph represents random variable, hexagon denotes parameter.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Un-shaded nodes hidden variables.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	plates represent replicates.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Sample sentence-number N Poisson(γ)..	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	2.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Sample topic-weight vector θd Dirichlet(α)..	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	3.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	sentence-pair (fn , en ) dtth doc-pair ,.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	3.2 BiTAM2: Monolingual Admixture.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	general, monolingual model English also rich topic-mixture.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	e generated	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	variational inference used approximate true posteriors hidden variables.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	4.1 Variational Approximation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	topics inferred directly observed bilingual data, result, improve alignment.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	3.3 BiTAM3: Word-level Admixture.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Essentially, Eqs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(810) constitute E-step, posterior estimations latent variables obtained.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	4.2 Data Sparseness Smoothing.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	reduce data sparsity problem, introduce two remedies models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	First: Laplace smoothing.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Second: interpolation smoothing.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(13) Eqn.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	4.3 Retrieving Word Alignments.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(14) i∈[1,Idn ] icon’s strength.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	evaluate BiTAM models word alignment accuracy translation quality.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Table 1: Training Test Data Statistics Tra #D oc.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	contains 14,769 alignment-links.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	evaluate translation quality, TIDES’02 Eval.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	test used development set, TIDES’03 Eval.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	test used unseen test data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.1 Model Settings.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	First, explore effects Null word smoothing strategies.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	$) |K ore an) 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	06 12 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	21 38 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	� )|K ore an) 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	83 79 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	61 16 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	two candidate translations may fade learned translation lexicons.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	33 functional words removed highlight main content topic.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Without smoothing, F- measure drops quickly two topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	following experiments, use Null word Laplace smoothing BiTAM models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Choosing number topics model selection problem.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	overall computation complexity BiTAM linear number hidden topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.2 Variational Inference.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Better initialization B help avoid local optimal shown § 5.5.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	learned B α fixed, variational parameters computed Eqn.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTam Null Laplace Smoothing Var.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.3 Topic-Specific Translation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	$); South Korean occurs often economics translated “HanGuo”(li!	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	�).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTAMs discriminate two considering topics context.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	values relatively sharper, clearly favors one candidates.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.4 Evaluating Word.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Alignments evaluate word alignment accuracies various settings.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Additional heuristics applied improve accuracies.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	45 8 15 .7 0 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	82 2 17 .7 0 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	92 6 18 .2 5 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	93 7 6.954 17 .93 18.14 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	90 4 6.976 18 .13 18.05 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	close look three BiTAMs yield significant difference.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.5 Boosting BiTAM Models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	boosted alignments denoted BUDA BBDA Table.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5, corresponding uni-direction bi-direction alignments, respectively.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	see improvement alignment quality.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.6 Evaluating Translations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	trigram language model built using 180 million English words.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Shown Table 4 results small- data track; large-data track results Table 5.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Similar improvements observed large-data track (see Table 5).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5 9 19 .1 9 7.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	7 7 21 .9 9 7.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	8 3 23 .1 8 7.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	experimental conditions similar Table.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	4.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	models enable word- alignment process leverage topical contents document-pairs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Efficient variational approximation algorithms designed inference parameter estimation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	approaches expensive, emphasize stochastic translation aspects.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	conclude brief discussion section 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	2.1 Baseline: IBM Model-1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation lexicon p(f |e) key component generative process.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	use end-user ter minology source target languages.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	first BiTAM model, assume topics sampled sentence-level.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	document- pair represented random mixture latent topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	node graph represents random variable, hexagon denotes parameter.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Un-shaded nodes hidden variables.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	plates represent replicates.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Sample sentence-number N Poisson(γ)..	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	2.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Sample topic-weight vector θd Dirichlet(α)..	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	3.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	sentence-pair (fn , en ) dtth doc-pair ,.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	3.2 BiTAM2: Monolingual Admixture.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	general, monolingual model English also rich topic-mixture.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	e generated	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	variational inference used approximate true posteriors hidden variables.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	4.1 Variational Approximation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	topics inferred directly observed bilingual data, result, improve alignment.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	3.3 BiTAM3: Word-level Admixture.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Essentially, Eqs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(810) constitute E-step, posterior estimations latent variables obtained.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	4.2 Data Sparseness Smoothing.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	reduce data sparsity problem, introduce two remedies models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	First: Laplace smoothing.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Second: interpolation smoothing.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(13) Eqn.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	4.3 Retrieving Word Alignments.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(14) i∈[1,Idn ] icon’s strength.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	evaluate BiTAM models word alignment accuracy translation quality.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Table 1: Training Test Data Statistics Tra #D oc.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	contains 14,769 alignment-links.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	evaluate translation quality, TIDES’02 Eval.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	test used development set, TIDES’03 Eval.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	test used unseen test data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.1 Model Settings.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	First, explore effects Null word smoothing strategies.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	$) |K ore an) 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	06 12 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	21 38 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	� )|K ore an) 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	83 79 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	61 16 0.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	two candidate translations may fade learned translation lexicons.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	33 functional words removed highlight main content topic.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Without smoothing, F- measure drops quickly two topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	following experiments, use Null word Laplace smoothing BiTAM models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Choosing number topics model selection problem.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	overall computation complexity BiTAM linear number hidden topics.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.2 Variational Inference.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Better initialization B help avoid local optimal shown § 5.5.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	learned B α fixed, variational parameters computed Eqn.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTam Null Laplace Smoothing Var.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.3 Topic-Specific Translation.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	$); South Korean occurs often economics translated “HanGuo”(li!	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	�).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTAMs discriminate two considering topics context.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	values relatively sharper, clearly favors one candidates.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.4 Evaluating Word.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Alignments evaluate word alignment accuracies various settings.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Additional heuristics applied improve accuracies.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	45 8 15 .7 0 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	82 2 17 .7 0 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	92 6 18 .2 5 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	93 7 6.954 17 .93 18.14 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	90 4 6.976 18 .13 18.05 6.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	close look three BiTAMs yield significant difference.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.5 Boosting BiTAM Models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	boosted alignments denoted BUDA BBDA Table.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5, corresponding uni-direction bi-direction alignments, respectively.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	see improvement alignment quality.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5.6 Evaluating Translations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	trigram language model built using 180 million English words.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Shown Table 4 results small- data track; large-data track results Table 5.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Similar improvements observed large-data track (see Table 5).	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	5 9 19 .1 9 7.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	7 7 21 .9 9 7.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	8 3 23 .1 8 7.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	1.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	experimental conditions similar Table.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	4.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
approach inspired recent studies (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010; Ruiz Federico, 2011) shown particular translation always appears specific topical contexts, topical context information great effect translation selection.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	models enable word- alignment process leverage topical contents document-pairs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Efficient variational approximation algorithms designed inference parameter estimation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	approaches expensive, emphasize stochastic translation aspects.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	conclude brief discussion section 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	2.1 Baseline: IBM Model-1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation lexicon p(f |e) key component generative process.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	use end-user ter minology source target languages.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	first BiTAM model, assume topics sampled sentence-level.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	document- pair represented random mixture latent topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	node graph represents random variable, hexagon denotes parameter.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Un-shaded nodes hidden variables.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	plates represent replicates.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Sample sentence-number N Poisson(γ)..	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	2.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Sample topic-weight vector θd Dirichlet(α)..	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	3.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	sentence-pair (fn , en ) dtth doc-pair ,.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	3.2 BiTAM2: Monolingual Admixture.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	general, monolingual model English also rich topic-mixture.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	e generated	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	variational inference used approximate true posteriors hidden variables.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	4.1 Variational Approximation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	topics inferred directly observed bilingual data, result, improve alignment.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	3.3 BiTAM3: Word-level Admixture.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Essentially, Eqs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(810) constitute E-step, posterior estimations latent variables obtained.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	4.2 Data Sparseness Smoothing.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	reduce data sparsity problem, introduce two remedies models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	First: Laplace smoothing.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Second: interpolation smoothing.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(13) Eqn.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	4.3 Retrieving Word Alignments.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(14) i∈[1,Idn ] icon’s strength.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	evaluate BiTAM models word alignment accuracy translation quality.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Table 1: Training Test Data Statistics Tra #D oc.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	contains 14,769 alignment-links.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	evaluate translation quality, TIDES’02 Eval.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	test used development set, TIDES’03 Eval.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	test used unseen test data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.1 Model Settings.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	First, explore effects Null word smoothing strategies.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	$) |K ore an) 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	06 12 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	21 38 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	� )|K ore an) 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	83 79 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	61 16 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	two candidate translations may fade learned translation lexicons.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	33 functional words removed highlight main content topic.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Without smoothing, F- measure drops quickly two topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	following experiments, use Null word Laplace smoothing BiTAM models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Choosing number topics model selection problem.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	overall computation complexity BiTAM linear number hidden topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.2 Variational Inference.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Better initialization B help avoid local optimal shown § 5.5.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	learned B α fixed, variational parameters computed Eqn.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTam Null Laplace Smoothing Var.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.3 Topic-Specific Translation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	$); South Korean occurs often economics translated “HanGuo”(li!	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	�).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTAMs discriminate two considering topics context.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	values relatively sharper, clearly favors one candidates.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.4 Evaluating Word.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Alignments evaluate word alignment accuracies various settings.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Additional heuristics applied improve accuracies.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	45 8 15 .7 0 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	82 2 17 .7 0 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	92 6 18 .2 5 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	93 7 6.954 17 .93 18.14 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	90 4 6.976 18 .13 18.05 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	close look three BiTAMs yield significant difference.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.5 Boosting BiTAM Models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	boosted alignments denoted BUDA BBDA Table.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5, corresponding uni-direction bi-direction alignments, respectively.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	see improvement alignment quality.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.6 Evaluating Translations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	trigram language model built using 180 million English words.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Shown Table 4 results small- data track; large-data track results Table 5.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Similar improvements observed large-data track (see Table 5).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5 9 19 .1 9 7.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	7 7 21 .9 9 7.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	8 3 23 .1 8 7.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	experimental conditions similar Table.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	4.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	models enable word- alignment process leverage topical contents document-pairs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Efficient variational approximation algorithms designed inference parameter estimation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	approaches expensive, emphasize stochastic translation aspects.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	conclude brief discussion section 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	2.1 Baseline: IBM Model-1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation lexicon p(f |e) key component generative process.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	use end-user ter minology source target languages.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	first BiTAM model, assume topics sampled sentence-level.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	document- pair represented random mixture latent topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	node graph represents random variable, hexagon denotes parameter.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Un-shaded nodes hidden variables.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	plates represent replicates.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Sample sentence-number N Poisson(γ)..	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	2.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Sample topic-weight vector θd Dirichlet(α)..	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	3.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	sentence-pair (fn , en ) dtth doc-pair ,.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	3.2 BiTAM2: Monolingual Admixture.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	general, monolingual model English also rich topic-mixture.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	e generated	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	variational inference used approximate true posteriors hidden variables.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	4.1 Variational Approximation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	topics inferred directly observed bilingual data, result, improve alignment.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	3.3 BiTAM3: Word-level Admixture.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Essentially, Eqs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(810) constitute E-step, posterior estimations latent variables obtained.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	4.2 Data Sparseness Smoothing.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	reduce data sparsity problem, introduce two remedies models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	First: Laplace smoothing.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Second: interpolation smoothing.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(13) Eqn.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	4.3 Retrieving Word Alignments.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(14) i∈[1,Idn ] icon’s strength.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	evaluate BiTAM models word alignment accuracy translation quality.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Table 1: Training Test Data Statistics Tra #D oc.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	contains 14,769 alignment-links.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	evaluate translation quality, TIDES’02 Eval.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	test used development set, TIDES’03 Eval.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	test used unseen test data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.1 Model Settings.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	First, explore effects Null word smoothing strategies.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	$) |K ore an) 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	06 12 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	21 38 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	� )|K ore an) 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	83 79 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	61 16 0.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	two candidate translations may fade learned translation lexicons.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	33 functional words removed highlight main content topic.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Without smoothing, F- measure drops quickly two topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	following experiments, use Null word Laplace smoothing BiTAM models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Choosing number topics model selection problem.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	overall computation complexity BiTAM linear number hidden topics.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.2 Variational Inference.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Better initialization B help avoid local optimal shown § 5.5.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	learned B α fixed, variational parameters computed Eqn.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTam Null Laplace Smoothing Var.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.3 Topic-Specific Translation.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	$); South Korean occurs often economics translated “HanGuo”(li!	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	�).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTAMs discriminate two considering topics context.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	values relatively sharper, clearly favors one candidates.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.4 Evaluating Word.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Alignments evaluate word alignment accuracies various settings.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Additional heuristics applied improve accuracies.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	45 8 15 .7 0 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	82 2 17 .7 0 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	92 6 18 .2 5 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	93 7 6.954 17 .93 18.14 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	90 4 6.976 18 .13 18.05 6.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	close look three BiTAMs yield significant difference.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.5 Boosting BiTAM Models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	boosted alignments denoted BUDA BBDA Table.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5, corresponding uni-direction bi-direction alignments, respectively.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	see improvement alignment quality.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5.6 Evaluating Translations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	trigram language model built using 180 million English words.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Shown Table 4 results small- data track; large-data track results Table 5.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Similar improvements observed large-data track (see Table 5).	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	5 9 19 .1 9 7.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	7 7 21 .9 9 7.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	8 3 23 .1 8 7.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	1.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	experimental conditions similar Table.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	4.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Assuming bilingual sentence constitutes mixture hidden topics word pair follows topic-specific bilingual translation model, Zhao Xing (2006,2007) presented bilingual topical admixture formalism improve word alignment capturing topic sharing different levels linguistic granularity.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	models enable word- alignment process leverage topical contents document-pairs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Efficient variational approximation algorithms designed inference parameter estimation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	approaches expensive, emphasize stochastic translation aspects.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	conclude brief discussion section 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	2.1 Baseline: IBM Model-1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation lexicon p(f |e) key component generative process.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	use end-user ter minology source target languages.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	first BiTAM model, assume topics sampled sentence-level.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	document- pair represented random mixture latent topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	node graph represents random variable, hexagon denotes parameter.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Un-shaded nodes hidden variables.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	plates represent replicates.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Sample sentence-number N Poisson(γ)..	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	2.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Sample topic-weight vector θd Dirichlet(α)..	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	3.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	sentence-pair (fn , en ) dtth doc-pair ,.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	3.2 BiTAM2: Monolingual Admixture.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	general, monolingual model English also rich topic-mixture.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	e generated	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	variational inference used approximate true posteriors hidden variables.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	4.1 Variational Approximation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	topics inferred directly observed bilingual data, result, improve alignment.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	3.3 BiTAM3: Word-level Admixture.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Essentially, Eqs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(810) constitute E-step, posterior estimations latent variables obtained.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	4.2 Data Sparseness Smoothing.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	reduce data sparsity problem, introduce two remedies models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	First: Laplace smoothing.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Second: interpolation smoothing.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(13) Eqn.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	4.3 Retrieving Word Alignments.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(14) i∈[1,Idn ] icon’s strength.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	evaluate BiTAM models word alignment accuracy translation quality.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Table 1: Training Test Data Statistics Tra #D oc.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	contains 14,769 alignment-links.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	evaluate translation quality, TIDES’02 Eval.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	test used development set, TIDES’03 Eval.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	test used unseen test data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.1 Model Settings.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	First, explore effects Null word smoothing strategies.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	$) |K ore an) 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	06 12 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	21 38 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	� )|K ore an) 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	83 79 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	61 16 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	two candidate translations may fade learned translation lexicons.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	33 functional words removed highlight main content topic.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Without smoothing, F- measure drops quickly two topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	following experiments, use Null word Laplace smoothing BiTAM models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Choosing number topics model selection problem.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	overall computation complexity BiTAM linear number hidden topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.2 Variational Inference.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Better initialization B help avoid local optimal shown § 5.5.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	learned B α fixed, variational parameters computed Eqn.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTam Null Laplace Smoothing Var.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.3 Topic-Specific Translation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	$); South Korean occurs often economics translated “HanGuo”(li!	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	�).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTAMs discriminate two considering topics context.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	values relatively sharper, clearly favors one candidates.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.4 Evaluating Word.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Alignments evaluate word alignment accuracies various settings.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Additional heuristics applied improve accuracies.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	45 8 15 .7 0 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	82 2 17 .7 0 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	92 6 18 .2 5 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	93 7 6.954 17 .93 18.14 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	90 4 6.976 18 .13 18.05 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	close look three BiTAMs yield significant difference.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.5 Boosting BiTAM Models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	boosted alignments denoted BUDA BBDA Table.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5, corresponding uni-direction bi-direction alignments, respectively.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	see improvement alignment quality.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.6 Evaluating Translations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	trigram language model built using 180 million English words.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Shown Table 4 results small- data track; large-data track results Table 5.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Similar improvements observed large-data track (see Table 5).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5 9 19 .1 9 7.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	7 7 21 .9 9 7.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	8 3 23 .1 8 7.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	experimental conditions similar Table.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	4.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	models enable word- alignment process leverage topical contents document-pairs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Efficient variational approximation algorithms designed inference parameter estimation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	approaches expensive, emphasize stochastic translation aspects.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	conclude brief discussion section 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	2.1 Baseline: IBM Model-1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation lexicon p(f |e) key component generative process.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	use end-user ter minology source target languages.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	first BiTAM model, assume topics sampled sentence-level.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	document- pair represented random mixture latent topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	node graph represents random variable, hexagon denotes parameter.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Un-shaded nodes hidden variables.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	plates represent replicates.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Sample sentence-number N Poisson(γ)..	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	2.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Sample topic-weight vector θd Dirichlet(α)..	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	3.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	sentence-pair (fn , en ) dtth doc-pair ,.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	3.2 BiTAM2: Monolingual Admixture.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	general, monolingual model English also rich topic-mixture.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	e generated	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	variational inference used approximate true posteriors hidden variables.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	4.1 Variational Approximation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	topics inferred directly observed bilingual data, result, improve alignment.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	3.3 BiTAM3: Word-level Admixture.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Essentially, Eqs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(810) constitute E-step, posterior estimations latent variables obtained.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	4.2 Data Sparseness Smoothing.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	reduce data sparsity problem, introduce two remedies models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	First: Laplace smoothing.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Second: interpolation smoothing.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(13) Eqn.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	4.3 Retrieving Word Alignments.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(14) i∈[1,Idn ] icon’s strength.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	evaluate BiTAM models word alignment accuracy translation quality.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Table 1: Training Test Data Statistics Tra #D oc.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	contains 14,769 alignment-links.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	evaluate translation quality, TIDES’02 Eval.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	test used development set, TIDES’03 Eval.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	test used unseen test data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.1 Model Settings.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	First, explore effects Null word smoothing strategies.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	$) |K ore an) 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	06 12 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	21 38 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	� )|K ore an) 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	83 79 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	61 16 0.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	two candidate translations may fade learned translation lexicons.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	33 functional words removed highlight main content topic.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Without smoothing, F- measure drops quickly two topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	following experiments, use Null word Laplace smoothing BiTAM models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Choosing number topics model selection problem.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	overall computation complexity BiTAM linear number hidden topics.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.2 Variational Inference.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Better initialization B help avoid local optimal shown § 5.5.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	learned B α fixed, variational parameters computed Eqn.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTam Null Laplace Smoothing Var.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.3 Topic-Specific Translation.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	$); South Korean occurs often economics translated “HanGuo”(li!	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	�).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTAMs discriminate two considering topics context.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	values relatively sharper, clearly favors one candidates.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.4 Evaluating Word.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Alignments evaluate word alignment accuracies various settings.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Additional heuristics applied improve accuracies.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	45 8 15 .7 0 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	82 2 17 .7 0 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	92 6 18 .2 5 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	93 7 6.954 17 .93 18.14 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	90 4 6.976 18 .13 18.05 6.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	close look three BiTAMs yield significant difference.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.5 Boosting BiTAM Models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	boosted alignments denoted BUDA BBDA Table.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5, corresponding uni-direction bi-direction alignments, respectively.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	see improvement alignment quality.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5.6 Evaluating Translations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	trigram language model built using 180 million English words.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Shown Table 4 results small- data track; large-data track results Table 5.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Similar improvements observed large-data track (see Table 5).	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	5 9 19 .1 9 7.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	7 7 21 .9 9 7.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	8 3 23 .1 8 7.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	1.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	experimental conditions similar Table.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	4.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
â€¢ addition utilization in-domain monolingual corpora, method different previous works (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007; Gong Zhou, 2010) following aspects: (1) use different topic model â€” HTMM different assumption PLSA LDA; (2) rather modeling topic-dependent translation lexicons training process, estimate topic-specific lexical probability taking account topical context extracting word pairs, method also directly applied topic-dependent phrase probability modeling.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	models enable word- alignment process leverage topical contents document-pairs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Efficient variational approximation algorithms designed inference parameter estimation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	approaches expensive, emphasize stochastic translation aspects.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	conclude brief discussion section 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	2.1 Baseline: IBM Model-1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation lexicon p(f |e) key component generative process.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	use end-user ter minology source target languages.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	first BiTAM model, assume topics sampled sentence-level.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	document- pair represented random mixture latent topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	node graph represents random variable, hexagon denotes parameter.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Un-shaded nodes hidden variables.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	plates represent replicates.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Sample sentence-number N Poisson(γ)..	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	2.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Sample topic-weight vector θd Dirichlet(α)..	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	3.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	sentence-pair (fn , en ) dtth doc-pair ,.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	3.2 BiTAM2: Monolingual Admixture.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	general, monolingual model English also rich topic-mixture.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	e generated	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	variational inference used approximate true posteriors hidden variables.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	4.1 Variational Approximation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	topics inferred directly observed bilingual data, result, improve alignment.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	3.3 BiTAM3: Word-level Admixture.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Essentially, Eqs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(810) constitute E-step, posterior estimations latent variables obtained.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	4.2 Data Sparseness Smoothing.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	reduce data sparsity problem, introduce two remedies models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	First: Laplace smoothing.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Second: interpolation smoothing.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(13) Eqn.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	4.3 Retrieving Word Alignments.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(14) i∈[1,Idn ] icon’s strength.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	evaluate BiTAM models word alignment accuracy translation quality.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Table 1: Training Test Data Statistics Tra #D oc.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	contains 14,769 alignment-links.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	evaluate translation quality, TIDES’02 Eval.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	test used development set, TIDES’03 Eval.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	test used unseen test data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.1 Model Settings.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	First, explore effects Null word smoothing strategies.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	$) |K ore an) 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	06 12 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	21 38 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	� )|K ore an) 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	83 79 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	61 16 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	two candidate translations may fade learned translation lexicons.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	33 functional words removed highlight main content topic.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Without smoothing, F- measure drops quickly two topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	following experiments, use Null word Laplace smoothing BiTAM models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Choosing number topics model selection problem.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	overall computation complexity BiTAM linear number hidden topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.2 Variational Inference.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Better initialization B help avoid local optimal shown § 5.5.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	learned B α fixed, variational parameters computed Eqn.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTam Null Laplace Smoothing Var.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.3 Topic-Specific Translation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	$); South Korean occurs often economics translated “HanGuo”(li!	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	�).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTAMs discriminate two considering topics context.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	values relatively sharper, clearly favors one candidates.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.4 Evaluating Word.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Alignments evaluate word alignment accuracies various settings.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Additional heuristics applied improve accuracies.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	45 8 15 .7 0 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	82 2 17 .7 0 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	92 6 18 .2 5 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	93 7 6.954 17 .93 18.14 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	90 4 6.976 18 .13 18.05 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	close look three BiTAMs yield significant difference.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.5 Boosting BiTAM Models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	boosted alignments denoted BUDA BBDA Table.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5, corresponding uni-direction bi-direction alignments, respectively.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	see improvement alignment quality.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.6 Evaluating Translations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	trigram language model built using 180 million English words.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Shown Table 4 results small- data track; large-data track results Table 5.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Similar improvements observed large-data track (see Table 5).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5 9 19 .1 9 7.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	7 7 21 .9 9 7.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	8 3 23 .1 8 7.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	experimental conditions similar Table.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	4.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	models enable word- alignment process leverage topical contents document-pairs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Efficient variational approximation algorithms designed inference parameter estimation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	approaches expensive, emphasize stochastic translation aspects.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	conclude brief discussion section 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	2.1 Baseline: IBM Model-1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation lexicon p(f |e) key component generative process.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	use end-user ter minology source target languages.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	first BiTAM model, assume topics sampled sentence-level.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	document- pair represented random mixture latent topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	node graph represents random variable, hexagon denotes parameter.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Un-shaded nodes hidden variables.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	plates represent replicates.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Sample sentence-number N Poisson(γ)..	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	2.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Sample topic-weight vector θd Dirichlet(α)..	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	3.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	sentence-pair (fn , en ) dtth doc-pair ,.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	3.2 BiTAM2: Monolingual Admixture.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	general, monolingual model English also rich topic-mixture.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	e generated	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	variational inference used approximate true posteriors hidden variables.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	4.1 Variational Approximation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	topics inferred directly observed bilingual data, result, improve alignment.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	3.3 BiTAM3: Word-level Admixture.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Essentially, Eqs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(810) constitute E-step, posterior estimations latent variables obtained.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	4.2 Data Sparseness Smoothing.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	reduce data sparsity problem, introduce two remedies models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	First: Laplace smoothing.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Second: interpolation smoothing.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(13) Eqn.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	4.3 Retrieving Word Alignments.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(14) i∈[1,Idn ] icon’s strength.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	evaluate BiTAM models word alignment accuracy translation quality.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Table 1: Training Test Data Statistics Tra #D oc.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	contains 14,769 alignment-links.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	evaluate translation quality, TIDES’02 Eval.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	test used development set, TIDES’03 Eval.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	test used unseen test data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.1 Model Settings.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	First, explore effects Null word smoothing strategies.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	$) |K ore an) 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	06 12 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	21 38 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	� )|K ore an) 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	83 79 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	61 16 0.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	two candidate translations may fade learned translation lexicons.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	33 functional words removed highlight main content topic.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Without smoothing, F- measure drops quickly two topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	following experiments, use Null word Laplace smoothing BiTAM models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Choosing number topics model selection problem.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	overall computation complexity BiTAM linear number hidden topics.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.2 Variational Inference.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Better initialization B help avoid local optimal shown § 5.5.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	learned B α fixed, variational parameters computed Eqn.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTam Null Laplace Smoothing Var.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.3 Topic-Specific Translation.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	$); South Korean occurs often economics translated “HanGuo”(li!	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	�).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTAMs discriminate two considering topics context.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	values relatively sharper, clearly favors one candidates.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.4 Evaluating Word.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Alignments evaluate word alignment accuracies various settings.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Additional heuristics applied improve accuracies.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	45 8 15 .7 0 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	82 2 17 .7 0 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	92 6 18 .2 5 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	93 7 6.954 17 .93 18.14 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	90 4 6.976 18 .13 18.05 6.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	close look three BiTAMs yield significant difference.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.5 Boosting BiTAM Models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	boosted alignments denoted BUDA BBDA Table.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5, corresponding uni-direction bi-direction alignments, respectively.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	see improvement alignment quality.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5.6 Evaluating Translations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	trigram language model built using 180 million English words.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Shown Table 4 results small- data track; large-data track results Table 5.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Similar improvements observed large-data track (see Table 5).	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	5 9 19 .1 9 7.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	7 7 21 .9 9 7.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	8 3 23 .1 8 7.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	1.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	experimental conditions similar Table.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	4.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
exploit topic information statistical machine translation (SMT), researchers proposed various topic-specific lexicon translation models (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007) improve translation quality.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	models enable word- alignment process leverage topical contents document-pairs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Efficient variational approximation algorithms designed inference parameter estimation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	approaches expensive, emphasize stochastic translation aspects.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	1
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	conclude brief discussion section 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	2.1 Baseline: IBM Model-1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation lexicon p(f |e) key component generative process.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	use end-user ter minology source target languages.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	first BiTAM model, assume topics sampled sentence-level.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	document- pair represented random mixture latent topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	node graph represents random variable, hexagon denotes parameter.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Un-shaded nodes hidden variables.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	plates represent replicates.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Sample sentence-number N Poisson(γ)..	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	2.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Sample topic-weight vector θd Dirichlet(α)..	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	3.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	sentence-pair (fn , en ) dtth doc-pair ,.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	3.2 BiTAM2: Monolingual Admixture.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	general, monolingual model English also rich topic-mixture.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	e generated	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	variational inference used approximate true posteriors hidden variables.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	4.1 Variational Approximation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	topics inferred directly observed bilingual data, result, improve alignment.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	3.3 BiTAM3: Word-level Admixture.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Essentially, Eqs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(810) constitute E-step, posterior estimations latent variables obtained.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	4.2 Data Sparseness Smoothing.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	reduce data sparsity problem, introduce two remedies models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	First: Laplace smoothing.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Second: interpolation smoothing.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(13) Eqn.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	4.3 Retrieving Word Alignments.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(14) i∈[1,Idn ] icon’s strength.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	evaluate BiTAM models word alignment accuracy translation quality.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Table 1: Training Test Data Statistics Tra #D oc.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	contains 14,769 alignment-links.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	evaluate translation quality, TIDES’02 Eval.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	test used development set, TIDES’03 Eval.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	test used unseen test data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.1 Model Settings.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	First, explore effects Null word smoothing strategies.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	$) |K ore an) 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	06 12 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	21 38 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	� )|K ore an) 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	83 79 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	61 16 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	two candidate translations may fade learned translation lexicons.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	33 functional words removed highlight main content topic.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Without smoothing, F- measure drops quickly two topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	following experiments, use Null word Laplace smoothing BiTAM models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Choosing number topics model selection problem.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	overall computation complexity BiTAM linear number hidden topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.2 Variational Inference.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Better initialization B help avoid local optimal shown § 5.5.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	learned B α fixed, variational parameters computed Eqn.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTam Null Laplace Smoothing Var.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.3 Topic-Specific Translation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	$); South Korean occurs often economics translated “HanGuo”(li!	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	�).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTAMs discriminate two considering topics context.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	values relatively sharper, clearly favors one candidates.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.4 Evaluating Word.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Alignments evaluate word alignment accuracies various settings.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Additional heuristics applied improve accuracies.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	45 8 15 .7 0 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	82 2 17 .7 0 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	92 6 18 .2 5 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	93 7 6.954 17 .93 18.14 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	90 4 6.976 18 .13 18.05 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	close look three BiTAMs yield significant difference.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.5 Boosting BiTAM Models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	boosted alignments denoted BUDA BBDA Table.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5, corresponding uni-direction bi-direction alignments, respectively.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	see improvement alignment quality.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.6 Evaluating Translations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	trigram language model built using 180 million English words.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Shown Table 4 results small- data track; large-data track results Table 5.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Similar improvements observed large-data track (see Table 5).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5 9 19 .1 9 7.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	7 7 21 .9 9 7.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	8 3 23 .1 8 7.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	experimental conditions similar Table.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	4.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	models enable word- alignment process leverage topical contents document-pairs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Efficient variational approximation algorithms designed inference parameter estimation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	approaches expensive, emphasize stochastic translation aspects.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	1
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	conclude brief discussion section 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	2.1 Baseline: IBM Model-1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation lexicon p(f |e) key component generative process.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	use end-user ter minology source target languages.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	first BiTAM model, assume topics sampled sentence-level.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	document- pair represented random mixture latent topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	node graph represents random variable, hexagon denotes parameter.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Un-shaded nodes hidden variables.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	plates represent replicates.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Sample sentence-number N Poisson(γ)..	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	2.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Sample topic-weight vector θd Dirichlet(α)..	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	3.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	sentence-pair (fn , en ) dtth doc-pair ,.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	3.2 BiTAM2: Monolingual Admixture.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	general, monolingual model English also rich topic-mixture.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	e generated	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	variational inference used approximate true posteriors hidden variables.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	4.1 Variational Approximation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	topics inferred directly observed bilingual data, result, improve alignment.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	3.3 BiTAM3: Word-level Admixture.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Essentially, Eqs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(810) constitute E-step, posterior estimations latent variables obtained.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	4.2 Data Sparseness Smoothing.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	reduce data sparsity problem, introduce two remedies models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	First: Laplace smoothing.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Second: interpolation smoothing.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(13) Eqn.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	4.3 Retrieving Word Alignments.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(14) i∈[1,Idn ] icon’s strength.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	evaluate BiTAM models word alignment accuracy translation quality.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Table 1: Training Test Data Statistics Tra #D oc.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	contains 14,769 alignment-links.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	evaluate translation quality, TIDES’02 Eval.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	test used development set, TIDES’03 Eval.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	test used unseen test data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.1 Model Settings.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	First, explore effects Null word smoothing strategies.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	$) |K ore an) 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	06 12 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	21 38 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	� )|K ore an) 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	83 79 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	61 16 0.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	two candidate translations may fade learned translation lexicons.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	33 functional words removed highlight main content topic.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Without smoothing, F- measure drops quickly two topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	following experiments, use Null word Laplace smoothing BiTAM models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Choosing number topics model selection problem.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	overall computation complexity BiTAM linear number hidden topics.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.2 Variational Inference.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Better initialization B help avoid local optimal shown § 5.5.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	learned B α fixed, variational parameters computed Eqn.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTam Null Laplace Smoothing Var.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.3 Topic-Specific Translation.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	$); South Korean occurs often economics translated “HanGuo”(li!	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	�).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTAMs discriminate two considering topics context.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	values relatively sharper, clearly favors one candidates.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.4 Evaluating Word.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Alignments evaluate word alignment accuracies various settings.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Additional heuristics applied improve accuracies.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	45 8 15 .7 0 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	82 2 17 .7 0 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	92 6 18 .2 5 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	93 7 6.954 17 .93 18.14 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	90 4 6.976 18 .13 18.05 6.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	close look three BiTAMs yield significant difference.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.5 Boosting BiTAM Models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	boosted alignments denoted BUDA BBDA Table.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5, corresponding uni-direction bi-direction alignments, respectively.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	see improvement alignment quality.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5.6 Evaluating Translations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	trigram language model built using 180 million English words.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Shown Table 4 results small- data track; large-data track results Table 5.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Similar improvements observed large-data track (see Table 5).	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	5 9 19 .1 9 7.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	7 7 21 .9 9 7.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	8 3 23 .1 8 7.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	1.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	experimental conditions similar Table.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	4.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Sentences translated consistence topics (Zhao Xing, 2006; Zhao Xing, 2007; Tam et al., 2007).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	models enable word- alignment process leverage topical contents document-pairs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Efficient variational approximation algorithms designed inference parameter estimation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	approaches expensive, emphasize stochastic translation aspects.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	conclude brief discussion section 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	2.1 Baseline: IBM Model-1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation lexicon p(f |e) key component generative process.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	use end-user ter minology source target languages.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	first BiTAM model, assume topics sampled sentence-level.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	document- pair represented random mixture latent topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	node graph represents random variable, hexagon denotes parameter.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Un-shaded nodes hidden variables.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	plates represent replicates.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Sample sentence-number N Poisson(γ)..	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	2.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Sample topic-weight vector θd Dirichlet(α)..	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	3.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	sentence-pair (fn , en ) dtth doc-pair ,.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	3.2 BiTAM2: Monolingual Admixture.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	general, monolingual model English also rich topic-mixture.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	e generated	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	variational inference used approximate true posteriors hidden variables.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	4.1 Variational Approximation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	topics inferred directly observed bilingual data, result, improve alignment.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	3.3 BiTAM3: Word-level Admixture.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Essentially, Eqs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(810) constitute E-step, posterior estimations latent variables obtained.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	4.2 Data Sparseness Smoothing.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	reduce data sparsity problem, introduce two remedies models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	First: Laplace smoothing.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Second: interpolation smoothing.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(13) Eqn.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	4.3 Retrieving Word Alignments.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(14) i∈[1,Idn ] icon’s strength.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	evaluate BiTAM models word alignment accuracy translation quality.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Table 1: Training Test Data Statistics Tra #D oc.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	contains 14,769 alignment-links.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	evaluate translation quality, TIDES’02 Eval.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	test used development set, TIDES’03 Eval.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	test used unseen test data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.1 Model Settings.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	First, explore effects Null word smoothing strategies.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	$) |K ore an) 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	06 12 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	21 38 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	� )|K ore an) 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	83 79 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	61 16 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	two candidate translations may fade learned translation lexicons.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	33 functional words removed highlight main content topic.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Without smoothing, F- measure drops quickly two topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	following experiments, use Null word Laplace smoothing BiTAM models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Choosing number topics model selection problem.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	overall computation complexity BiTAM linear number hidden topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.2 Variational Inference.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Better initialization B help avoid local optimal shown § 5.5.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	learned B α fixed, variational parameters computed Eqn.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTam Null Laplace Smoothing Var.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.3 Topic-Specific Translation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	$); South Korean occurs often economics translated “HanGuo”(li!	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	�).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTAMs discriminate two considering topics context.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	values relatively sharper, clearly favors one candidates.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.4 Evaluating Word.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Alignments evaluate word alignment accuracies various settings.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Additional heuristics applied improve accuracies.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	45 8 15 .7 0 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	82 2 17 .7 0 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	92 6 18 .2 5 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	93 7 6.954 17 .93 18.14 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	90 4 6.976 18 .13 18.05 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	close look three BiTAMs yield significant difference.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.5 Boosting BiTAM Models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	boosted alignments denoted BUDA BBDA Table.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5, corresponding uni-direction bi-direction alignments, respectively.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	see improvement alignment quality.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.6 Evaluating Translations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	trigram language model built using 180 million English words.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Shown Table 4 results small- data track; large-data track results Table 5.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Similar improvements observed large-data track (see Table 5).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5 9 19 .1 9 7.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	7 7 21 .9 9 7.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	8 3 23 .1 8 7.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	experimental conditions similar Table.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	4.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	models enable word- alignment process leverage topical contents document-pairs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Efficient variational approximation algorithms designed inference parameter estimation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	topical information, translation models expected sharper word-alignment process less ambiguous.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	approaches expensive, emphasize stochastic translation aspects.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	conclude brief discussion section 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	2.1 Baseline: IBM Model-1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation lexicon p(f |e) key component generative process.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	use end-user ter minology source target languages.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	first BiTAM model, assume topics sampled sentence-level.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	document- pair represented random mixture latent topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	node graph represents random variable, hexagon denotes parameter.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Un-shaded nodes hidden variables.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	plates represent replicates.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	generative process, document-pair (Fd, Ed), summarized below: 1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Sample sentence-number N Poisson(γ)..	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	2.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Sample topic-weight vector θd Dirichlet(α)..	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	3.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	sentence-pair (fn , en ) dtth doc-pair ,.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(6) contains one word: “Null”, alignment link longer hidden variable.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	3.2 BiTAM2: Monolingual Admixture.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	general, monolingual model English also rich topic-mixture.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	e generated	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	variational inference used approximate true posteriors hidden variables.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	4.1 Variational Approximation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	topics inferred directly observed bilingual data, result, improve alignment.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	3.3 BiTAM3: Word-level Admixture.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Similar IBM models, “Null” word used source words translation counterparts target language.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Essentially, Eqs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(810) constitute E-step, posterior estimations latent variables obtained.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	4.2 Data Sparseness Smoothing.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	reduce data sparsity problem, introduce two remedies models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	First: Laplace smoothing.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Second: interpolation smoothing.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(13) Eqn.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	4.3 Retrieving Word Alignments.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(14) i∈[1,Idn ] icon’s strength.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	evaluate BiTAM models word alignment accuracy translation quality.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Table 1: Training Test Data Statistics Tra #D oc.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	contains 14,769 alignment-links.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	evaluate translation quality, TIDES’02 Eval.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	test used development set, TIDES’03 Eval.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	test used unseen test data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.1 Model Settings.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	First, explore effects Null word smoothing strategies.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	pics Le xic ons pic1 pic2 pic3 Co oc.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	$) |K ore an) 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	06 12 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	21 38 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	� )|K ore an) 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	83 79 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	61 16 0.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	two candidate translations may fade learned translation lexicons.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	33 functional words removed highlight main content topic.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Laplace smoothing chosen emphasize BiTAM’s strength.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Without smoothing, F- measure drops quickly two topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	following experiments, use Null word Laplace smoothing BiTAM models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Choosing number topics model selection problem.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	overall computation complexity BiTAM linear number hidden topics.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.2 Variational Inference.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Better initialization B help avoid local optimal shown § 5.5.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	learned B α fixed, variational parameters computed Eqn.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTam Null Laplace Smoothing Var.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.3 Topic-Specific Translation.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	$); South Korean occurs often economics translated “HanGuo”(li!	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	�).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTAMs discriminate two considering topics context.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	values relatively sharper, clearly favors one candidates.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.4 Evaluating Word.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Alignments evaluate word alignment accuracies various settings.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Additional heuristics applied improve accuracies.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	45 8 15 .7 0 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	82 2 17 .7 0 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	92 6 18 .2 5 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	93 7 6.954 17 .93 18.14 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	90 4 6.976 18 .13 18.05 6.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	close look three BiTAMs yield significant difference.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.5 Boosting BiTAM Models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	boosted alignments denoted BUDA BBDA Table.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5, corresponding uni-direction bi-direction alignments, respectively.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	see improvement alignment quality.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5.6 Evaluating Translations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	trigram language model built using 180 million English words.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Shown Table 4 results small- data track; large-data track results Table 5.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Similar improvements observed large-data track (see Table 5).	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	5 9 19 .1 9 7.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	7 7 21 .9 9 7.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	8 3 23 .1 8 7.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	1.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	experimental conditions similar Table.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	4.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	proposed models significantly improve alignment accuracy lead better translation qualities.	0
Topic modeling received use SMT, instance Bilingual LSA adaptation (Tam et al., 2007), BiTAM model (Zhao Xing, 2006), uses bilingual topic model learning alignment.	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	1
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	conclude brief discussion section 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	use end-user ter minology source target languages.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	plates represent replicates.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	2.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	3.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	e generated	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	4.1 Variational Approximation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Essentially, Eqs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	First: Laplace smoothing.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Second: interpolation smoothing.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(13) Eqn.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	contains 14,769 alignment-links.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	test used unseen test data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.1 Model Settings.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	$) |K ore an) 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	06 12 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	21 38 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	� )|K ore an) 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	83 79 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	61 16 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Choosing number topics model selection problem.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.2 Variational Inference.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	�).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.4 Evaluating Word.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	45 8 15 .7 0 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	82 2 17 .7 0 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	92 6 18 .2 5 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	see improvement alignment quality.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.6 Evaluating Translations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5 9 19 .1 9 7.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	7 7 21 .9 9 7.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	8 3 23 .1 8 7.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	experimental conditions similar Table.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	4.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	1
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	conclude brief discussion section 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	use end-user ter minology source target languages.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	plates represent replicates.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	2.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	3.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	e generated	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	4.1 Variational Approximation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Essentially, Eqs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	First: Laplace smoothing.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Second: interpolation smoothing.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(13) Eqn.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	contains 14,769 alignment-links.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	test used unseen test data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.1 Model Settings.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	$) |K ore an) 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	06 12 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	21 38 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	� )|K ore an) 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	83 79 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	61 16 0.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Choosing number topics model selection problem.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.2 Variational Inference.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	�).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.4 Evaluating Word.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	45 8 15 .7 0 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	82 2 17 .7 0 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	92 6 18 .2 5 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	see improvement alignment quality.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5.6 Evaluating Translations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	5 9 19 .1 9 7.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	7 7 21 .9 9 7.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	8 3 23 .1 8 7.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	1.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	experimental conditions similar Table.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	4.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
avoid need hard decisions domain membership, used topic modeling improve SMT performance, e.g., using latent semantic analysis (Tam et al., 2007) â€˜biTAMâ€™ (Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	models enable word- alignment process leverage topical contents document-pairs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Efficient variational approximation algorithms designed inference parameter estimation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	topical information, translation models expected sharper word-alignment process less ambiguous.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	approaches expensive, emphasize stochastic translation aspects.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	conclude brief discussion section 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	2.1 Baseline: IBM Model-1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation lexicon p(f |e) key component generative process.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	use end-user ter minology source target languages.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	first BiTAM model, assume topics sampled sentence-level.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	document- pair represented random mixture latent topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	node graph represents random variable, hexagon denotes parameter.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Un-shaded nodes hidden variables.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	plates represent replicates.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	generative process, document-pair (Fd, Ed), summarized below: 1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Sample sentence-number N Poisson(γ)..	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	2.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Sample topic-weight vector θd Dirichlet(α)..	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	3.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	sentence-pair (fn , en ) dtth doc-pair ,.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(6) contains one word: “Null”, alignment link longer hidden variable.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	3.2 BiTAM2: Monolingual Admixture.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	general, monolingual model English also rich topic-mixture.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	e generated	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	variational inference used approximate true posteriors hidden variables.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	4.1 Variational Approximation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	topics inferred directly observed bilingual data, result, improve alignment.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	3.3 BiTAM3: Word-level Admixture.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Similar IBM models, “Null” word used source words translation counterparts target language.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Essentially, Eqs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(810) constitute E-step, posterior estimations latent variables obtained.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	4.2 Data Sparseness Smoothing.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	reduce data sparsity problem, introduce two remedies models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	First: Laplace smoothing.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Second: interpolation smoothing.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(13) Eqn.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	4.3 Retrieving Word Alignments.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(14) i∈[1,Idn ] icon’s strength.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	evaluate BiTAM models word alignment accuracy translation quality.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Table 1: Training Test Data Statistics Tra #D oc.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	contains 14,769 alignment-links.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	evaluate translation quality, TIDES’02 Eval.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	test used development set, TIDES’03 Eval.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	test used unseen test data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.1 Model Settings.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	First, explore effects Null word smoothing strategies.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	pics Le xic ons pic1 pic2 pic3 Co oc.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	$) |K ore an) 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	06 12 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	21 38 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	� )|K ore an) 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	83 79 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	61 16 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	two candidate translations may fade learned translation lexicons.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	33 functional words removed highlight main content topic.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Laplace smoothing chosen emphasize BiTAM’s strength.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Without smoothing, F- measure drops quickly two topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	following experiments, use Null word Laplace smoothing BiTAM models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Choosing number topics model selection problem.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	overall computation complexity BiTAM linear number hidden topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.2 Variational Inference.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Better initialization B help avoid local optimal shown § 5.5.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	learned B α fixed, variational parameters computed Eqn.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTam Null Laplace Smoothing Var.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.3 Topic-Specific Translation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	$); South Korean occurs often economics translated “HanGuo”(li!	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	�).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTAMs discriminate two considering topics context.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	values relatively sharper, clearly favors one candidates.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.4 Evaluating Word.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Alignments evaluate word alignment accuracies various settings.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Additional heuristics applied improve accuracies.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	45 8 15 .7 0 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	82 2 17 .7 0 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	92 6 18 .2 5 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	93 7 6.954 17 .93 18.14 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	90 4 6.976 18 .13 18.05 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	close look three BiTAMs yield significant difference.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.5 Boosting BiTAM Models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	boosted alignments denoted BUDA BBDA Table.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5, corresponding uni-direction bi-direction alignments, respectively.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	see improvement alignment quality.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.6 Evaluating Translations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	trigram language model built using 180 million English words.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Shown Table 4 results small- data track; large-data track results Table 5.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Similar improvements observed large-data track (see Table 5).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5 9 19 .1 9 7.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	7 7 21 .9 9 7.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	8 3 23 .1 8 7.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	experimental conditions similar Table.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	4.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	proposed models significantly improve alignment accuracy lead better translation qualities.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	models enable word- alignment process leverage topical contents document-pairs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Efficient variational approximation algorithms designed inference parameter estimation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	topical information, translation models expected sharper word-alignment process less ambiguous.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	approaches expensive, emphasize stochastic translation aspects.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	conclude brief discussion section 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	2.1 Baseline: IBM Model-1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation lexicon p(f |e) key component generative process.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	use end-user ter minology source target languages.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	first BiTAM model, assume topics sampled sentence-level.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	document- pair represented random mixture latent topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	node graph represents random variable, hexagon denotes parameter.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Un-shaded nodes hidden variables.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	plates represent replicates.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	generative process, document-pair (Fd, Ed), summarized below: 1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Sample sentence-number N Poisson(γ)..	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	2.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Sample topic-weight vector θd Dirichlet(α)..	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	3.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	sentence-pair (fn , en ) dtth doc-pair ,.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(6) contains one word: “Null”, alignment link longer hidden variable.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	3.2 BiTAM2: Monolingual Admixture.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	general, monolingual model English also rich topic-mixture.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	e generated	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	variational inference used approximate true posteriors hidden variables.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	4.1 Variational Approximation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	topics inferred directly observed bilingual data, result, improve alignment.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	3.3 BiTAM3: Word-level Admixture.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Similar IBM models, “Null” word used source words translation counterparts target language.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Essentially, Eqs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(810) constitute E-step, posterior estimations latent variables obtained.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	4.2 Data Sparseness Smoothing.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	reduce data sparsity problem, introduce two remedies models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	First: Laplace smoothing.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Second: interpolation smoothing.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(13) Eqn.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	4.3 Retrieving Word Alignments.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(14) i∈[1,Idn ] icon’s strength.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	evaluate BiTAM models word alignment accuracy translation quality.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Table 1: Training Test Data Statistics Tra #D oc.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	contains 14,769 alignment-links.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	evaluate translation quality, TIDES’02 Eval.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	test used development set, TIDES’03 Eval.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	test used unseen test data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.1 Model Settings.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	First, explore effects Null word smoothing strategies.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	pics Le xic ons pic1 pic2 pic3 Co oc.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	$) |K ore an) 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	06 12 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	21 38 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	� )|K ore an) 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	83 79 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	61 16 0.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	two candidate translations may fade learned translation lexicons.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	33 functional words removed highlight main content topic.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Laplace smoothing chosen emphasize BiTAM’s strength.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Without smoothing, F- measure drops quickly two topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	following experiments, use Null word Laplace smoothing BiTAM models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Choosing number topics model selection problem.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	overall computation complexity BiTAM linear number hidden topics.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.2 Variational Inference.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Better initialization B help avoid local optimal shown § 5.5.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	learned B α fixed, variational parameters computed Eqn.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTam Null Laplace Smoothing Var.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.3 Topic-Specific Translation.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	$); South Korean occurs often economics translated “HanGuo”(li!	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	�).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTAMs discriminate two considering topics context.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	values relatively sharper, clearly favors one candidates.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.4 Evaluating Word.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Alignments evaluate word alignment accuracies various settings.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Additional heuristics applied improve accuracies.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	45 8 15 .7 0 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	82 2 17 .7 0 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	92 6 18 .2 5 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	93 7 6.954 17 .93 18.14 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	90 4 6.976 18 .13 18.05 6.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	close look three BiTAMs yield significant difference.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.5 Boosting BiTAM Models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	boosted alignments denoted BUDA BBDA Table.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5, corresponding uni-direction bi-direction alignments, respectively.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	see improvement alignment quality.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5.6 Evaluating Translations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	trigram language model built using 180 million English words.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Shown Table 4 results small- data track; large-data track results Table 5.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Similar improvements observed large-data track (see Table 5).	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	5 9 19 .1 9 7.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	7 7 21 .9 9 7.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	8 3 23 .1 8 7.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	1.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	experimental conditions similar Table.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	4.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	proposed models significantly improve alignment accuracy lead better translation qualities.	0
"(Zhao Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 baseline model, presented bilingual topic admixture model formalism.</S><S sid =""14"" ssid = ""14"">These models capture latent topics document level order reduce semantic ambiguity improve translation coherence.</S><S sid =""15"" ssid = ""15"">The models proposed provide cases better word alignment translation quality HMM IBM models EnglishChinese task."	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	conclude brief discussion section 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	use end-user ter minology source target languages.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	plates represent replicates.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	2.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	3.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	e generated	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	4.1 Variational Approximation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Essentially, Eqs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	First: Laplace smoothing.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Second: interpolation smoothing.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(13) Eqn.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	contains 14,769 alignment-links.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	test used unseen test data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.1 Model Settings.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	$) |K ore an) 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	06 12 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	21 38 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	� )|K ore an) 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	83 79 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	61 16 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Choosing number topics model selection problem.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.2 Variational Inference.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	�).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.4 Evaluating Word.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	45 8 15 .7 0 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	82 2 17 .7 0 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	92 6 18 .2 5 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	see improvement alignment quality.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.6 Evaluating Translations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5 9 19 .1 9 7.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	7 7 21 .9 9 7.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	8 3 23 .1 8 7.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	experimental conditions similar Table.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	4.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTAM: Bilingual Topic AdMixture Models forWord Alignment	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	propose novel bilingual topical admixture (BiTAM) formalism word alignment statistical machine translation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	formalism, parallel sentence-pairs within document-pair assumed constitute mixture hidden topics; word-pair follows topic-specific bilingual translation model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Three BiTAM models proposed capture topic sharing different levels linguistic granularity (i.e., sentence word levels).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	models enable word- alignment process leverage topical contents document-pairs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Efficient variational approximation algorithms designed inference parameter estimation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	inferred latent topics, BiTAM models facilitate coherent pairing bilingual linguistic entities share common topical aspects.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	preliminary experiments show proposed models improve word alignment accuracy, lead better translation quality.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Parallel data treated sets unrelated sentence-pairs state-of-the-art statistical machine translation (SMT) models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	current approaches emphasize within-sentence dependencies distortion (Brown et al., 1993), dependency alignment HMM (Vogel et al., 1996), syntax mappings (Yamada Knight, 2001).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Beyond sentence-level, corpus- level word-correlation contextual-level topical information may help disambiguate translation candidates word-alignment choices.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	example, frequent source words (e.g., functional words) likely translated words also frequent target side; words topic generally bear correlations similar translations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Extended contextual information especially useful translation models vague due reliance solely word-pair co- occurrence statistics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	example, word shot “It nice shot.” translated differently depending context sentence: goal context sports, photo within context sightseeing.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Nida (1964) stated sentence-pairs tied logic-flow document-pair; words, document-pair word-aligned one entity instead uncorrelated instances.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	paper, propose probabilistic admixture model capture latent topics underlying context document- pairs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	topical information, translation models expected sharper word-alignment process less ambiguous.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Previous works topical translation models concern mainly explicit logical representations semantics machine translation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	include knowledge-based (Nyberg Mitamura, 1992) interlingua-based (Dorr Habash, 2002) approaches.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	approaches expensive, emphasize stochastic translation aspects.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Recent investigations along line includes using word-disambiguation schemes (Carpua Wu, 2005) non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) particular translation models, showed various degrees success.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	propose new statistical formalism: Bilingual Topic AdMixture model, BiTAM, facilitate topic-based word alignment SMT.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Variants admixture models appeared population genetics (Pritchard et al., 2000) text modeling (Blei et al., 2003).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Statistically, object said derived admixture consists bag elements, sampled independently coupled way, mixture model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	typical SMT setting, document- pair corresponds object; depending chosen modeling granularity, sentence-pairs word-pairs document-pair correspond elements constituting object.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Correspondingly, latent topic sampled pair prior topic distribution induce topic-specific translations; resulting sentence-pairs word- pairs marginally dependent.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Generatively, admixture formalism enables word translations instantiated topic-specific bilingual models 969 Proceedings COLING/ACL 2006 Main Conference Poster Sessions, pages 969–976, Sydney, July 2006.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Qc 2006 Association Computational Linguistics and/or monolingual models, depending contexts.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	paper investigate three instances BiTAM model, data-driven need handcrafted knowledge engineering.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	remainder paper follows: section 2, introduce notations baselines; section 3, propose topic admixture models; section 4, present learning inference algorithms; section 5 show experiments models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	conclude brief discussion section 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	statistical machine translation, one typically uses parallel data identify entities “word-pair”, “sentence-pair”, “document- pair”.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Formally, define following terms1: • word-pair (fj , ei) basic unit word alignment, fj French word ei English word; j position indices corresponding French sentence f English sentence e. • sentence-pair (f , e) contains source sentence f sentence length J ; target sentence e length . two sentences f e translations other.• document-pair (F, E) refers two doc uments translations other.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Assuming sentences one-to-one correspondent, document-pair sequence N parallel sentence-pairs {(fn, en)}, (fn, en) ntth parallel sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	• parallel corpus C collection parallel document-pairs: {(Fd, Ed)}.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	2.1 Baseline: IBM Model-1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation process viewed operations word substitutions, permutations, insertions/deletions (Brown et al., 1993) noisy- channel modeling scheme parallel sentence-pair level.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation lexicon p(f |e) key component generative process.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	efficient way learn p(f |e) IBM1: IBM1 global optimum; efficient easily scalable large training data; one informative components re-ranking translations (Och et al., 2004).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	start IBM1 baseline model, higher-order alignment models embedded similarly within proposed framework.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	describe BiTAM formalism captures latent topical structure generalizes word alignments translations beyond sentence-level via topic sharing across sentence- pairs: E∗ = arg max p(F|E)p(E), (2) {E} p(F|E) document-level translation model, generating document F one entity.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTAM model, document-pair (F, E) treated admixture topics, induced random draws topic, pool topics, sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	unique normalized real-valued vector θ, referred topic-weight vector, captures contributions different topics, instantiated document-pair, sentence-pairs alignments generated topics mixed according common proportions.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Marginally, sentence- pair word-aligned according unique bilingual model governed hidden topical assignments.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Therefore, sentence-level translations coupled, rather independent assumed IBM models extensions.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	coupling sentence-pairs (via topic sharing across sentence-pairs according common topic-weight vector), BiTAM likely improve coherency translations treating document whole entity, instead uncorrelated segments independently aligned assembled.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	least two levels hidden topics sampled document-pair, namely: sentence- pair word-pair levels.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	propose three variants BiTAM model capture latent topics bilingual documents different levels.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	J 3.1 BiTAM1: Frameworks p(f |e) = n ) p(fj |ei ) · p(ei |e).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(1) j=1 i=1 1 follow notations (Brown et al., 1993) for.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	English-French, i.e., e ↔ f , although models tested,in paper, EnglishChinese.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	use end-user ter minology source target languages.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	first BiTAM model, assume topics sampled sentence-level.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	document- pair represented random mixture latent topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	topic, topic-k, presented topic-specific word-translation table: Bk , e e β e α θ z f J B N α θ z f J B α θ z N f J B N (a) (b) (c) Figure 1: BiTAM models Bilingual document- sentence-pairs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	node graph represents random variable, hexagon denotes parameter.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Un-shaded nodes hidden variables.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	plates represent replicates.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	outmost plate (M -plate) represents bilingual document-pairs, inner N -plate represents N repeated choice topics sentence-pairs document; inner J -plate represents J word-pairs within sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(a) BiTAM1 samples one topic (denoted z) per sentence-pair; (b) BiTAM2 utilizes sentence-level topics translation model (i.e., p(f |e, z)) monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), z indicator variable denote choice topic.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Given specific topic-weight vector θd document-pair, sentence-pair draws conditionally independent topics mixture topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	generative process, document-pair (Fd, Ed), summarized below: 1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Sample sentence-number N Poisson(γ)..	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	2.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Sample topic-weight vector θd Dirichlet(α)..	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	3.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	sentence-pair (fn , en ) dtth doc-pair ,.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(a) Sample sentence-length Jn Poisson(δ); (b) Sample topic zdn Multinomial(θd ); (c) Sample ej monolingual model p(ej );(d) Sample word alignment link aj uni form model p(aj ) (or HMM); (e) Sample fj according topic-specific graphical model representation BiTAM generative scheme discussed far.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Note that, sentence-pairs connected node θd. Therefore, marginally, sentence-pairs independent traditional SMT models, instead conditionally independent given topic-weight vector θd. Specifically, BiTAM1 assumes sentence-pair one single topic.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Thus, word-pairs within sentence-pair conditionally independent given hidden topic index z sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	last two sub-steps (3.d 3.e) BiTam sampling scheme define translation model, alignment link aj proposed translation lexicon p(fj |e, aj , zn , B).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	observation fj generated accordingWe assume that, model, K pos sible topics document-pair bear.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	document-pair, K -dimensional Dirichlet random variable θd, referred topic-weight vector document, take values (K −1)-simplex following probability density: proposed distributions.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	simplify alignment model a, IBM1, assuming aj sampled uniformly random.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Given parameters α, B, English part E, joint conditional distribution topic-weight vector θ, topic indicators z, alignment vectors A, document F written as: Γ( K αk ) p(θ|α) = k=1 θα1 −1 · · · θαK −1 , (3) p(F,A, θ, z|E, α, B) = k=1 Γ(αk ) N (4) hyperparameter α K -dimension vector component αk >0, Γ(x) Gamma function.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	alignment represented J -dimension vector = {a1, a2, · · · , aJ }; French word fj position j, position variable aj maps anEnglish word eaj position aj English sen p(θ | α) n p(zn |θ)p(fn , |en , α, Bzn), n=1 N number sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Marginalizing θ z, obtain marginal conditional probability generating F E document-pair: p(F, A|E, α, Bzn ) = tence.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	word level translation lexicon probabil- r ( (5) ities topic-specific, parameterized matrix B = {Bk }.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	p(θ|α) n) p(zn |θ)p(fn , |en , Bzn ) dθ, n=1 zn simplicity, current models omit modelings sentence-number N sentence-length Jn, focus bilingual translation model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Figure 1 (a) shows p(fn, an|en, Bzn ) topic-specific sentence-level translation model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	simplicity, assume French words fj ’s conditionally independent other; alignment variables aj ’s independent variables uniformly distributed priori.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Therefore, distribution sentence-pair is: p(fn , |en , Bzn) = p(fn |en , , Bzn)p(an |en , Bzn) Jn “Null” attached every target sentence align source words miss translations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Specifically, latent Dirichlet allocation (LDA) (Blei et al., 2003) viewed special case BiTAM3, target sentence 1 n p(f n n j=1 |eanj , Bzn ).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(6) contains one word: “Null”, alignment link longer hidden variable.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Thus, conditional likelihood entire parallel corpus given taking product marginal probabilities individual document-pair Eqn.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	3.2 BiTAM2: Monolingual Admixture.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	general, monolingual model English also rich topic-mixture.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	realized using topic-weight vector θd topic indicator zdn sampled according θd, described §3.1, introduce onlytopic-dependent translation lexicon, also topic dependent monolingual model source language, English case, generating sentence-pair (Figure 1 (b)).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	e generated	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Due hybrid nature BiTAM models, exact posterior inference hidden variables A, z θ intractable.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	variational inference used approximate true posteriors hidden variables.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	inference scheme presented BiTAM1; algorithms BiTAM2 BiTAM3 straight forward extensions omitted.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	4.1 Variational Approximation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	approximate: p(θ, z, A|E, F, α, B), joint posterior, use fully factorized distribution set hidden variables: q(θ,z, A) ∝ q(θ|γ, α)· topic-based language model β, instead N Jn (7) uniform distribution BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	refer n q(zn |φn ) n q(anj , fnj |ϕnj , en , B), model BiTAM2.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	n=1 j=1 Unlike BiTAM1, information observed ei indirectly passed z via node fj hidden variable aj , BiTAM2, topics corresponding English French sentences also strictly aligned information observed ei directly passed z, hope finding accurate topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	topics inferred directly observed bilingual data, result, improve alignment.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	3.3 BiTAM3: Word-level Admixture.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Dirichlet parameter γ, multinomial parameters (φ1, · · · , φn), parameters (ϕn1, · · · , ϕnJn ) known variational param eters, optimized respect KullbackLeibler divergence q(·) original p(·) via iterative fixed-point algorithm.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	shown fixed-point equations variational parameters BiTAM1 follows: Nd γk = αk + ) φdnk (8) n=1 K straightforward extend sentence-level BiTAM1 word-level admixture model, φdnk ∝ exp (Ψ(γk ) − Ψ( Jdn Idn ) kt =1 γkt ) · sampling topic indicator zn,j word-pair (fj , eaj ) ntth sentence-pair, rather (words) sentence (Figure 1 (c)).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	exp ( ) ) ϕdnji log Bf ,e ,k (9) j j=1 i=1 K ( gives rise BiTAM3.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	conditional ϕdnji ∝ exp ) φdnk log Bf ,e ,k , (10) k=1 likelihood functions obtained extending Ψ(·) digamma function.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Note inthe formulas §3.1 move variable zn,j side loop fn,j . formulas φ dnkis variational param 3.4 Incorporation Word “Null”.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Similar IBM models, “Null” word used source words translation counterparts target language.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	example, Chinese words “de” (ffl) , “ba” (I\) “bei” (%i) generally translations English.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	eter underlying topic indicator zdn nth sentence-pair document d, used predict topic distribution sentence-pair.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Following variational EM scheme (Beal Ghahramani, 2002), estimate model parameters α B unsupervised fashion.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Essentially, Eqs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(810) constitute E-step, posterior estimations latent variables obtained.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	M-step, update α B improve lower bound log-likelihood defined bellow: L(γ, φ, ϕ; α, B) = Eq [log p(θ|α)]+Eq [log p(z|θ)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]−Eq [log q(θ)] −Eq [log q(z)]−Eq [log q(a)].	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(11) close-form iterative updating formula B is: BDA selects iteratively, f , best aligned e, word-pair (f, e) maximum row column, neighbors aligned pairs combpeting candidates.A close check {ϕdnji} Eqn.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	10 veals essentially exponential model: weighted log probabilities individual topic- specific translation lexicons; viewed weighted geometric mean individual lex Nd Jdn Idn Bf,e,k ∝ ) ) ) ) δ(f, fj )δ(e, ei )φdnk ϕdnji (12) n=1 j=1 i=1 α, close-form update available, resort gradient accent (Sjo¨ lander et al., 1996) restarts ensure updated αk >0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	4.2 Data Sparseness Smoothing.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation lexicons Bf,e,k potential size V 2K , assuming vocabulary sizes languages V . data sparsity (i.e., lack large volume document-pairs) poses serious problem estimating Bf,e,k monolingual case, instance, (Blei et al., 2003).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	reduce data sparsity problem, introduce two remedies models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	First: Laplace smoothing.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	approach, matrix set B, whose columns correspond parameters conditional multinomial distributions, treated collection random vectors symmetric Dirichlet prior; posterior expectation multinomial parameter vectors estimated using Bayesian theory.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Second: interpolation smoothing.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Empirically, employ linear interpolation IBM1 avoid overfitting: Bf,e,k = λBf,e,k +(1−λ)p(f |e).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(13) Eqn.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	1, p(f |e) learned via IBM1; λ estimated via EM held data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	4.3 Retrieving Word Alignments.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Two word-alignment retrieval schemes designed BiTAMs: uni-direction alignment (UDA) bi-direction alignment (BDA).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	use posterior mean alignment indicators adnji, captured call poste rior alignment matrix ϕ ≡ {ϕdnji}.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	UDA uses French word fdnj (at jtth position ntth sentence dtth document) query ϕ get best aligned English word (by taking maximum point row ϕ): adnj = arg max ϕdnji .	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(14) i∈[1,Idn ] icon’s strength.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	evaluate BiTAM models word alignment accuracy translation quality.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	word alignment accuracy, F-measure reported, i.e., harmonic mean precision recall gold-standard reference set; translation quality, Bleu (Papineni et al., 2002) variation NIST scores reported.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Table 1: Training Test Data Statistics Tra #D oc.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	#S ent . #T ok en En gli sh Ch ine se Tr ee b n k F B . B J Si n Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes 95 62 7 25, 50 0 19, 72 6 two training data settings different sizes (see Table 1).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	small one consists 316 document-pairs Tree- bank (LDC2002E17).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	large training data setting, collected additional document- pairs FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), Xinhua News (LDC2002E18, document boundaries kept sentence-aligner (Zhao Vogel, 2002)).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	27,940 document-pairs, containing 327K sentence-pairs 12 million (12M) English tokens 11M Chinese tokens.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	evaluate word alignment, hand-labeled 627 sentence-pairs 95 document-pairs sampled TIDES’01 dryrun data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	contains 14,769 alignment-links.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	evaluate translation quality, TIDES’02 Eval.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	test used development set, TIDES’03 Eval.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	test used unseen test data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.1 Model Settings.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	First, explore effects Null word smoothing strategies.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Empirically, find adding “Null” word always beneficial models regardless number topics selected.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	pics Le xic ons pic1 pic2 pic3 Co oc.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	IBM 1 H IBM 4 p( Ch ao Xi (Ji!	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	$) |K ore an) 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	06 12 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	21 38 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	� )|K ore an) 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	83 79 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	61 16 0.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons learned 3-topic BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	third lexicon (Topic-3) prefers translate word Korean ChaoXian (Ji!$:North Korean).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	co-occurrence (Cooc), IBM1&4 HMM prefer translate HanGuo (li!�:South Korean).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	two candidate translations may fade learned translation lexicons.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h n u . . dev elop men trad e ente rpri ses tech nolo gy cou ntri es e r eco nom ic Topi c B. cho ngqi ng com pani es take co pa ny cit bi lli n r e eco nom ic c h e u n Topi c C. sp ts dis abl ed te p e p l e caus e w e r na tio na l ga es han dica ppe mb ers Table 3: Three distinctive topics displayed.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	English words topic ranked according p(e|z) estimated topic-specific English sentences weighted {φdnk }.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	33 functional words removed highlight main content topic.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Topic Us-China economic relationships; Topic B relates Chinese companies’ merging; Topic C shows sports handicapped people.The interpolation smoothing §4.2 effec tive, gives slightly better performance Laplace smoothing different number topics BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	However, interpolation leverages competing baseline lexicon, blur evaluations BiTAM’s contributions.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Laplace smoothing chosen emphasize BiTAM’s strength.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Without smoothing, F- measure drops quickly two topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	following experiments, use Null word Laplace smoothing BiTAM models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	train, comparison, IBM1&4 HMM models 8 iterations IBM1, 7 HMM 3 IBM4 (18h743) Null word maximum fertility 3 ChineseEnglish.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Choosing number topics model selection problem.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	performed tenfold cross- validation, setting three-topic chosen small large training data sets.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	overall computation complexity BiTAM linear number hidden topics.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.2 Variational Inference.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	non-symmetric Dirichlet prior, hyperparameter α initialized randomly; B (K translation lexicons) initialized uniformly IBM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Better initialization B help avoid local optimal shown § 5.5.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	learned B α fixed, variational parameters computed Eqn.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	(810) initialized randomly; fixed-point iterative updates stop change likelihood smaller 10−5.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	convergent variational parameters, corresponding highest likelihood 20 random restarts, used retrieving word alignment unseen document-pairs.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	estimate B, β (for BiTAM2) α, eight variational EM iterations run training data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Figure 2 shows absolute 2∼3% better F-measure iterations variational EM using two three topics BiTAM1 comparing IBM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTam Null Laplace Smoothing Var.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	EM Iterations 41 40 39 38 37 36 35 BiTam−1, Topic #=3 34 BiTam−1, Topic #=2.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	IB −1 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number EM/Variational EM Iterations IBM−1 BiTam−1 Figure 2: performances eight Variational EM iterations BiTAM1 using “Null” word laplace smoothing; IBM1 shown eight EM iterations comparison.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.3 Topic-Specific Translation.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Lexicons topic-specific lexicons Bk smaller size IBM1, and, typically, contain topic trends.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	example, training data, North Korean usually related politics translated “ChaoXian” (Ji!	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	$); South Korean occurs often economics translated “HanGuo”(li!	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	�).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTAMs discriminate two considering topics context.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Table 2 shows lexicon entries “Korean” learned 3-topic BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	values relatively sharper, clearly favors one candidates.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	co-occurrence count, however, favors “HanGuo”, easily dominate decisions IBM HMM models due ignorance topical context.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Monolingual topics learned BiTAMs are, roughly speaking, fuzzy especially number topics small.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	proper filtering, find BiTAMs capture topics illustrated Table 3.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.4 Evaluating Word.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Alignments evaluate word alignment accuracies various settings.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Notably, BiTAM allows test alignments two directions: English-to Chinese (EC) Chinese-to-English (CE).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Additional heuristics applied improve accuracies.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Inter takes intersection two directions generates high-precision alignments; SE TI N G IBM 1 H IBM 4 B 1 U BDA B 2 U BDA B 3 U BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E ( % ) U N N ( % ) TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N B L E U 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	45 8 15 .7 0 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	82 2 17 .7 0 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	92 6 18 .2 5 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	93 7 6.954 17 .93 18.14 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	90 4 6.976 18 .13 18.05 6.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) Machine Translation Quality BiTAM Models, comparing IBM Models, HMMs training scheme 18 h7 43 Treebank data listed Table 1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	column, highlighted alignment (the best one model setting) picked evaluate translation quality.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Union two directions gives high-recall; Refined grows intersection neighboring word- pairs seen union, yields high-precision high-recall alignments.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	shown Table 4, baseline IBM1 gives best performance 36.27% CE direc tion; UDA alignments BiTAM1∼3 give 40.13%, 40.26%, 40.47%, respectively, significantly better IBM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	close look three BiTAMs yield significant difference.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BiTAM3 slightly better settings; BiTAM1 slightly worse two, topics sampled sentence level concentrated.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BDA align ments BiTAM1∼3 yield 48.26%, 48.63% 49.02%, even better HMM IBM4 — best performances 44.26% 45.96%, respectively.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	BDA partially utilizes similar heuristics approximated posterior matrix {ϕdnji} instead di rect operations alignments two directions heuristics Refined.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Practically, also apply BDA together heuristics IBM1, HMM IBM4, best achieved performances 40.56%, 46.52% 49.18%, respectively.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Overall, BiTAM models achieve performances close higher HMM, using simple IBM1 style alignment model.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Similar improvements IBM models HMM preserved applying three kinds heuristics above.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	expected, since BDA already encodes heuristics, slightly improved Union heuristic; UDA, similar viterbi style alignment IBM HMM, improved better Refined heuristic.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	also test BiTAM3 large training data, similar improvements observed baseline models (see Table.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.5 Boosting BiTAM Models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	translation lexicons Bf,e,k initialized uniformly previous experiments.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Better ini tializations potentially lead better performances help avoid undesirable local optima variational EM iterations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	use lexicons IBM Model-4 initialize Bf,e,k boost BiTAM models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	one way applying proposed BiTAM models current state-of-the-art SMT systems improvement.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	boosted alignments denoted BUDA BBDA Table.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5, corresponding uni-direction bi-direction alignments, respectively.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	see improvement alignment quality.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5.6 Evaluating Translations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	evaluate BiTAM models, word alignments used phrase-based decoder evaluating translation qualities.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Similar Pharoah package (Koehn, 2004), extract phrase-pairs directly word alignment together coherence constraints (Fox, 2002) remove noisy ones.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	use TIDES Eval’02 CE test set development data tune decoder parameters; Eval’03 data (919 sentences) unseen data.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	trigram language model built using 180 million English words.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Across reported comparative settings, key difference bilingual ngram-identity phrase-pair, collected directly underlying word alignment.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Shown Table 4 results small- data track; large-data track results Table 5.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	small-data track, baseline Bleu scores IBM1, HMM IBM4 15.70, 17.70 18.25, respectively.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	UDA alignment BiTAM1 gives improvement baseline IBM1 15.70 17.93, close HMM’s performance, even though BiTAM doesn’t exploit sequential structures words.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	proposed BiTAM2 BiTAM 3 slightly better BiTAM1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Similar improvements observed large-data track (see Table 5).	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Note that, boosted BiTAM3 us SE TI N G IBM 1 H IBM 4 B 3 U BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E ( % ) U N N ( % ) N E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N B L E U 7.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	5 9 19 .1 9 7.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	7 7 21 .9 9 7.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	8 3 23 .1 8 7.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies Machine Translation Qualities BiTAM Models, IBM Models, HMMs, boosted BiTAMs using training data listed Table.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	1.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	experimental conditions similar Table.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	4.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	ing IBM4 seed lexicon, outperform Refined IBM4: 23.18 24.07 Bleu score, 7.83 8.23 NIST.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	result suggests straightforward way leverage BiTAMs improve statistical machine translations.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	paper, proposed novel formalism statistical word alignment based bilingual admixture (BiTAM) models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Three BiTAM models proposed evaluated word alignment translation qualities state-of- the-art translation models.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	proposed models significantly improve alignment accuracy lead better translation qualities.	0
possible solution implementation interpolation techniques smooth sharp distributions estimated events (Och Ney, 2003; Zhao Xing, 2006).	Incorporation within-sentence dependencies alignment-jumps distortions, better treatment source monolingual model worth investigations.	0
