Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Using Random Walks Question-focused Sentence Retrieval	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Annotators generated list questions central understanding story corpus.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	experiments, themethod achieves TRDR score significantly higher baseline.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Understanding storiesis challenging number reasons.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	may choose receiveeither precise answer question-focused summary.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Currently, address question-focused sentence retrieval task.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	passage retrieval (PR) isclearly new problem (e.g.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	(7:6) Figure 1: Question tracking interface summarization system.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	sized it.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	end, propose use stochastic, graph-based method.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	1
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	system rank input documents, restricted terms number ofsentences may selected document.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Alternatively, sentences canbe returned user question-focused summary.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	similar snippet retrieval (Wu etal., 2004).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	3.1 LexRank method.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	apply LexRank, similarity graph producedfor sentences input document set.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	thegraph, node represents sentence.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	degree given node isan indication much information respective sentence common sentences.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	previously mentioned, original LexRank method performed wellin context generic summarization.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	3.2 Relevance question.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	also stem question remove stop words it.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	3.3 mixture model.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	1 0.03614457831325301 least two people dead, inclu...	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	0 0.28454242157110576 Officials said plane carryin...	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	2 0.1973852892722677 Italian police said plane car..	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	3 0.28454242157110576 Rescue officials said least th...	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	equation determined empirically.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	denominatorsin terms normalization, described below.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	matrix called stochasticand defines Markov chain.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Every transition weighted accordingto similarity distributions.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	also model used torank sentences (Erkan Radev, 2004).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	3.4 Experiments topic-sensitive LexRank.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	experimented different values ourtraining data.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	thenfound configurations outperformed baseline.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	configurations applied ourdevelopment/test set.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	remainder thepaper explain process results detail.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	4 Experimental setup.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	4.1 Corpus.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	clusters characteristics shown Table 1.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	news articles collected varioussources.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Newstracker clusters collected automatically Web-based news summarization system.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	generated list factualquestions key understanding story.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	4.2 Evaluation metrics methods.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	One developing PRsystem, output input ananswer extraction system processing.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Table 1: Corpus complex news stories.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	sentence numbers top 20 sentences.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	measuregives us idea far must look theranked list order find correct answer.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	contrast, TRDR total reciprocal ranks allanswers found system.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	measure many relevant sentenceswere identified system.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	However, reportboth average MRR TRDR questionsin given data set.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	optimal range parameter 0.14 and0.20.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	ically diverse (e.g. paraphrases).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	clear ahigh question bias needed.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	5.1 Development/testing phase.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Table 5.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	system versus baseline unseen test set of134 questions.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	5.2 Testing phase.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	improvement average TRDR score statistically significant p-value 0.0619.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	last two sentences provide answers according judges, provide context information situation.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
Considering Question Topic: first introduce incorporate question topic Markov Random Walk model, similar Topic-sensitive LexRank (Otterbacher et al., 2005).	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Using Random Walks Question-focused Sentence Retrieval	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Annotators generated list questions central understanding story corpus.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	experiments, themethod achieves TRDR score significantly higher baseline.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Understanding storiesis challenging number reasons.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	may choose receiveeither precise answer question-focused summary.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Currently, address question-focused sentence retrieval task.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	passage retrieval (PR) isclearly new problem (e.g.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	(7:6) Figure 1: Question tracking interface summarization system.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	sized it.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	end, propose use stochastic, graph-based method.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	system rank input documents, restricted terms number ofsentences may selected document.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Alternatively, sentences canbe returned user question-focused summary.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	similar snippet retrieval (Wu etal., 2004).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	3.1 LexRank method.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	apply LexRank, similarity graph producedfor sentences input document set.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	thegraph, node represents sentence.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	degree given node isan indication much information respective sentence common sentences.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	previously mentioned, original LexRank method performed wellin context generic summarization.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	3.2 Relevance question.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	also stem question remove stop words it.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	3.3 mixture model.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	1 0.03614457831325301 least two people dead, inclu...	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	0 0.28454242157110576 Officials said plane carryin...	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	2 0.1973852892722677 Italian police said plane car..	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	3 0.28454242157110576 Rescue officials said least th...	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	equation determined empirically.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	denominatorsin terms normalization, described below.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	matrix called stochasticand defines Markov chain.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Every transition weighted accordingto similarity distributions.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	also model used torank sentences (Erkan Radev, 2004).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	3.4 Experiments topic-sensitive LexRank.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	experimented different values ourtraining data.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	thenfound configurations outperformed baseline.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	configurations applied ourdevelopment/test set.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	remainder thepaper explain process results detail.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	4 Experimental setup.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	4.1 Corpus.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	clusters characteristics shown Table 1.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	news articles collected varioussources.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Newstracker clusters collected automatically Web-based news summarization system.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	generated list factualquestions key understanding story.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	4.2 Evaluation metrics methods.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	One developing PRsystem, output input ananswer extraction system processing.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Table 1: Corpus complex news stories.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	sentence numbers top 20 sentences.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	measuregives us idea far must look theranked list order find correct answer.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	contrast, TRDR total reciprocal ranks allanswers found system.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	measure many relevant sentenceswere identified system.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	However, reportboth average MRR TRDR questionsin given data set.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	optimal range parameter 0.14 and0.20.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	ically diverse (e.g. paraphrases).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	clear ahigh question bias needed.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	5.1 Development/testing phase.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Table 5.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	system versus baseline unseen test set of134 questions.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	5.2 Testing phase.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	improvement average TRDR score statistically significant p-value 0.0619.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	last two sentences provide answers according judges, provide context information situation.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
weight twij calculated tf · idf (Otterbacher et al., 2005).	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Using Random Walks Question-focused Sentence Retrieval	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Annotators generated list questions central understanding story corpus.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	experiments, themethod achieves TRDR score significantly higher baseline.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Understanding storiesis challenging number reasons.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	may choose receiveeither precise answer question-focused summary.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Currently, address question-focused sentence retrieval task.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	passage retrieval (PR) isclearly new problem (e.g.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	(7:6) Figure 1: Question tracking interface summarization system.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	sized it.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	end, propose use stochastic, graph-based method.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	1
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	system rank input documents, restricted terms number ofsentences may selected document.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Alternatively, sentences canbe returned user question-focused summary.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	similar snippet retrieval (Wu etal., 2004).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	3.1 LexRank method.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	apply LexRank, similarity graph producedfor sentences input document set.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	thegraph, node represents sentence.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	degree given node isan indication much information respective sentence common sentences.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	previously mentioned, original LexRank method performed wellin context generic summarization.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	3.2 Relevance question.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	also stem question remove stop words it.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	3.3 mixture model.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	1 0.03614457831325301 least two people dead, inclu...	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	0 0.28454242157110576 Officials said plane carryin...	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	2 0.1973852892722677 Italian police said plane car..	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	3 0.28454242157110576 Rescue officials said least th...	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	equation determined empirically.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	denominatorsin terms normalization, described below.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	matrix called stochasticand defines Markov chain.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Every transition weighted accordingto similarity distributions.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	also model used torank sentences (Erkan Radev, 2004).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	3.4 Experiments topic-sensitive LexRank.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	experimented different values ourtraining data.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	thenfound configurations outperformed baseline.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	configurations applied ourdevelopment/test set.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	remainder thepaper explain process results detail.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	4 Experimental setup.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	4.1 Corpus.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	clusters characteristics shown Table 1.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	news articles collected varioussources.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Newstracker clusters collected automatically Web-based news summarization system.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	generated list factualquestions key understanding story.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	4.2 Evaluation metrics methods.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	One developing PRsystem, output input ananswer extraction system processing.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Table 1: Corpus complex news stories.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	sentence numbers top 20 sentences.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	measuregives us idea far must look theranked list order find correct answer.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	contrast, TRDR total reciprocal ranks allanswers found system.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	measure many relevant sentenceswere identified system.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	However, reportboth average MRR TRDR questionsin given data set.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	optimal range parameter 0.14 and0.20.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	ically diverse (e.g. paraphrases).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	clear ahigh question bias needed.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	5.1 Development/testing phase.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Table 5.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	system versus baseline unseen test set of134 questions.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	5.2 Testing phase.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	improvement average TRDR score statistically significant p-value 0.0619.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	last two sentences provide answers according judges, provide context information situation.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
includingfact-based QA text summarization (Erkan andRadev, 2004; Mihalcea Tarau, 2004; Otter-bacher et al., 2005; Wan Yang, 2008).	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Using Random Walks Question-focused Sentence Retrieval	0
sentence retrieval question answering (Otterbacher et al., 2005).	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Annotators generated list questions central understanding story corpus.	0
sentence retrieval question answering (Otterbacher et al., 2005).	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
sentence retrieval question answering (Otterbacher et al., 2005).	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
sentence retrieval question answering (Otterbacher et al., 2005).	experiments, themethod achieves TRDR score significantly higher baseline.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
sentence retrieval question answering (Otterbacher et al., 2005).	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
sentence retrieval question answering (Otterbacher et al., 2005).	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Understanding storiesis challenging number reasons.	0
sentence retrieval question answering (Otterbacher et al., 2005).	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
sentence retrieval question answering (Otterbacher et al., 2005).	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
sentence retrieval question answering (Otterbacher et al., 2005).	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
sentence retrieval question answering (Otterbacher et al., 2005).	may choose receiveeither precise answer question-focused summary.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Currently, address question-focused sentence retrieval task.	0
sentence retrieval question answering (Otterbacher et al., 2005).	passage retrieval (PR) isclearly new problem (e.g.	0
sentence retrieval question answering (Otterbacher et al., 2005).	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
sentence retrieval question answering (Otterbacher et al., 2005).	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
sentence retrieval question answering (Otterbacher et al., 2005).	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
sentence retrieval question answering (Otterbacher et al., 2005).	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
sentence retrieval question answering (Otterbacher et al., 2005).	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
sentence retrieval question answering (Otterbacher et al., 2005).	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
sentence retrieval question answering (Otterbacher et al., 2005).	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
sentence retrieval question answering (Otterbacher et al., 2005).	(7:6) Figure 1: Question tracking interface summarization system.	0
sentence retrieval question answering (Otterbacher et al., 2005).	sized it.	0
sentence retrieval question answering (Otterbacher et al., 2005).	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
sentence retrieval question answering (Otterbacher et al., 2005).	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
sentence retrieval question answering (Otterbacher et al., 2005).	end, propose use stochastic, graph-based method.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
sentence retrieval question answering (Otterbacher et al., 2005).	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
sentence retrieval question answering (Otterbacher et al., 2005).	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
sentence retrieval question answering (Otterbacher et al., 2005).	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
sentence retrieval question answering (Otterbacher et al., 2005).	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	1
sentence retrieval question answering (Otterbacher et al., 2005).	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
sentence retrieval question answering (Otterbacher et al., 2005).	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
sentence retrieval question answering (Otterbacher et al., 2005).	system rank input documents, restricted terms number ofsentences may selected document.	0
sentence retrieval question answering (Otterbacher et al., 2005).	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Alternatively, sentences canbe returned user question-focused summary.	0
sentence retrieval question answering (Otterbacher et al., 2005).	similar snippet retrieval (Wu etal., 2004).	0
sentence retrieval question answering (Otterbacher et al., 2005).	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
sentence retrieval question answering (Otterbacher et al., 2005).	3.1 LexRank method.	0
sentence retrieval question answering (Otterbacher et al., 2005).	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
sentence retrieval question answering (Otterbacher et al., 2005).	apply LexRank, similarity graph producedfor sentences input document set.	0
sentence retrieval question answering (Otterbacher et al., 2005).	thegraph, node represents sentence.	0
sentence retrieval question answering (Otterbacher et al., 2005).	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
sentence retrieval question answering (Otterbacher et al., 2005).	degree given node isan indication much information respective sentence common sentences.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
sentence retrieval question answering (Otterbacher et al., 2005).	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
sentence retrieval question answering (Otterbacher et al., 2005).	previously mentioned, original LexRank method performed wellin context generic summarization.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
sentence retrieval question answering (Otterbacher et al., 2005).	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
sentence retrieval question answering (Otterbacher et al., 2005).	3.2 Relevance question.	0
sentence retrieval question answering (Otterbacher et al., 2005).	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
sentence retrieval question answering (Otterbacher et al., 2005).	also stem question remove stop words it.	0
sentence retrieval question answering (Otterbacher et al., 2005).	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
sentence retrieval question answering (Otterbacher et al., 2005).	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
sentence retrieval question answering (Otterbacher et al., 2005).	3.3 mixture model.	0
sentence retrieval question answering (Otterbacher et al., 2005).	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
sentence retrieval question answering (Otterbacher et al., 2005).	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
sentence retrieval question answering (Otterbacher et al., 2005).	1 0.03614457831325301 least two people dead, inclu...	0
sentence retrieval question answering (Otterbacher et al., 2005).	0 0.28454242157110576 Officials said plane carryin...	0
sentence retrieval question answering (Otterbacher et al., 2005).	2 0.1973852892722677 Italian police said plane car..	0
sentence retrieval question answering (Otterbacher et al., 2005).	3 0.28454242157110576 Rescue officials said least th...	0
sentence retrieval question answering (Otterbacher et al., 2005).	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
sentence retrieval question answering (Otterbacher et al., 2005).	equation determined empirically.	0
sentence retrieval question answering (Otterbacher et al., 2005).	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
sentence retrieval question answering (Otterbacher et al., 2005).	denominatorsin terms normalization, described below.	0
sentence retrieval question answering (Otterbacher et al., 2005).	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
sentence retrieval question answering (Otterbacher et al., 2005).	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
sentence retrieval question answering (Otterbacher et al., 2005).	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
sentence retrieval question answering (Otterbacher et al., 2005).	matrix called stochasticand defines Markov chain.	0
sentence retrieval question answering (Otterbacher et al., 2005).	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
sentence retrieval question answering (Otterbacher et al., 2005).	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
sentence retrieval question answering (Otterbacher et al., 2005).	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
sentence retrieval question answering (Otterbacher et al., 2005).	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Every transition weighted accordingto similarity distributions.	0
sentence retrieval question answering (Otterbacher et al., 2005).	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
sentence retrieval question answering (Otterbacher et al., 2005).	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
sentence retrieval question answering (Otterbacher et al., 2005).	also model used torank sentences (Erkan Radev, 2004).	0
sentence retrieval question answering (Otterbacher et al., 2005).	3.4 Experiments topic-sensitive LexRank.	0
sentence retrieval question answering (Otterbacher et al., 2005).	experimented different values ourtraining data.	0
sentence retrieval question answering (Otterbacher et al., 2005).	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
sentence retrieval question answering (Otterbacher et al., 2005).	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
sentence retrieval question answering (Otterbacher et al., 2005).	thenfound configurations outperformed baseline.	0
sentence retrieval question answering (Otterbacher et al., 2005).	configurations applied ourdevelopment/test set.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
sentence retrieval question answering (Otterbacher et al., 2005).	remainder thepaper explain process results detail.	0
sentence retrieval question answering (Otterbacher et al., 2005).	4 Experimental setup.	0
sentence retrieval question answering (Otterbacher et al., 2005).	4.1 Corpus.	0
sentence retrieval question answering (Otterbacher et al., 2005).	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
sentence retrieval question answering (Otterbacher et al., 2005).	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
sentence retrieval question answering (Otterbacher et al., 2005).	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
sentence retrieval question answering (Otterbacher et al., 2005).	clusters characteristics shown Table 1.	0
sentence retrieval question answering (Otterbacher et al., 2005).	news articles collected varioussources.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Newstracker clusters collected automatically Web-based news summarization system.	0
sentence retrieval question answering (Otterbacher et al., 2005).	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
sentence retrieval question answering (Otterbacher et al., 2005).	generated list factualquestions key understanding story.	0
sentence retrieval question answering (Otterbacher et al., 2005).	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
sentence retrieval question answering (Otterbacher et al., 2005).	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
sentence retrieval question answering (Otterbacher et al., 2005).	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
sentence retrieval question answering (Otterbacher et al., 2005).	4.2 Evaluation metrics methods.	0
sentence retrieval question answering (Otterbacher et al., 2005).	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
sentence retrieval question answering (Otterbacher et al., 2005).	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
sentence retrieval question answering (Otterbacher et al., 2005).	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
sentence retrieval question answering (Otterbacher et al., 2005).	One developing PRsystem, output input ananswer extraction system processing.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
sentence retrieval question answering (Otterbacher et al., 2005).	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
sentence retrieval question answering (Otterbacher et al., 2005).	Table 1: Corpus complex news stories.	0
sentence retrieval question answering (Otterbacher et al., 2005).	sentence numbers top 20 sentences.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
sentence retrieval question answering (Otterbacher et al., 2005).	measuregives us idea far must look theranked list order find correct answer.	0
sentence retrieval question answering (Otterbacher et al., 2005).	contrast, TRDR total reciprocal ranks allanswers found system.	0
sentence retrieval question answering (Otterbacher et al., 2005).	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
sentence retrieval question answering (Otterbacher et al., 2005).	measure many relevant sentenceswere identified system.	0
sentence retrieval question answering (Otterbacher et al., 2005).	However, reportboth average MRR TRDR questionsin given data set.	0
sentence retrieval question answering (Otterbacher et al., 2005).	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
sentence retrieval question answering (Otterbacher et al., 2005).	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
sentence retrieval question answering (Otterbacher et al., 2005).	optimal range parameter 0.14 and0.20.	0
sentence retrieval question answering (Otterbacher et al., 2005).	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
sentence retrieval question answering (Otterbacher et al., 2005).	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
sentence retrieval question answering (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
sentence retrieval question answering (Otterbacher et al., 2005).	ically diverse (e.g. paraphrases).	0
sentence retrieval question answering (Otterbacher et al., 2005).	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
sentence retrieval question answering (Otterbacher et al., 2005).	clear ahigh question bias needed.	0
sentence retrieval question answering (Otterbacher et al., 2005).	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
sentence retrieval question answering (Otterbacher et al., 2005).	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
sentence retrieval question answering (Otterbacher et al., 2005).	5.1 Development/testing phase.	0
sentence retrieval question answering (Otterbacher et al., 2005).	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
sentence retrieval question answering (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
sentence retrieval question answering (Otterbacher et al., 2005).	Table 5.	0
sentence retrieval question answering (Otterbacher et al., 2005).	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
sentence retrieval question answering (Otterbacher et al., 2005).	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
sentence retrieval question answering (Otterbacher et al., 2005).	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
sentence retrieval question answering (Otterbacher et al., 2005).	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
sentence retrieval question answering (Otterbacher et al., 2005).	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
sentence retrieval question answering (Otterbacher et al., 2005).	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
sentence retrieval question answering (Otterbacher et al., 2005).	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
sentence retrieval question answering (Otterbacher et al., 2005).	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
sentence retrieval question answering (Otterbacher et al., 2005).	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
sentence retrieval question answering (Otterbacher et al., 2005).	system versus baseline unseen test set of134 questions.	0
sentence retrieval question answering (Otterbacher et al., 2005).	5.2 Testing phase.	0
sentence retrieval question answering (Otterbacher et al., 2005).	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
sentence retrieval question answering (Otterbacher et al., 2005).	improvement average TRDR score statistically significant p-value 0.0619.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
sentence retrieval question answering (Otterbacher et al., 2005).	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
sentence retrieval question answering (Otterbacher et al., 2005).	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
sentence retrieval question answering (Otterbacher et al., 2005).	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
sentence retrieval question answering (Otterbacher et al., 2005).	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
sentence retrieval question answering (Otterbacher et al., 2005).	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
sentence retrieval question answering (Otterbacher et al., 2005).	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
sentence retrieval question answering (Otterbacher et al., 2005).	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
sentence retrieval question answering (Otterbacher et al., 2005).	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
sentence retrieval question answering (Otterbacher et al., 2005).	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
sentence retrieval question answering (Otterbacher et al., 2005).	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
sentence retrieval question answering (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
sentence retrieval question answering (Otterbacher et al., 2005).	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
sentence retrieval question answering (Otterbacher et al., 2005).	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
sentence retrieval question answering (Otterbacher et al., 2005).	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
sentence retrieval question answering (Otterbacher et al., 2005).	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
sentence retrieval question answering (Otterbacher et al., 2005).	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
sentence retrieval question answering (Otterbacher et al., 2005).	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
sentence retrieval question answering (Otterbacher et al., 2005).	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
sentence retrieval question answering (Otterbacher et al., 2005).	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
sentence retrieval question answering (Otterbacher et al., 2005).	last two sentences provide answers according judges, provide context information situation.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
sentence retrieval question answering (Otterbacher et al., 2005).	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
sentence retrieval question answering (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
sentence retrieval question answering (Otterbacher et al., 2005).	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
sentence retrieval question answering (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
sentence retrieval question answering (Otterbacher et al., 2005).	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
sentence retrieval question answering (Otterbacher et al., 2005).	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
sentence retrieval question answering (Otterbacher et al., 2005).	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
sentence retrieval question answering (Otterbacher et al., 2005).	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
sentence retrieval question answering (Otterbacher et al., 2005).	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
sentence retrieval question answering (Otterbacher et al., 2005).	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
sentence retrieval question answering (Otterbacher et al., 2005).	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Using Random Walks Question-focused Sentence Retrieval	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Annotators generated list questions central understanding story corpus.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	experiments, themethod achieves TRDR score significantly higher baseline.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Understanding storiesis challenging number reasons.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	may choose receiveeither precise answer question-focused summary.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Currently, address question-focused sentence retrieval task.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	passage retrieval (PR) isclearly new problem (e.g.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	(7:6) Figure 1: Question tracking interface summarization system.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	sized it.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	end, propose use stochastic, graph-based method.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	1
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	system rank input documents, restricted terms number ofsentences may selected document.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Alternatively, sentences canbe returned user question-focused summary.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	similar snippet retrieval (Wu etal., 2004).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	3.1 LexRank method.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	apply LexRank, similarity graph producedfor sentences input document set.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	thegraph, node represents sentence.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	degree given node isan indication much information respective sentence common sentences.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	previously mentioned, original LexRank method performed wellin context generic summarization.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	3.2 Relevance question.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	also stem question remove stop words it.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	3.3 mixture model.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	1 0.03614457831325301 least two people dead, inclu...	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	0 0.28454242157110576 Officials said plane carryin...	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	2 0.1973852892722677 Italian police said plane car..	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	3 0.28454242157110576 Rescue officials said least th...	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	equation determined empirically.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	denominatorsin terms normalization, described below.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	matrix called stochasticand defines Markov chain.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Every transition weighted accordingto similarity distributions.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	also model used torank sentences (Erkan Radev, 2004).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	3.4 Experiments topic-sensitive LexRank.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	experimented different values ourtraining data.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	thenfound configurations outperformed baseline.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	configurations applied ourdevelopment/test set.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	remainder thepaper explain process results detail.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	4 Experimental setup.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	4.1 Corpus.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	clusters characteristics shown Table 1.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	news articles collected varioussources.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Newstracker clusters collected automatically Web-based news summarization system.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	generated list factualquestions key understanding story.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	4.2 Evaluation metrics methods.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	One developing PRsystem, output input ananswer extraction system processing.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Table 1: Corpus complex news stories.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	sentence numbers top 20 sentences.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	measuregives us idea far must look theranked list order find correct answer.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	contrast, TRDR total reciprocal ranks allanswers found system.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	measure many relevant sentenceswere identified system.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	However, reportboth average MRR TRDR questionsin given data set.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	optimal range parameter 0.14 and0.20.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	ically diverse (e.g. paraphrases).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	clear ahigh question bias needed.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	5.1 Development/testing phase.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Table 5.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	system versus baseline unseen test set of134 questions.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	5.2 Testing phase.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	improvement average TRDR score statistically significant p-value 0.0619.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	last two sentences provide answers according judges, provide context information situation.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
algorithms based query-sensitive LexRank (OtterBacher et al., 2005).	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Using Random Walks Question-focused Sentence Retrieval	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Annotators generated list questions central understanding story corpus.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	experiments, themethod achieves TRDR score significantly higher baseline.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Understanding storiesis challenging number reasons.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	may choose receiveeither precise answer question-focused summary.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Currently, address question-focused sentence retrieval task.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	passage retrieval (PR) isclearly new problem (e.g.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	(7:6) Figure 1: Question tracking interface summarization system.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	sized it.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	end, propose use stochastic, graph-based method.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	system rank input documents, restricted terms number ofsentences may selected document.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Alternatively, sentences canbe returned user question-focused summary.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	similar snippet retrieval (Wu etal., 2004).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	3.1 LexRank method.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	apply LexRank, similarity graph producedfor sentences input document set.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	thegraph, node represents sentence.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	degree given node isan indication much information respective sentence common sentences.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	previously mentioned, original LexRank method performed wellin context generic summarization.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	3.2 Relevance question.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	1
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	also stem question remove stop words it.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	3.3 mixture model.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	1 0.03614457831325301 least two people dead, inclu...	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	0 0.28454242157110576 Officials said plane carryin...	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	2 0.1973852892722677 Italian police said plane car..	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	3 0.28454242157110576 Rescue officials said least th...	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	equation determined empirically.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	denominatorsin terms normalization, described below.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	matrix called stochasticand defines Markov chain.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Every transition weighted accordingto similarity distributions.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	also model used torank sentences (Erkan Radev, 2004).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	3.4 Experiments topic-sensitive LexRank.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	experimented different values ourtraining data.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	thenfound configurations outperformed baseline.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	configurations applied ourdevelopment/test set.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	remainder thepaper explain process results detail.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	4 Experimental setup.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	4.1 Corpus.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	clusters characteristics shown Table 1.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	news articles collected varioussources.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Newstracker clusters collected automatically Web-based news summarization system.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	generated list factualquestions key understanding story.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	4.2 Evaluation metrics methods.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	One developing PRsystem, output input ananswer extraction system processing.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Table 1: Corpus complex news stories.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	sentence numbers top 20 sentences.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	measuregives us idea far must look theranked list order find correct answer.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	contrast, TRDR total reciprocal ranks allanswers found system.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	measure many relevant sentenceswere identified system.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	However, reportboth average MRR TRDR questionsin given data set.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	optimal range parameter 0.14 and0.20.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	ically diverse (e.g. paraphrases).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	clear ahigh question bias needed.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	5.1 Development/testing phase.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Table 5.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	system versus baseline unseen test set of134 questions.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	5.2 Testing phase.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	improvement average TRDR score statistically significant p-value 0.0619.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	last two sentences provide answers according judges, provide context information situation.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
Component relevance scores calculated using Term Frequency × Inverse Sentence Frequency (TF×ISF) (Otterbacher et al., 2005):quiring specific strategy.	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Using Random Walks Question-focused Sentence Retrieval	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Annotators generated list questions central understanding story corpus.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	experiments, themethod achieves TRDR score significantly higher baseline.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Understanding storiesis challenging number reasons.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	may choose receiveeither precise answer question-focused summary.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Currently, address question-focused sentence retrieval task.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	passage retrieval (PR) isclearly new problem (e.g.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	(7:6) Figure 1: Question tracking interface summarization system.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	sized it.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	end, propose use stochastic, graph-based method.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	system rank input documents, restricted terms number ofsentences may selected document.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Alternatively, sentences canbe returned user question-focused summary.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	similar snippet retrieval (Wu etal., 2004).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	3.1 LexRank method.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	apply LexRank, similarity graph producedfor sentences input document set.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	thegraph, node represents sentence.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	degree given node isan indication much information respective sentence common sentences.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	previously mentioned, original LexRank method performed wellin context generic summarization.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	3.2 Relevance question.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	also stem question remove stop words it.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	3.3 mixture model.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	1 0.03614457831325301 least two people dead, inclu...	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	0 0.28454242157110576 Officials said plane carryin...	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	2 0.1973852892722677 Italian police said plane car..	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	3 0.28454242157110576 Rescue officials said least th...	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	equation determined empirically.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	denominatorsin terms normalization, described below.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	matrix called stochasticand defines Markov chain.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Every transition weighted accordingto similarity distributions.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	also model used torank sentences (Erkan Radev, 2004).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	3.4 Experiments topic-sensitive LexRank.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	experimented different values ourtraining data.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	thenfound configurations outperformed baseline.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	configurations applied ourdevelopment/test set.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	remainder thepaper explain process results detail.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	4 Experimental setup.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	4.1 Corpus.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	clusters characteristics shown Table 1.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	news articles collected varioussources.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Newstracker clusters collected automatically Web-based news summarization system.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	generated list factualquestions key understanding story.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	4.2 Evaluation metrics methods.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	One developing PRsystem, output input ananswer extraction system processing.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Table 1: Corpus complex news stories.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	sentence numbers top 20 sentences.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	measuregives us idea far must look theranked list order find correct answer.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	contrast, TRDR total reciprocal ranks allanswers found system.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	measure many relevant sentenceswere identified system.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	However, reportboth average MRR TRDR questionsin given data set.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	optimal range parameter 0.14 and0.20.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	ically diverse (e.g. paraphrases).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	clear ahigh question bias needed.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	5.1 Development/testing phase.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Table 5.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	system versus baseline unseen test set of134 questions.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	5.2 Testing phase.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	improvement average TRDR score statistically significant p-value 0.0619.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	last two sentences provide answers according judges, provide context information situation.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
model similar spirit random- walk summarization model (Otterbacher et al., 2005).	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Using Random Walks Question-focused Sentence Retrieval	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Annotators generated list questions central understanding story corpus.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	experiments, themethod achieves TRDR score significantly higher baseline.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Understanding storiesis challenging number reasons.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	may choose receiveeither precise answer question-focused summary.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Currently, address question-focused sentence retrieval task.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	passage retrieval (PR) isclearly new problem (e.g.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	(7:6) Figure 1: Question tracking interface summarization system.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	sized it.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	end, propose use stochastic, graph-based method.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	1
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	system rank input documents, restricted terms number ofsentences may selected document.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Alternatively, sentences canbe returned user question-focused summary.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	similar snippet retrieval (Wu etal., 2004).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	3.1 LexRank method.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	apply LexRank, similarity graph producedfor sentences input document set.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	thegraph, node represents sentence.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	degree given node isan indication much information respective sentence common sentences.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	previously mentioned, original LexRank method performed wellin context generic summarization.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	3.2 Relevance question.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	also stem question remove stop words it.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	3.3 mixture model.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	1 0.03614457831325301 least two people dead, inclu...	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	0 0.28454242157110576 Officials said plane carryin...	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	2 0.1973852892722677 Italian police said plane car..	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	3 0.28454242157110576 Rescue officials said least th...	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	equation determined empirically.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	denominatorsin terms normalization, described below.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	matrix called stochasticand defines Markov chain.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Every transition weighted accordingto similarity distributions.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	also model used torank sentences (Erkan Radev, 2004).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	3.4 Experiments topic-sensitive LexRank.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	experimented different values ourtraining data.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	thenfound configurations outperformed baseline.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	configurations applied ourdevelopment/test set.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	remainder thepaper explain process results detail.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	4 Experimental setup.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	4.1 Corpus.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	clusters characteristics shown Table 1.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	news articles collected varioussources.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Newstracker clusters collected automatically Web-based news summarization system.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	generated list factualquestions key understanding story.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	4.2 Evaluation metrics methods.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	One developing PRsystem, output input ananswer extraction system processing.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Table 1: Corpus complex news stories.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	sentence numbers top 20 sentences.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	measuregives us idea far must look theranked list order find correct answer.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	contrast, TRDR total reciprocal ranks allanswers found system.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	measure many relevant sentenceswere identified system.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	However, reportboth average MRR TRDR questionsin given data set.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	optimal range parameter 0.14 and0.20.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	ically diverse (e.g. paraphrases).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	clear ahigh question bias needed.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	5.1 Development/testing phase.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Table 5.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	system versus baseline unseen test set of134 questions.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	5.2 Testing phase.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	improvement average TRDR score statistically significant p-value 0.0619.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	last two sentences provide answers according judges, provide context information situation.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
"topic-sensitiveLexRank proposed (Otterbacher et al., 2005).As LexRank, set sentences documentcluster represented graph, nodes aresentences links nodes inducedby similarity relation sentences.</S><S sid =""25"" ssid = ""4"">Thenthe system ranked sentences according random walk model defined terms inter-sentence similarities similarities sentences topic description question."	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Using Random Walks Question-focused Sentence Retrieval	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Annotators generated list questions central understanding story corpus.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	experiments, themethod achieves TRDR score significantly higher baseline.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Understanding storiesis challenging number reasons.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	may choose receiveeither precise answer question-focused summary.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Currently, address question-focused sentence retrieval task.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	passage retrieval (PR) isclearly new problem (e.g.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	(7:6) Figure 1: Question tracking interface summarization system.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	sized it.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	end, propose use stochastic, graph-based method.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	system rank input documents, restricted terms number ofsentences may selected document.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Alternatively, sentences canbe returned user question-focused summary.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	similar snippet retrieval (Wu etal., 2004).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	3.1 LexRank method.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	apply LexRank, similarity graph producedfor sentences input document set.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	thegraph, node represents sentence.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	degree given node isan indication much information respective sentence common sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	previously mentioned, original LexRank method performed wellin context generic summarization.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	3.2 Relevance question.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	also stem question remove stop words it.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	3.3 mixture model.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	1 0.03614457831325301 least two people dead, inclu...	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	0 0.28454242157110576 Officials said plane carryin...	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	2 0.1973852892722677 Italian police said plane car..	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	3 0.28454242157110576 Rescue officials said least th...	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	equation determined empirically.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	denominatorsin terms normalization, described below.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	matrix called stochasticand defines Markov chain.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Every transition weighted accordingto similarity distributions.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	also model used torank sentences (Erkan Radev, 2004).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	3.4 Experiments topic-sensitive LexRank.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	experimented different values ourtraining data.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	thenfound configurations outperformed baseline.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	configurations applied ourdevelopment/test set.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	remainder thepaper explain process results detail.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	4 Experimental setup.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	4.1 Corpus.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	clusters characteristics shown Table 1.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	news articles collected varioussources.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Newstracker clusters collected automatically Web-based news summarization system.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	generated list factualquestions key understanding story.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	4.2 Evaluation metrics methods.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	One developing PRsystem, output input ananswer extraction system processing.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Table 1: Corpus complex news stories.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	sentence numbers top 20 sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	measuregives us idea far must look theranked list order find correct answer.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	contrast, TRDR total reciprocal ranks allanswers found system.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	measure many relevant sentenceswere identified system.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	However, reportboth average MRR TRDR questionsin given data set.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	optimal range parameter 0.14 and0.20.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	ically diverse (e.g. paraphrases).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	clear ahigh question bias needed.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	5.1 Development/testing phase.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Table 5.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	system versus baseline unseen test set of134 questions.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	5.2 Testing phase.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	improvement average TRDR score statistically significant p-value 0.0619.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	last two sentences provide answers according judges, provide context information situation.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
apply LexRank query-focused context, topic-sensitive version LexRank isproposed (Otterbacher et al., 2005).	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Using Random Walks Question-focused Sentence Retrieval	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Annotators generated list questions central understanding story corpus.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	experiments, themethod achieves TRDR score significantly higher baseline.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Understanding storiesis challenging number reasons.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	may choose receiveeither precise answer question-focused summary.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Currently, address question-focused sentence retrieval task.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	passage retrieval (PR) isclearly new problem (e.g.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	(7:6) Figure 1: Question tracking interface summarization system.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	sized it.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	end, propose use stochastic, graph-based method.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	1
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	system rank input documents, restricted terms number ofsentences may selected document.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Alternatively, sentences canbe returned user question-focused summary.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	similar snippet retrieval (Wu etal., 2004).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	3.1 LexRank method.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	apply LexRank, similarity graph producedfor sentences input document set.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	thegraph, node represents sentence.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	degree given node isan indication much information respective sentence common sentences.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	previously mentioned, original LexRank method performed wellin context generic summarization.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	3.2 Relevance question.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	also stem question remove stop words it.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	3.3 mixture model.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	1 0.03614457831325301 least two people dead, inclu...	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	0 0.28454242157110576 Officials said plane carryin...	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	2 0.1973852892722677 Italian police said plane car..	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	3 0.28454242157110576 Rescue officials said least th...	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	equation determined empirically.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	denominatorsin terms normalization, described below.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	matrix called stochasticand defines Markov chain.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Every transition weighted accordingto similarity distributions.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	also model used torank sentences (Erkan Radev, 2004).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	3.4 Experiments topic-sensitive LexRank.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	experimented different values ourtraining data.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	thenfound configurations outperformed baseline.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	configurations applied ourdevelopment/test set.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	remainder thepaper explain process results detail.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	4 Experimental setup.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	4.1 Corpus.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	clusters characteristics shown Table 1.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	news articles collected varioussources.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Newstracker clusters collected automatically Web-based news summarization system.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	generated list factualquestions key understanding story.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	4.2 Evaluation metrics methods.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	One developing PRsystem, output input ananswer extraction system processing.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Table 1: Corpus complex news stories.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	sentence numbers top 20 sentences.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	measuregives us idea far must look theranked list order find correct answer.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	contrast, TRDR total reciprocal ranks allanswers found system.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	measure many relevant sentenceswere identified system.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	However, reportboth average MRR TRDR questionsin given data set.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	optimal range parameter 0.14 and0.20.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	ically diverse (e.g. paraphrases).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	clear ahigh question bias needed.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	5.1 Development/testing phase.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Table 5.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	system versus baseline unseen test set of134 questions.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	5.2 Testing phase.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	improvement average TRDR score statistically significant p-value 0.0619.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	last two sentences provide answers according judges, provide context information situation.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
topic- sensitive LexRank proposed (Otterbacher et al., 2005).	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Using Random Walks Question-focused Sentence Retrieval	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Annotators generated list questions central understanding story corpus.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	experiments, themethod achieves TRDR score significantly higher baseline.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Understanding storiesis challenging number reasons.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	may choose receiveeither precise answer question-focused summary.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Currently, address question-focused sentence retrieval task.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	passage retrieval (PR) isclearly new problem (e.g.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	(7:6) Figure 1: Question tracking interface summarization system.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	sized it.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	end, propose use stochastic, graph-based method.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	1
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	system rank input documents, restricted terms number ofsentences may selected document.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Alternatively, sentences canbe returned user question-focused summary.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	similar snippet retrieval (Wu etal., 2004).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	3.1 LexRank method.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	apply LexRank, similarity graph producedfor sentences input document set.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	thegraph, node represents sentence.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	degree given node isan indication much information respective sentence common sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	previously mentioned, original LexRank method performed wellin context generic summarization.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	3.2 Relevance question.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	also stem question remove stop words it.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	3.3 mixture model.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	1 0.03614457831325301 least two people dead, inclu...	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	0 0.28454242157110576 Officials said plane carryin...	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	2 0.1973852892722677 Italian police said plane car..	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	3 0.28454242157110576 Rescue officials said least th...	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	equation determined empirically.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	denominatorsin terms normalization, described below.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	matrix called stochasticand defines Markov chain.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Every transition weighted accordingto similarity distributions.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	also model used torank sentences (Erkan Radev, 2004).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	3.4 Experiments topic-sensitive LexRank.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	experimented different values ourtraining data.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	thenfound configurations outperformed baseline.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	configurations applied ourdevelopment/test set.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	remainder thepaper explain process results detail.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	4 Experimental setup.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	4.1 Corpus.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	clusters characteristics shown Table 1.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	news articles collected varioussources.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Newstracker clusters collected automatically Web-based news summarization system.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	generated list factualquestions key understanding story.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	4.2 Evaluation metrics methods.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	One developing PRsystem, output input ananswer extraction system processing.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Table 1: Corpus complex news stories.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	sentence numbers top 20 sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	measuregives us idea far must look theranked list order find correct answer.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	contrast, TRDR total reciprocal ranks allanswers found system.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	measure many relevant sentenceswere identified system.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	However, reportboth average MRR TRDR questionsin given data set.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	optimal range parameter 0.14 and0.20.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	ically diverse (e.g. paraphrases).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	clear ahigh question bias needed.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	5.1 Development/testing phase.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Table 5.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	system versus baseline unseen test set of134 questions.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	5.2 Testing phase.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	improvement average TRDR score statistically significant p-value 0.0619.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	last two sentences provide answers according judges, provide context information situation.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
apply LexRank query-focused context, topic-sensitive version LexRank proposed (Otterbacher et al., 2005).	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Using Random Walks Question-focused Sentence Retrieval	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	consider problem question-focused sentence retrieval complexnews articles describing multi-event stories published time.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Annotators generated list questions central understanding story corpus.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	dynamic nature stories,many questions time-sensitive (e.g.How many victims found?)Judges found sentences providing answer question.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	address thesentence retrieval problem, apply astochastic, graph-based method comparing relative importance textual units, previously used successfully generic summarization.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Currently, present topic-sensitive versionof method hypothesize canoutperform competitive baseline, whichcompares similarity sentenceto input question via IDFweightedword overlap.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	experiments, themethod achieves TRDR score significantly higher baseline.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Recent work motivated need systemsthat support Information Synthesis tasks, whicha user seeks global understanding topic orstory (Amigo et al., 2004).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	contrast classical question answering setting (e.g. TREC-style Q&A (Voorhees Tice, 2000)), userpresents single question system returns acorresponding answer (or set likely answers), inthis case user complex informationneed.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Similarly, reading complex newsstory, emergency situation, users mightseek answers set questions order understand better.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	example, Figure 1 showsthe interface Web-based news summarizationsystem, user queried informationabout Hurricane Isabel.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Understanding storiesis challenging number reasons.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	particular,complex stories contain many sub-events (e.g. thedevastation hurricane, relief effort, etc.) Inaddition, facts surrounding situationdo change (such Which area hurricane first hit?), others may change time (Howmany people left homeless?).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Therefore, working towards developing systemfor question answering clusters complex stories published time.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	seen bottom Figure 1, plan add component ourcurrent system allows users ask questions asthey read story.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	may choose receiveeither precise answer question-focused summary.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Currently, address question-focused sentence retrieval task.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	passage retrieval (PR) isclearly new problem (e.g.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	(Robertson et al.,1992; Salton et al., 1993)), remains important andyet often overlooked.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	noted (Gaizauskas et al.,2004), PR crucial first step questionanswering, Q&A research typically empha915 Hurricane Isabel's outer bands moving onshoreproduced 09/18, 6:18 2% SummaryThe North Carolina coast braced weakened still potent Hurricane Isabel already rain-soaked areas faraway Pennsylvania prepared possibly ruinous flooding.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	(2:3) hurricane warning effect CapeFear southern North Carolina VirginiaMaryland line, tropical storm warnings extended South Carolinato New Jersey.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	(2:14) outer edge hurricane approached North Carolina coast Wednesday, center storm still400 miles south-southeast Cape Hatteras, N.C., late Wednesday morning.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states affected hurricane far?	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Around 200,000 people coastal areas North Carolina Virginia ordered evacuate risk getting trappedby flooding storm surges 11 feet.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	(5:8) storm expected hit full fury today, slamming intothe North Carolina coast 105mph winds 45-foot wave crests, moving Virginia bashing thecapital gusts 60 mph.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	(7:6) Figure 1: Question tracking interface summarization system.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	sized it.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	specific problem consider differsfrom classic task PR Q&A system ininteresting ways, due time-sensitive nature ofthe stories corpus.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	example, one challengeis answer users question may updated reworded time journalists orderto keep running story fresh, factsthemselves change.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Therefore, often morethan one correct answer question.We aim develop method sentence retrieval goes beyond finding sentences aresimilar single query.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	end, propose use stochastic, graph-based method.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Recently, graph-based methods proved useful fora number NLP IR tasks documentre-ranking ad hoc IR (Kurland Lee, 2005)and analyzing sentiments text (Pang Lee,2004).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	(Erkan Radev, 2004), introducedthe LexRank method successfully applied togeneric, multi-document summarization.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Presently,we introduce topic-sensitive LexRank creating asentence retrieval system.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	evaluate performance competitive baseline, considers similarity sentence thequestion (using IDF-weighed word overlap).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Wedemonstrate LexRank significantly improvesquestion-focused sentence selection baseline.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	goal build question-focused sentence retrieval mechanism using topic-sensitive version ofthe LexRank method.	1
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	contrast previous PR systems Okapi (Robertson et al., 1992), ranks documents relevancy proceeds tofind paragraphs related question, address thefinergrained problem finding sentences containing answers.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	addition, input system isa set documents relevant topic querythat user already identified (e.g. via searchengine).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	system rank input documents, restricted terms number ofsentences may selected document.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	output system, ranked list sentences relevant users question, subsequently used input answer selection system order find specific answers extracted sentences.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Alternatively, sentences canbe returned user question-focused summary.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	similar snippet retrieval (Wu etal., 2004).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	However, system answers extracted set multiple documents rather thanon document-by-document basis.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	3.1 LexRank method.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	(Erkan Radev, 2004), concept graph-based centrality used rank set sentences,in producing generic multi-document summaries.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	apply LexRank, similarity graph producedfor sentences input document set.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	thegraph, node represents sentence.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	areedges nodes cosine similarity respective pair sentences exceedsa given threshold.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	degree given node isan indication much information respective sentence common sentences.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Therefore, sentences contain salient information document set centralwithin graph.Figure 2 shows example similarity graph set five input sentences, using cosine similarity threshold 0.15.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	similarity graph isconstructed, sentences ranked accordingto eigenvector centrality.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	previously mentioned, original LexRank method performed wellin context generic summarization.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Below,we describe topic-sensitive version LexRank,which appropriate question-focusedsentence retrieval problem.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	new approach, 916 score sentence determined mixture modelof relevance sentence query thesimilarity sentence high-scoring sentences.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	3.2 Relevance question.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	topic-sensitive LexRank, first stem thesentences set articles compute word IDFsby following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN total number sentences cluster, sfw number sentences wordw appears in.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	also stem question remove stop words it.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	relevance sentence tothe question q computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) tfw,s tfw,q number times wappears q, respectively.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	model hasproven successful query-based sentence retrieval (Allan et al., 2003), used competitive baseline study (e.g. Tables 4, 5 and7).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	3.3 mixture model.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	baseline system explained makeuse inter-sentence information cluster.We hypothesize sentence similar tothe high scoring sentences cluster alsohave high score.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	instance, sentence thatgets high score baseline model likely tocontain answer question, related sentence, may similar question itself, also likely contain answer.This idea captured following mixture model, p(s|q), score sentence givena question q, determined sum relevance question (using measure asthe baseline described above) similarity tothe sentences document cluster: p(s|q) = rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) C set sentences cluster.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Thevalue d, also refer question bias, trade-off two terms Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	1 0.03614457831325301 least two people dead, inclu...	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	0 0.28454242157110576 Officials said plane carryin...	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	2 0.1973852892722677 Italian police said plane car..	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	3 0.28454242157110576 Rescue officials said least th...	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Graph Figure 2: LexRank example: sentence similaritygraph cosine threshold 0.15.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	equation determined empirically.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	highervalues d, give importance relevance question compared similarity tothe sentences cluster.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	denominatorsin terms normalization, described below.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	use cosine measure weightedby word IDFs similarity two sentences cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 written matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) square matrix given index i,all elements ith column proportionalto rel(i|q).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	B also square matrix eachentry B(i, j) proportional sim(i, j).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	matrices normalized row sums add 1.Note result normalization, rowsof resulting square matrixQ = [dA+(1-d)B]also add 1.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	matrix called stochasticand defines Markov chain.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	view sentence state Markov chain, thenQ(i, j) specifies transition probability state state jin corresponding Markov chain.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	vector pwe looking Equation 5 stationarydistribution Markov chain.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	intuitive interpretation stationary distribution under- 917 stood concept random walk graphrepresentation Markov chain.With probability d, transition made current node (sentence) nodes similar query.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	probability (1-d), transitionis made nodes lexically similar thecurrent node.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Every transition weighted accordingto similarity distributions.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	element thevector p gives asymptotic probability endingup corresponding state long run regardless starting state.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	stationary distributionof Markov chain computed simple iterative algorithm, called power method.1 simpler version Equation 5, auniform matrix andB normalized binary matrix,is known PageRank (Brin Page, 1998; Pageet al., 1998) used rank web pages theGoogle search engine.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	also model used torank sentences (Erkan Radev, 2004).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	3.4 Experiments topic-sensitive LexRank.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	experimented different values ourtraining data.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	also considered several thresholdvalues inter-sentence cosine similarities, wherewe ignored similarities sentencesthat threshold.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	training phaseof experiment, evaluated combinationsof LexRank range [0, 1] (in increments 0.10) similarity threshold ranging [0, 0.9] (in increments 0.05).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	thenfound configurations outperformed baseline.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	configurations applied ourdevelopment/test set.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Finally, best sentence retrieval system applied test data set andevaluated baseline.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	remainder thepaper explain process results detail.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	4 Experimental setup.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	4.1 Corpus.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	built corpus 20 multi-document clusters ofcomplex news stories, plane crashes, political controversies natural disasters.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	data 1The stationary distribution unique power methodis guaranteed converge provided Markov chain isergodic (Seneta, 1981).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	non-ergodic Markov chain bemade ergodic reserving small probability jumping toany state current state (Page et al., 1998).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	clusters characteristics shown Table 1.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	news articles collected varioussources.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Newstracker clusters collected automatically Web-based news summarization system.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	number clusters randomly assignedto training, development/test test data setswere 11, 3 6, respectively.Next, assigned cluster articles annotator, asked read articles thecluster.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	generated list factualquestions key understanding story.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	wecollected questions cluster, two judgesindependently annotated nine training clusters.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	sentence question pair givencluster, judges asked indicate whetheror sentence contained complete answerto question.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	acceptable rate inter-judge agreement verified first nine clusters (Kappa (Carletta, 1996) 0.68), remaining11 clusters annotated one judge each.In cases, judges find sentences containing answer given question.Such questions removed corpus.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Thefinal number questions annotated answersover entire corpus 341, distributionsof questions per cluster found Table 1.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	4.2 Evaluation metrics methods.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	evaluate sentence retrieval mechanism, weproduced extract files, contain list sentences deemed relevant question, thesystem human judgment.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	compare different configurations system baselinesystem, produced extracts fixed length 20sentences.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	evaluations question answeringsystems often based shorter list rankedsentences, chose generate longer lists several reasons.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	One developing PRsystem, output input ananswer extraction system processing.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Insuch setting, would likely want generate relatively longer list candidate sentences.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Aspreviously mentioned, corpus questionsoften one relevant answer, ideally,our PR system would find many relevant sentences, sending answer componentto decide answer(s) returned theuser.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	systems extract file lists document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train condition whichthreat GIA take action?Milan plane MSNBC, CNN, ABC, 9 15 train many people thecrash Fox, USAToday building time crash?Turkish plane BBC, ABC, 10 12 train plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train many people killed inattack recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train blame forclub fire Fox, BBC, Ananova fire?FBI AFP, UPI 3 14 train much State Department offeringwanted information leading bin Ladens arrest?Russia bombing AP, AFP 2 11 train cause blast?Bali terror CNN, FoxNews, ABC, 10 30 train motivationsattack BBC, Ananova attackers?Washington DC FoxNews, Haaretz, BBC, 8 28 train kinds equipment weaponssniper BBC, Washington Times, CBS used killings?GSPC terror Newstracker 8 29 train charges againstgroup GSPC suspects?China Novelty 43 25 18 train magnitude theearthquake earthquake Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test many people FoxNews, Washington Post board?David Beckham AFP 20 28 dev/test long Beckham playing fortrade MU moved RM?Miami airport Newstracker 12 15 dev/test many concourses doesevacuation airport have?US hurricane DUC d04a 14 14 test places hurricane landed?EgyptAir crash Novelty 4 25 29 test many people killed?Kursk submarine Novelty 33 25 30 test Kursk sink?Hebrew University bombing Newstracker 11 27 test many people injured?Finland mall bombing Newstracker 9 15 test many people mall time bombing?Putin visits Newstracker 12 20 test issue concerned BritishEngland human rights groups?	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Table 1: Corpus complex news stories.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	sentence numbers top 20 sentences.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Thegold standard extracts list sentences judged ascontaining answers given question annotators (and therefore variable sizes) particular order.2 evaluated performance systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees Tice, 2000) Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used TREC Q&A evaluations, thereciprocal rank first correct answer (or sentence, case) given question.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	measuregives us idea far must look theranked list order find correct answer.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	contrast, TRDR total reciprocal ranks allanswers found system.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	context answering questions complex stories, thereis often one correct answer question,and answers typically time-dependent, weshould focus maximizing TRDR, gives us 2For clusters annotated two judges, sentences chosenby least one judge included.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	measure many relevant sentenceswere identified system.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	However, reportboth average MRR TRDR questionsin given data set.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	training phase, searched parameterspace values (the question bias) thesimilarity threshold order optimize resultingTRDR scores.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	problem, expected arelatively low similarity threshold pair highquestion bias would achieve best results.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Table 2shows effect varying similarity threshold.3 notation LR[a, d] used, similarity threshold question bias.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	optimal range parameter 0.14 and0.20.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	intuitive threshold toohigh, lexically similar sentences represented graph, method doesnot find sentences related lex3A threshold -1 means threshold used suchthat sentences included graph.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect similarity threshold (a) Ave. MRR TRDR.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect question bias (d)on Ave. MRR TRDR.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	ically diverse (e.g. paraphrases).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Table 3 shows theeffect varying question bias two differentsimilarity thresholds (0.02 0.20).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	clear ahigh question bias needed.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	However, small probability jumping node lexically similar given sentence (rather questionitself) needed.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Table 4 shows configurationsof LexRank performed better baselinesystem training data, based mean TRDRscores 184 training questions.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	appliedall four configurations unseen development/test data, order see could furtherdifferentiate performances.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	5.1 Development/testing phase.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	scores four LexRank systems thebaseline development/test data shown System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline terms TRDR score.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores cluster: baseline versusLR[0.20,0.95].	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Table 5.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	time, four LexRank systems outperformed baseline, terms average MRRand TRDR scores.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	analysis average scoresover 72 questions within three clusters best system, LR[0.20,0.95], shownin Table 6.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	LexRank outperforms baseline system first two clusters termsof MRR TRDR, performances substantially different third cluster.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Therefore,we examined properties questions within eachcluster order see effect might onsystem performance.We hypothesized baseline system, compares similarity sentence question using IDF-weighted word overlap, perform well questions provide many contentwords.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	contrast, LexRank might perform better question provides fewer content words,since considers similarity query andinter-sentence similarity.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	72 questions inthe development/test set, baseline system outperformed LexRank 22 questions.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	fact, theaverage number content words among 22questions slightly, significantly, higherthan average remaining questions (3.63words per question versus 3.46).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Given observation, experimented two mixed strategies,in number content words questiondetermined whether LexRank baseline systemwas used sentence retrieval.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	tried thresholdvalues 4 6 content words, however, didnot improve performance pure strategyof system LR[0.20,0.95].	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Therefore, applied 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	system versus baseline unseen test set of134 questions.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	5.2 Testing phase.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	shown Table 7, LR[0.20,0.95] outperformedthe baseline system test data termsof average MRR TRDR scores.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	improvement average TRDR score statistically significant p-value 0.0619.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Since interested passage retrieval mechanism findssentences relevant given question, providing input question answering component system, improvement average TRDR score isvery promising.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	saw Section 5.1 thatLR[0.20,0.95] may perform better questionor cluster types others, conclude beatsthe competitive baseline one looking optimize mean TRDR scores large set questions.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	However, future work, continueto improve performance, perhaps developing mixed strategies using different configurationsof LexRank.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	idea behind using LexRank sentence retrieval system considers similarity candidate sentences inputquery, similarity candidatesentences themselves, likely miss important sentences.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	using metric comparesentences query, always likely bea tie multiple sentences (or, similarly, theremay cases fewer number desired sentences similarity scores zero).LexRank effectively provides means break suchties.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	example scenario illustrated inTables 8 9, show top ranked sentencesby baseline LexRank, respectively thequestion What caused Kursk sink? theKursk submarine cluster.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	seen topfive sentences chosen baseline system Rank Sentence Score Relevant?	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	1 Russian governmental commission 4.2282 Naccident submarine Kursk sinking inthe Barents Sea August 12 rejected11 original explanations disaster,but still cannot conclude caused the.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	tragedy indeed, Russian Deputy Premier IlyaKlebanov said Friday.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	2 final word caused 4.2282 Nthe submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Kursk may collided anotherobject receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 said Thursday collision bigobject caused Kursk nuclear submarineto sink bottom Barents Sea.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday collision big.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	5 President Clintons national security adviser, 4.2282 NSamuel Berger, provided Russian.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	counterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Table 8: Top ranked sentences using baseline systemon question What caused Kursk sink?.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	sentence score (similarity query), yetthe top ranking two sentences actually relevant according judges.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	contrast, LexRankachieved better ranking sentences since isbetter able differentiate them.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	shouldbe noted LexRank baseline systems, chronological ordering documents andsentences preserved, cases twosentences score, one publishedearlier ranked higher.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	presented topic-sensitive LexRank appliedit problem sentence retrieval.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Web-based news summarization setting, users system could choose see retrieved sentences (asin Table 9) question-focused summary.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	indicated Table 9, top three sentenceswere judged annotators providing complete answer respective question.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	thefirst two sentences provide answer (a collision caused Kursk sink), third sentenceprovides different answer (an explosion caused thedisaster).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	last two sentences provide answers according judges, provide context information situation.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Alternatively, user might prefer see extracted 921 Rank Sentence Score Relevant?	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday collision big.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	object caused Kursk nuclear submarineto sink bottom Barents Sea.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	3 Russian navy refused confirm this, 0.0125 Ybut officers said explosion thetorpedo compartment front the.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	submarine apparently caused Kursk sink.4 President Clintons national security adviser, 0.0124 N Samuel Berger, provided Russiancounterpart written summary whatU.S. naval intelligence officials believe caused nuclear-powered submarine Kursk tosink last month Barents Sea, officials said Wednesday.5 final word caused 0.0123 N submarine sink participatingin major naval exercise, DefenseMinister Igor Sergeyev said theory Kursk may collided anotherobject receiving increasingly concrete confirmation.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	Table 9: Top ranked sentences using theLR[0.20,0.95] system question What causedthe Kursk sink? answers retrieved sentences.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	case,the sentences selected system would sentto answer identification component furtherprocessing.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	discussed Section 2, goal wasto develop topic-sensitive version LexRank andto use improve baseline system, hadpreviously used successfully query-basedsentence retrieval (Allan et al., 2003).	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	terms ofthis task, shown large set unaltered questions written annotators, LexRankcan, average, outperform baseline system,particularly terms TRDR scores.	0
"Afterwards, approach evaluated two existing approaches, rely conventional semantic network able capture binary relations only.</S><S sid =""108"" ssid = ""40"">The one based topic-sensitive LexRank (Otterbacher et al., 2005), called title-sensitive PageRank here."	would like thank members CLAIRgroup Michigan particular Siwei Shen andYang Ye assistance project.	0
