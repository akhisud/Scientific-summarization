"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Decision Tree Bigrams Accurate Predictor Word Sense	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	paper presents corpus-based approach word sense disambiguation decision tree assigns sense ambiguous word based bigrams occur nearby.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	approach evaluated using sense-tagged corpora 1998 SENSEVAL word sense disambiguation exercise.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	accurate average results reported 30 36 words, accurate best results 19 36 words.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Word sense disambiguation process selecting appropriate meaning word, based context occurs.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	purposes assumed set possible meanings, i.e., sense inventory, already determined.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	example, suppose bill following set possible meanings: piece currency, pending legislation, bird jaw.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	used context Senate bill consideration, human reader immediately understands bill used legislative sense.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	However, computer program attempting perform task faces diÆcult problem since bene?t innate common{sense linguistic knowledge.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Rather attempting provide computer programs real{world knowledge comparable humans, natural language processing turned corpus{based methods.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	approaches use techniques statistics machine learning induce models language usage large samples text.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	models trained perform particular tasks, usually via supervised learning.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	paper describes approach decision tree learned number sentences instance ambiguous word manually annotated sense{tag denotes appropriate sense context.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Prior learning, sense{tagged corpus must converted regular form suitable automatic processing.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	sense{tagged occurrence ambiguous word converted feature vector, feature represents property surrounding text considered relevant disambiguation process.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Given exibility complexity human language, potentially in?nite set features could utilized.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	However, corpus{based approaches features usually consist information readily iden- ti?ed text, without relying extensive external knowledge sources.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	typically include part{of{speech surrounding words, presence certain key words within window context, various syntactic properties sentence ambiguous word.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	approach paper relies upon feature set made bigrams, two word sequences occur text.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	context ambiguous word occurs represented number binary features indicate whether particular bigram occurred within approximately 50 words left right word disambiguated.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	take approach since surface lexical features like bigrams, collocations, co{occurrences often contribute great deal disambiguation accuracy.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	clear much disambiguation accuracy improved use features identi?ed complex pre{processing part{of{speech tagging, parsing, anaphora resolution.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	One objectives establish clear upper bounds accuracy disambiguation using feature sets impose substantial pre{ processing requirements.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	paper continues discussion methods identifying bigrams included feature set learning.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	decision tree learning algorithm described, benchmark learning algorithms included purposes comparison.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	experimental data discussed, empirical results presented.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	close analysis ?ndings discussion related work.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	developed approach word sense disambiguation represents text entirely terms occurrence bigrams, de?ne two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation Bigram Counts consecutive words occur text.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	distributional characteristics bigrams fairly consistent across corpora; majority occur one time.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Given sparse skewed nature data, statistical methods used select interesting bigrams must carefully chosen.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	explore two alternatives, power divergence family goodness ?t statistics Dice CoeÆcient, information theoretic measure related point- wise Mutual Information.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Figure 1 summarizes notation word bigram counts used paper way 2 ? 2 contingency table.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	value n 11 shows many times bigram big cat occurs corpus.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	value n 12 shows often bigrams occur big ?rst word cat second.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	counts n +1 n 1+ indicate often words big cat occur ?rst second words bigram corpus.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	total number bigrams corpus represented n ++ . 2.1 Power Divergence Family.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	(Cressie Read, 1984) introduce power divergence family goodness ?t statistics.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	number well known statistics belong family, including likelihood ratio statisticG 2 Pearson'sX 2 statistic.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	measure divergence observed (n ij ) expected (m ij ) bigram counts, ij estimated based assumption component words bigram occur together strictly chance.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	(Dunning, 1993) argues favor G2 X2, especially dealing sparse skewed data distributions.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	However, (Cressie Read, 1984) suggest cases Pearson's statistic reliable likelihood ratio one test always preferred other.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	light this, (Pedersen, 1996) presents Fisher's exact test alternative since rely distributional assumptions underly Pearson's test likelihood ratio.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Unfortunately usually clear test appropriate particular sample data.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	take following approach, based observation tests assign approximately measure statistical signi?cance bi- gram counts contingency table violate distributional assumptions underly goodness ?t statistics.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	perform tests using X 2 , G 2 , Fisher's exact test bigram.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	resulting measures statistical signi?cance di?er, distribution bigram counts causing least one tests become unreliable.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	occurs rely upon value Fisher's exact test since makes fewer assumptions underlying distribution data.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	experiments paper, identi?ed top 100 ranked bigrams occur 5 times training corpus associated word.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	cases rankings produced G 2 , X 2 , Fisher's exact test disagreed, altogether surprising given low frequency bigrams excluded.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Since statistics produced rankings, hereafter make distinction among simply refer generically power divergence statistic.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	2.2 Dice CoeÆcient.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Dice CoeÆcient descriptive statistic provides measure association among two words corpus.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	similar pointwise Mutual Information, widely used measure ?rst introduced identifying lexical relationships (Church Hanks, 1990).	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Pointwise Mutual Information de?ned follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ w 1 w 2 represent two words make bigram.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Pointwise Mutual Information quanti?es often two words occur together bigram (the numerator) relative often occur overall corpus (the denominator).	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	However, curious limitation pointwise Mutual Information.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	bigram w 1 w 2 occurs n 11 times corpus, whose component words w 1 w 2 occur part bigram, result increasingly strong measures association value n 11 decreases.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Thus, maximum pointwise Mutual Information given corpus assigned bi- grams occur one time, whose component words never occur outside bigram.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	usually bigrams prove useful disambiguation, yet dominate ranked list determined pointwise Mutual Information.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Dice CoeÆcient overcomes limitation, de?ned follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ n 11 = n 1+ = n +1 value Dice(w 1 ; w 2 ) 1 values n 11 . value n. 11 less either marginal totals (the typical case) rankings produced Dice Co- eÆcient similar Mutual Information.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	relationship pointwise Mutual Information Dice CoeÆcient also discussed (Smadja et al., 1996).	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	developed Bigram Statistics Package produce ranked lists bigrams using range tests.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	software written Perl freely available www.d.umn.edu/~tpederse.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Decision trees among widely used machine learning algorithms.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	perform general speci?c search feature space, adding informative features tree structure search proceeds.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	objective select minimal set features eÆciently partitions feature space classes observations assemble tree.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	case, observations manually sense{tagged examples ambiguous word context partitions correspond di?erent possible senses.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	feature selected search process represented node learned decision tree.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	node represents choice point number di?erent possible values feature.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Learning continues training examples accounted decision tree.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	general, tree overly speci?c training data generalize well new examples.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Therefore learning followed pruning step nodes eliminated reorganized produce tree generalize new circumstances.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Test instances disambiguated ?nding path learned decision tree root leaf node corresponds observed features.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	instance ambiguous word dis- ambiguated passing series tests, test asks particular bigram occurs available window context.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	also include three benchmark learning algorithms study: majority classi?er, decision stump, Naive Bayesian classi?er.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	majority classi?er assigns common sense training data every instance test data.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	decision stump one node decision tree(Holte, 1993) created stopping decision tree learner single informative feature added tree.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Naive Bayesian classi?er (Duda Hart, 1973) based certain blanket assumptions interactions among features corpus.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	search feature space performed build representative model case decision trees.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Instead, features included classi- ?er assumed relevant task hand.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	assumption feature conditionally independent features, given sense ambiguous word.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	often used bag words feature set, every word training sample represented binary feature indicates whether occurs window context surrounding ambiguous word.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	use Weka (Witten Frank, 2000) implementations C4.5 decision tree learner (known J48), decision stump, Naive Bayesian classi?er.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Weka written Java freely available www.cs.waikato.ac.nz/~ml.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	empirical study utilizes training test data 1998 SENSEVAL evaluation word sense disambiguation systems.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Ten teams participated supervised learning portion event.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Additional details exercise, including data results referred paper, found SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) (Kilgarri?	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Palmer, 2000).	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	included 36 tasks SENSEVAL training test data provided.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	task requires occurrences particular word test data disambiguated based model learned sense{tagged instances training data.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	words used multiple tasks di?erent parts speech.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	example, two tasks associated bet, one use noun verb.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Thus, 36 tasks involving disambiguation 29 di?erent words.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	words part speech associated task shown Table 1 column 1.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Note parts speech encoded n noun, adjective, v verb, p words part speech provided.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	number test training instances task shown columns 2 4.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	instance consists sentence ambiguous word occurs well one two surrounding sentences.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	general total context available ambiguous word less 100 surrounding words.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	number distinct senses test data task shown column 3.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	following process repeated task.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Capitalization punctuation removed training test data.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Two feature sets selected training data based top 100 ranked bigrams according power divergence statistic Dice CoeÆcient.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	bigram must occurred 5 times included feature.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	step ?lters large number possible bi- grams allows decision tree learner focus small number candidate bigrams likely helpful disambiguation process.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	training test data converted feature vectors feature represents occurrence one bigrams belong feature set.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	representation training data actual input learning algorithms.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Decision tree decision stump learning performed twice, using feature set determined power divergence statistic using feature set identi?ed Dice CoeÆcient.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	majority classi?er simply determines frequent sense training data assigns instances test data.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Naive Bayesian classi?er based feature set every word occurs 5 times training data included feature.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	learned models used disambiguate test data.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	test data kept separate stage.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	employ ?ne grained scoring method, word counted correctly disambiguated assigned sense tag exactly matches true sense tag.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	partial credit assigned near misses.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	accuracy attained learning algorithms shown Table 1.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Column 5 reports accuracy majority classi?er, columns 6 7 show best average accuracy reported 10 participating SENSEVAL teams.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	evaluation SENSEVAL based precision recall, converted scores accuracy taking product.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	However, best precision recall may come di?erent teams, best accuracy shown column 6 may actually higher single participating SENSEVAL system.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	average accuracy column 7 product average precision recall reported participating SENSEVAL teams.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Column 8 shows accuracy decision tree using J48 learning algorithm features identi?ed power divergence statistic.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Column 10 shows accuracy decision tree Dice CoeÆcient selects features.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Columns 9 11 show accuracy decision stump based power divergence statistic Dice CoeÆcient respectively.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Finally, column 13 shows accuracy Naive Bayesian classi?er based bag words feature set.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	accurate method decision tree based feature set determined power divergence statistic.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	last line Table 1 shows win-tie-loss score decision tree/power divergence method relative every method.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	win shows accurate method column, loss means less accurate, tie means equally accurate.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	decision tree/power divergence method accurate best reported SENSEVAL results 19 36 tasks, accurate 30 36 tasks compared average reported accuracy.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	decision stumps also fared well, proving accurate best SENSEVAL results 14 36 tasks.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	general feature sets selected power divergence statistic result accurate decision trees selected Dice CoeÆcient.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	power divergence tests prove reliable since account possible events surrounding two words w 1 w 2 ; occur bigram w 1 w 2 , w 1 w 2 occurs bigram without other, bigram consists neither.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Dice CoeÆcient based strictly event w 1 w 2 occur together bigram.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	6 tasks decision tree / power divergence approach less accurate SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, sanction-p.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	dramatic difference occurred amaze-v, SENSE- VAL average 92.4% decision tree accuracy 58.6%.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	However, unusual task every instance test data belonged single sense minority sense training data.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	characteristics decision trees decision stumps learned word shown Table 2.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Column 1 shows word part speech.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Columns 2, 3, 4 based feature set selected power divergence statistic columns 5, 6, 7 based Dice CoeÆ- cient.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Columns 2 5 show node selected serve decision stump.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Columns 3 6 show number leaf nodes learned decision tree relative number total nodes.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Columns 4 7 show number bigram features selected Table 1: Experimental Results.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	table shows little di?erence decision stump nodes selected feature sets determined power divergence statistics versus Dice CoeÆcient.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	expected since top ranked bigrams measure consistent, decision stump node generally chosen among those.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	However, di?erences feature sets selected power divergence statistics Dice CoeÆcient.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	ected different sized trees learned based feature sets.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	number leaf nodes total number nodes learned tree shown columns 3 6.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	number internal nodes simply di?erence total nodes leaf nodes.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	leaf node represents end path decision tree makes sense distinction.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Since bigram feature appear decision tree, number inter- Table 2: Decision Tree Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n accident 8/15 101 accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n excess 13/25 104 excess 11/21 102 oat-n oat 7/13 13 oat 7/13 13 giant-n giants 16/31 103 giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n 1/1 7 1/1 7 promise-n promise 95/189 100 promising 49/97 107 sack-n sack 5/9 31 sack 5/9 31 scrap-n scrap 7/13 8 scrap 7/13 8 shirt-n shirt 38/75 101 shirt 55/109 101 amaze-v amazed 11/21 102 amazed 11/21 102 bet-v bet 4/7 10 bet 4/7 10 bother-v bothered 19/37 101 bothered 20/39 106 bury-v buried 28/55 103 buried 32/63 103 calculate-v calculated 5/9 103 calculated 5/9 103 consume-v 4/7 20 4/7 20 derive-v derived 10/19 104 derived 10/19 104 oat-v oated 24/47 80 oated 24/47 80 invade-v invade 55/109 107 invade 66/127 108 promise-v promise 3/5 100 promise 5/9 106 sack-v return 1/1 91 return 1/1 91 scrap-v 1/1 7 1/1 7 seize-v seize 26/51 104 seize 57/113 104 brilliant-a brilliant 26/51 101 brilliant 42/83 103 oating-a 7/13 10 7/13 10 generous-a generous 57/113 103 generous 56/111 102 giant-a giant 2/3 102 giant 1/1 101 modest-a modest 14/27 101 modest 10/19 105 slight-a slightest 2/3 105 slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band 14/27 100 band 21/41 117 bitter-p bitter 22/43 54 bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p head 90/179 100 head 81/161 105 nal nodes represents number bigram features selected decision tree learner.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	One original hypotheses accurate decision trees bigrams include relatively small number features.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	motivated success decision stumps performing disambiguation based single bigram feature.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	experiments, decision trees used bigram features identi?ed ?ltering step, many words decision tree learner went eliminate candidate features.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	seen comparing number internal nodes number candidate features shown columns 4 7.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	1 also noteworthy bigrams ultimately selected decision tree learner inclusion tree always include bigrams ranked highly power divergence statistic Dice CoeÆcient.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	expected, since selection bigrams raw text mea1 words 100 top ranked bigrams form set candidate features presented decision tree learner.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	ties top 100 rankings may 100 features, fewer 100 bi- grams occurred 5 times bigrams included feature set.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	suring association two words, decision tree seeks bigrams partition instances ambiguous word distinct senses.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	particular, decision tree learner makes decisions bigram include nodes tree using gain ratio, measure based overall Mutual Information bigram particular word sense.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Finally, note smallest decision trees functionally equivalent benchmark methods.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	decision tree 1 leaf node internal nodes (1/1) acts majority classi?er.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	decision tree 2 leaf nodes 1 internal node (2/3) structure decision stump.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	One long-term objectives identify core set features useful disambiguating wide class words using supervised unsupervised methodologies.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	presented ensemble approach word sense disambiguation (Pedersen, 2000) multiple Naive Bayesian classi?ers, based co{ occurrence features varying sized windows context, shown perform well widely studied nouns interest line.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	accuracy approach good previously published results, learned models complex diÆcult interpret, e?ect acting accurate black boxes.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	experience variations learning algorithms far less signi?cant contributors disambiguation accuracy variations feature set.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	words, informative feature set result accurate disambiguation used wide range learning algorithms, learning algorithm perform well given uninformative misleading set features.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Therefore, focus developing discovering feature sets make distinctions among word senses.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	learning algorithms must produce accurate models, also shed new light relationships among features allow us continue re?ning understanding feature sets.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	believe decision trees meet criteria.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	wide range implementations available, known robust accurate across range domains.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	important, structure easy interpret may provide insights relationships exist among features general rules disambiguation.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Bigrams used features word sense disambiguation, particularly form collocations ambiguous word one component bigram (e.g., (Bruce Wiebe, 1994), (Ng Lee, 1996), (Yarowsky, 1995)).	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	bigrams identify collocations include word disambiguated, requirement case.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Decision trees used supervised learning approaches word sense disambiguation, fared well number comparative studies (e.g., (Mooney, 1996), (Pedersen Bruce, 1997)).	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	former used bag word feature sets latter used mixed feature set included part-of-speech neighboring words, three collocations, morphology ambiguous word.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	believe approach paper ?rst time decision trees based strictly bigram features employed.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	decision list closely related approach also applied word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks Stevenson, 1998), (Yarowsky, 2000)).	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Rather building traversing tree perform disambiguation, list employed.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	general case decision list may suffer less fragmentation learning decision trees; practical matter means decision list less likely over{trained.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	However, believe fragmentation also ects feature set used learning.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	consists approximately 100 binary features.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	results relatively small feature space likely su?er fragmentation larger spaces.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	number immediate extensions work.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	?rst ease requirement bi- grams made two consecutive words.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Rather, search bigrams component words may separated words text.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	second eliminate ?ltering step candidate bigrams selected power divergence statistic.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Instead, decision tree learner would consider possible bigrams.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Despite increasing danger fragmentation, interesting issue since bigrams judged informative decision tree learner always ranked highly ?ltering step.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	particular, determine ?ltering process ever eliminates bi- grams could signi?cant sources disambiguation information.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	longer term, hope adapt approach unsupervised learning, disambiguation performed without bene?t sense tagged text.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	optimistic viable, since bigram features easy identify raw text.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	results approach compared 1998 SENSEVAL word sense disambiguation exercise show bigram based decision tree approach accurate best SENSEVAL results 19 36 words.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	Bigram Statistics Package implemented Satanjeev Banerjee, supported Grant{in{Aid Research, Artistry Scholarship OÆce Vice President Research Dean Graduate School University Minnesota.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	would like thank SENSEVAL organizers making data results 1998 event freely available.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	comments three anonymous reviewers helpful preparing ?nal version paper.	0
"frustration models lack intuitive interpretation led development decision trees based bigram features (Pedersen, 2001a).</S><S sid =""132"" ssid = ""34"">This quite similar bagged decision trees bigrams (B) presented here, except ear­lier work learns single decision tree training examples represented top 100 ranked bi-grams, according log–likelihood ratio.</S><S sid =""133"" ssid = ""35"">This earlier approach evaluated SENSEVAL­1 data achieved overall accuracy 64%, whereas bagged decision tree presented achieves accuracy 68% data."	preliminary version paper appears (Pedersen, 2001).	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Decision Tree Bigrams Accurate Predictor Word Sense	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	paper presents corpus-based approach word sense disambiguation decision tree assigns sense ambiguous word based bigrams occur nearby.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	approach evaluated using sense-tagged corpora 1998 SENSEVAL word sense disambiguation exercise.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	accurate average results reported 30 36 words, accurate best results 19 36 words.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Word sense disambiguation process selecting appropriate meaning word, based context occurs.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	purposes assumed set possible meanings, i.e., sense inventory, already determined.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	example, suppose bill following set possible meanings: piece currency, pending legislation, bird jaw.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	used context Senate bill consideration, human reader immediately understands bill used legislative sense.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	However, computer program attempting perform task faces diÆcult problem since bene?t innate common{sense linguistic knowledge.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Rather attempting provide computer programs real{world knowledge comparable humans, natural language processing turned corpus{based methods.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	approaches use techniques statistics machine learning induce models language usage large samples text.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	models trained perform particular tasks, usually via supervised learning.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	paper describes approach decision tree learned number sentences instance ambiguous word manually annotated sense{tag denotes appropriate sense context.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Prior learning, sense{tagged corpus must converted regular form suitable automatic processing.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	sense{tagged occurrence ambiguous word converted feature vector, feature represents property surrounding text considered relevant disambiguation process.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Given exibility complexity human language, potentially in?nite set features could utilized.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	However, corpus{based approaches features usually consist information readily iden- ti?ed text, without relying extensive external knowledge sources.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	typically include part{of{speech surrounding words, presence certain key words within window context, various syntactic properties sentence ambiguous word.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	approach paper relies upon feature set made bigrams, two word sequences occur text.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	context ambiguous word occurs represented number binary features indicate whether particular bigram occurred within approximately 50 words left right word disambiguated.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	take approach since surface lexical features like bigrams, collocations, co{occurrences often contribute great deal disambiguation accuracy.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	clear much disambiguation accuracy improved use features identi?ed complex pre{processing part{of{speech tagging, parsing, anaphora resolution.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	One objectives establish clear upper bounds accuracy disambiguation using feature sets impose substantial pre{ processing requirements.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	paper continues discussion methods identifying bigrams included feature set learning.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	decision tree learning algorithm described, benchmark learning algorithms included purposes comparison.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	experimental data discussed, empirical results presented.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	close analysis ?ndings discussion related work.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	developed approach word sense disambiguation represents text entirely terms occurrence bigrams, de?ne two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation Bigram Counts consecutive words occur text.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	distributional characteristics bigrams fairly consistent across corpora; majority occur one time.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Given sparse skewed nature data, statistical methods used select interesting bigrams must carefully chosen.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	explore two alternatives, power divergence family goodness ?t statistics Dice CoeÆcient, information theoretic measure related point- wise Mutual Information.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Figure 1 summarizes notation word bigram counts used paper way 2 ? 2 contingency table.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	value n 11 shows many times bigram big cat occurs corpus.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	value n 12 shows often bigrams occur big ?rst word cat second.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	counts n +1 n 1+ indicate often words big cat occur ?rst second words bigram corpus.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	total number bigrams corpus represented n ++ . 2.1 Power Divergence Family.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	(Cressie Read, 1984) introduce power divergence family goodness ?t statistics.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	number well known statistics belong family, including likelihood ratio statisticG 2 Pearson'sX 2 statistic.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	measure divergence observed (n ij ) expected (m ij ) bigram counts, ij estimated based assumption component words bigram occur together strictly chance.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	(Dunning, 1993) argues favor G2 X2, especially dealing sparse skewed data distributions.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	However, (Cressie Read, 1984) suggest cases Pearson's statistic reliable likelihood ratio one test always preferred other.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	light this, (Pedersen, 1996) presents Fisher's exact test alternative since rely distributional assumptions underly Pearson's test likelihood ratio.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Unfortunately usually clear test appropriate particular sample data.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	take following approach, based observation tests assign approximately measure statistical signi?cance bi- gram counts contingency table violate distributional assumptions underly goodness ?t statistics.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	perform tests using X 2 , G 2 , Fisher's exact test bigram.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	resulting measures statistical signi?cance di?er, distribution bigram counts causing least one tests become unreliable.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	occurs rely upon value Fisher's exact test since makes fewer assumptions underlying distribution data.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	experiments paper, identi?ed top 100 ranked bigrams occur 5 times training corpus associated word.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	cases rankings produced G 2 , X 2 , Fisher's exact test disagreed, altogether surprising given low frequency bigrams excluded.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Since statistics produced rankings, hereafter make distinction among simply refer generically power divergence statistic.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	2.2 Dice CoeÆcient.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Dice CoeÆcient descriptive statistic provides measure association among two words corpus.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	similar pointwise Mutual Information, widely used measure ?rst introduced identifying lexical relationships (Church Hanks, 1990).	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Pointwise Mutual Information de?ned follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ w 1 w 2 represent two words make bigram.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Pointwise Mutual Information quanti?es often two words occur together bigram (the numerator) relative often occur overall corpus (the denominator).	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	However, curious limitation pointwise Mutual Information.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	bigram w 1 w 2 occurs n 11 times corpus, whose component words w 1 w 2 occur part bigram, result increasingly strong measures association value n 11 decreases.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Thus, maximum pointwise Mutual Information given corpus assigned bi- grams occur one time, whose component words never occur outside bigram.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	usually bigrams prove useful disambiguation, yet dominate ranked list determined pointwise Mutual Information.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Dice CoeÆcient overcomes limitation, de?ned follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ n 11 = n 1+ = n +1 value Dice(w 1 ; w 2 ) 1 values n 11 . value n. 11 less either marginal totals (the typical case) rankings produced Dice Co- eÆcient similar Mutual Information.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	relationship pointwise Mutual Information Dice CoeÆcient also discussed (Smadja et al., 1996).	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	developed Bigram Statistics Package produce ranked lists bigrams using range tests.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	software written Perl freely available www.d.umn.edu/~tpederse.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Decision trees among widely used machine learning algorithms.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	perform general speci?c search feature space, adding informative features tree structure search proceeds.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	objective select minimal set features eÆciently partitions feature space classes observations assemble tree.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	case, observations manually sense{tagged examples ambiguous word context partitions correspond di?erent possible senses.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	feature selected search process represented node learned decision tree.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	node represents choice point number di?erent possible values feature.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Learning continues training examples accounted decision tree.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	general, tree overly speci?c training data generalize well new examples.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Therefore learning followed pruning step nodes eliminated reorganized produce tree generalize new circumstances.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Test instances disambiguated ?nding path learned decision tree root leaf node corresponds observed features.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	instance ambiguous word dis- ambiguated passing series tests, test asks particular bigram occurs available window context.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	also include three benchmark learning algorithms study: majority classi?er, decision stump, Naive Bayesian classi?er.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	majority classi?er assigns common sense training data every instance test data.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	decision stump one node decision tree(Holte, 1993) created stopping decision tree learner single informative feature added tree.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Naive Bayesian classi?er (Duda Hart, 1973) based certain blanket assumptions interactions among features corpus.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	search feature space performed build representative model case decision trees.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Instead, features included classi- ?er assumed relevant task hand.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	assumption feature conditionally independent features, given sense ambiguous word.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	often used bag words feature set, every word training sample represented binary feature indicates whether occurs window context surrounding ambiguous word.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	use Weka (Witten Frank, 2000) implementations C4.5 decision tree learner (known J48), decision stump, Naive Bayesian classi?er.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Weka written Java freely available www.cs.waikato.ac.nz/~ml.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	empirical study utilizes training test data 1998 SENSEVAL evaluation word sense disambiguation systems.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Ten teams participated supervised learning portion event.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Additional details exercise, including data results referred paper, found SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) (Kilgarri?	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Palmer, 2000).	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	included 36 tasks SENSEVAL training test data provided.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	task requires occurrences particular word test data disambiguated based model learned sense{tagged instances training data.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	words used multiple tasks di?erent parts speech.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	example, two tasks associated bet, one use noun verb.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Thus, 36 tasks involving disambiguation 29 di?erent words.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	words part speech associated task shown Table 1 column 1.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Note parts speech encoded n noun, adjective, v verb, p words part speech provided.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	number test training instances task shown columns 2 4.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	instance consists sentence ambiguous word occurs well one two surrounding sentences.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	general total context available ambiguous word less 100 surrounding words.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	number distinct senses test data task shown column 3.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	following process repeated task.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Capitalization punctuation removed training test data.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Two feature sets selected training data based top 100 ranked bigrams according power divergence statistic Dice CoeÆcient.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	bigram must occurred 5 times included feature.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	step ?lters large number possible bi- grams allows decision tree learner focus small number candidate bigrams likely helpful disambiguation process.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	training test data converted feature vectors feature represents occurrence one bigrams belong feature set.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	representation training data actual input learning algorithms.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Decision tree decision stump learning performed twice, using feature set determined power divergence statistic using feature set identi?ed Dice CoeÆcient.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	majority classi?er simply determines frequent sense training data assigns instances test data.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Naive Bayesian classi?er based feature set every word occurs 5 times training data included feature.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	learned models used disambiguate test data.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	test data kept separate stage.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	employ ?ne grained scoring method, word counted correctly disambiguated assigned sense tag exactly matches true sense tag.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	partial credit assigned near misses.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	accuracy attained learning algorithms shown Table 1.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Column 5 reports accuracy majority classi?er, columns 6 7 show best average accuracy reported 10 participating SENSEVAL teams.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	evaluation SENSEVAL based precision recall, converted scores accuracy taking product.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	However, best precision recall may come di?erent teams, best accuracy shown column 6 may actually higher single participating SENSEVAL system.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	average accuracy column 7 product average precision recall reported participating SENSEVAL teams.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Column 8 shows accuracy decision tree using J48 learning algorithm features identi?ed power divergence statistic.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Column 10 shows accuracy decision tree Dice CoeÆcient selects features.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Columns 9 11 show accuracy decision stump based power divergence statistic Dice CoeÆcient respectively.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Finally, column 13 shows accuracy Naive Bayesian classi?er based bag words feature set.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	accurate method decision tree based feature set determined power divergence statistic.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	last line Table 1 shows win-tie-loss score decision tree/power divergence method relative every method.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	win shows accurate method column, loss means less accurate, tie means equally accurate.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	decision tree/power divergence method accurate best reported SENSEVAL results 19 36 tasks, accurate 30 36 tasks compared average reported accuracy.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	decision stumps also fared well, proving accurate best SENSEVAL results 14 36 tasks.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	general feature sets selected power divergence statistic result accurate decision trees selected Dice CoeÆcient.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	power divergence tests prove reliable since account possible events surrounding two words w 1 w 2 ; occur bigram w 1 w 2 , w 1 w 2 occurs bigram without other, bigram consists neither.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Dice CoeÆcient based strictly event w 1 w 2 occur together bigram.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	6 tasks decision tree / power divergence approach less accurate SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, sanction-p.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	dramatic difference occurred amaze-v, SENSE- VAL average 92.4% decision tree accuracy 58.6%.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	However, unusual task every instance test data belonged single sense minority sense training data.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	characteristics decision trees decision stumps learned word shown Table 2.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Column 1 shows word part speech.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Columns 2, 3, 4 based feature set selected power divergence statistic columns 5, 6, 7 based Dice CoeÆ- cient.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Columns 2 5 show node selected serve decision stump.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Columns 3 6 show number leaf nodes learned decision tree relative number total nodes.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Columns 4 7 show number bigram features selected Table 1: Experimental Results.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	table shows little di?erence decision stump nodes selected feature sets determined power divergence statistics versus Dice CoeÆcient.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	expected since top ranked bigrams measure consistent, decision stump node generally chosen among those.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	However, di?erences feature sets selected power divergence statistics Dice CoeÆcient.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	ected different sized trees learned based feature sets.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	number leaf nodes total number nodes learned tree shown columns 3 6.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	number internal nodes simply di?erence total nodes leaf nodes.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	leaf node represents end path decision tree makes sense distinction.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Since bigram feature appear decision tree, number inter- Table 2: Decision Tree Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n accident 8/15 101 accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n excess 13/25 104 excess 11/21 102 oat-n oat 7/13 13 oat 7/13 13 giant-n giants 16/31 103 giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n 1/1 7 1/1 7 promise-n promise 95/189 100 promising 49/97 107 sack-n sack 5/9 31 sack 5/9 31 scrap-n scrap 7/13 8 scrap 7/13 8 shirt-n shirt 38/75 101 shirt 55/109 101 amaze-v amazed 11/21 102 amazed 11/21 102 bet-v bet 4/7 10 bet 4/7 10 bother-v bothered 19/37 101 bothered 20/39 106 bury-v buried 28/55 103 buried 32/63 103 calculate-v calculated 5/9 103 calculated 5/9 103 consume-v 4/7 20 4/7 20 derive-v derived 10/19 104 derived 10/19 104 oat-v oated 24/47 80 oated 24/47 80 invade-v invade 55/109 107 invade 66/127 108 promise-v promise 3/5 100 promise 5/9 106 sack-v return 1/1 91 return 1/1 91 scrap-v 1/1 7 1/1 7 seize-v seize 26/51 104 seize 57/113 104 brilliant-a brilliant 26/51 101 brilliant 42/83 103 oating-a 7/13 10 7/13 10 generous-a generous 57/113 103 generous 56/111 102 giant-a giant 2/3 102 giant 1/1 101 modest-a modest 14/27 101 modest 10/19 105 slight-a slightest 2/3 105 slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band 14/27 100 band 21/41 117 bitter-p bitter 22/43 54 bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p head 90/179 100 head 81/161 105 nal nodes represents number bigram features selected decision tree learner.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	One original hypotheses accurate decision trees bigrams include relatively small number features.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	motivated success decision stumps performing disambiguation based single bigram feature.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	experiments, decision trees used bigram features identi?ed ?ltering step, many words decision tree learner went eliminate candidate features.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	seen comparing number internal nodes number candidate features shown columns 4 7.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	1 also noteworthy bigrams ultimately selected decision tree learner inclusion tree always include bigrams ranked highly power divergence statistic Dice CoeÆcient.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	expected, since selection bigrams raw text mea1 words 100 top ranked bigrams form set candidate features presented decision tree learner.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	ties top 100 rankings may 100 features, fewer 100 bi- grams occurred 5 times bigrams included feature set.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	suring association two words, decision tree seeks bigrams partition instances ambiguous word distinct senses.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	particular, decision tree learner makes decisions bigram include nodes tree using gain ratio, measure based overall Mutual Information bigram particular word sense.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Finally, note smallest decision trees functionally equivalent benchmark methods.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	decision tree 1 leaf node internal nodes (1/1) acts majority classi?er.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	decision tree 2 leaf nodes 1 internal node (2/3) structure decision stump.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	One long-term objectives identify core set features useful disambiguating wide class words using supervised unsupervised methodologies.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	presented ensemble approach word sense disambiguation (Pedersen, 2000) multiple Naive Bayesian classi?ers, based co{ occurrence features varying sized windows context, shown perform well widely studied nouns interest line.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	accuracy approach good previously published results, learned models complex diÆcult interpret, e?ect acting accurate black boxes.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	experience variations learning algorithms far less signi?cant contributors disambiguation accuracy variations feature set.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	words, informative feature set result accurate disambiguation used wide range learning algorithms, learning algorithm perform well given uninformative misleading set features.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Therefore, focus developing discovering feature sets make distinctions among word senses.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	learning algorithms must produce accurate models, also shed new light relationships among features allow us continue re?ning understanding feature sets.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	believe decision trees meet criteria.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	wide range implementations available, known robust accurate across range domains.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	important, structure easy interpret may provide insights relationships exist among features general rules disambiguation.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Bigrams used features word sense disambiguation, particularly form collocations ambiguous word one component bigram (e.g., (Bruce Wiebe, 1994), (Ng Lee, 1996), (Yarowsky, 1995)).	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	bigrams identify collocations include word disambiguated, requirement case.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Decision trees used supervised learning approaches word sense disambiguation, fared well number comparative studies (e.g., (Mooney, 1996), (Pedersen Bruce, 1997)).	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	former used bag word feature sets latter used mixed feature set included part-of-speech neighboring words, three collocations, morphology ambiguous word.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	believe approach paper ?rst time decision trees based strictly bigram features employed.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	decision list closely related approach also applied word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks Stevenson, 1998), (Yarowsky, 2000)).	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Rather building traversing tree perform disambiguation, list employed.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	general case decision list may suffer less fragmentation learning decision trees; practical matter means decision list less likely over{trained.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	However, believe fragmentation also ects feature set used learning.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	consists approximately 100 binary features.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	results relatively small feature space likely su?er fragmentation larger spaces.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	number immediate extensions work.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	?rst ease requirement bi- grams made two consecutive words.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Rather, search bigrams component words may separated words text.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	second eliminate ?ltering step candidate bigrams selected power divergence statistic.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Instead, decision tree learner would consider possible bigrams.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Despite increasing danger fragmentation, interesting issue since bigrams judged informative decision tree learner always ranked highly ?ltering step.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	particular, determine ?ltering process ever eliminates bi- grams could signi?cant sources disambiguation information.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	longer term, hope adapt approach unsupervised learning, disambiguation performed without bene?t sense tagged text.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	optimistic viable, since bigram features easy identify raw text.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	results approach compared 1998 SENSEVAL word sense disambiguation exercise show bigram based decision tree approach accurate best SENSEVAL results 19 36 words.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	Bigram Statistics Package implemented Satanjeev Banerjee, supported Grant{in{Aid Research, Artistry Scholarship OÆce Vice President Research Dean Graduate School University Minnesota.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	would like thank SENSEVAL organizers making data results 1998 event freely available.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	comments three anonymous reviewers helpful preparing ?nal version paper.	0
also obtain salient bigrams con­text, methods software de­scribed (Pedersen, 2001).	preliminary version paper appears (Pedersen, 2001).	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Decision Tree Bigrams Accurate Predictor Word Sense	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	paper presents corpus-based approach word sense disambiguation decision tree assigns sense ambiguous word based bigrams occur nearby.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	approach evaluated using sense-tagged corpora 1998 SENSEVAL word sense disambiguation exercise.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	accurate average results reported 30 36 words, accurate best results 19 36 words.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Word sense disambiguation process selecting appropriate meaning word, based context occurs.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	purposes assumed set possible meanings, i.e., sense inventory, already determined.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	example, suppose bill following set possible meanings: piece currency, pending legislation, bird jaw.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	used context Senate bill consideration, human reader immediately understands bill used legislative sense.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	However, computer program attempting perform task faces diÆcult problem since bene?t innate common{sense linguistic knowledge.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Rather attempting provide computer programs real{world knowledge comparable humans, natural language processing turned corpus{based methods.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	approaches use techniques statistics machine learning induce models language usage large samples text.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	models trained perform particular tasks, usually via supervised learning.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	paper describes approach decision tree learned number sentences instance ambiguous word manually annotated sense{tag denotes appropriate sense context.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Prior learning, sense{tagged corpus must converted regular form suitable automatic processing.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	sense{tagged occurrence ambiguous word converted feature vector, feature represents property surrounding text considered relevant disambiguation process.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Given exibility complexity human language, potentially in?nite set features could utilized.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	However, corpus{based approaches features usually consist information readily iden- ti?ed text, without relying extensive external knowledge sources.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	typically include part{of{speech surrounding words, presence certain key words within window context, various syntactic properties sentence ambiguous word.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	approach paper relies upon feature set made bigrams, two word sequences occur text.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	context ambiguous word occurs represented number binary features indicate whether particular bigram occurred within approximately 50 words left right word disambiguated.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	take approach since surface lexical features like bigrams, collocations, co{occurrences often contribute great deal disambiguation accuracy.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	clear much disambiguation accuracy improved use features identi?ed complex pre{processing part{of{speech tagging, parsing, anaphora resolution.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	One objectives establish clear upper bounds accuracy disambiguation using feature sets impose substantial pre{ processing requirements.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	paper continues discussion methods identifying bigrams included feature set learning.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	decision tree learning algorithm described, benchmark learning algorithms included purposes comparison.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	experimental data discussed, empirical results presented.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	close analysis ?ndings discussion related work.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	developed approach word sense disambiguation represents text entirely terms occurrence bigrams, de?ne two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation Bigram Counts consecutive words occur text.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	distributional characteristics bigrams fairly consistent across corpora; majority occur one time.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Given sparse skewed nature data, statistical methods used select interesting bigrams must carefully chosen.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	explore two alternatives, power divergence family goodness ?t statistics Dice CoeÆcient, information theoretic measure related point- wise Mutual Information.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Figure 1 summarizes notation word bigram counts used paper way 2 ? 2 contingency table.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	value n 11 shows many times bigram big cat occurs corpus.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	value n 12 shows often bigrams occur big ?rst word cat second.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	counts n +1 n 1+ indicate often words big cat occur ?rst second words bigram corpus.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	total number bigrams corpus represented n ++ . 2.1 Power Divergence Family.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	(Cressie Read, 1984) introduce power divergence family goodness ?t statistics.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	number well known statistics belong family, including likelihood ratio statisticG 2 Pearson'sX 2 statistic.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	measure divergence observed (n ij ) expected (m ij ) bigram counts, ij estimated based assumption component words bigram occur together strictly chance.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	(Dunning, 1993) argues favor G2 X2, especially dealing sparse skewed data distributions.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	However, (Cressie Read, 1984) suggest cases Pearson's statistic reliable likelihood ratio one test always preferred other.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	light this, (Pedersen, 1996) presents Fisher's exact test alternative since rely distributional assumptions underly Pearson's test likelihood ratio.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Unfortunately usually clear test appropriate particular sample data.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	take following approach, based observation tests assign approximately measure statistical signi?cance bi- gram counts contingency table violate distributional assumptions underly goodness ?t statistics.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	perform tests using X 2 , G 2 , Fisher's exact test bigram.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	resulting measures statistical signi?cance di?er, distribution bigram counts causing least one tests become unreliable.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	occurs rely upon value Fisher's exact test since makes fewer assumptions underlying distribution data.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	experiments paper, identi?ed top 100 ranked bigrams occur 5 times training corpus associated word.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	cases rankings produced G 2 , X 2 , Fisher's exact test disagreed, altogether surprising given low frequency bigrams excluded.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Since statistics produced rankings, hereafter make distinction among simply refer generically power divergence statistic.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	2.2 Dice CoeÆcient.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Dice CoeÆcient descriptive statistic provides measure association among two words corpus.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	similar pointwise Mutual Information, widely used measure ?rst introduced identifying lexical relationships (Church Hanks, 1990).	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Pointwise Mutual Information de?ned follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ w 1 w 2 represent two words make bigram.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Pointwise Mutual Information quanti?es often two words occur together bigram (the numerator) relative often occur overall corpus (the denominator).	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	However, curious limitation pointwise Mutual Information.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	bigram w 1 w 2 occurs n 11 times corpus, whose component words w 1 w 2 occur part bigram, result increasingly strong measures association value n 11 decreases.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Thus, maximum pointwise Mutual Information given corpus assigned bi- grams occur one time, whose component words never occur outside bigram.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	usually bigrams prove useful disambiguation, yet dominate ranked list determined pointwise Mutual Information.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Dice CoeÆcient overcomes limitation, de?ned follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ n 11 = n 1+ = n +1 value Dice(w 1 ; w 2 ) 1 values n 11 . value n. 11 less either marginal totals (the typical case) rankings produced Dice Co- eÆcient similar Mutual Information.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	relationship pointwise Mutual Information Dice CoeÆcient also discussed (Smadja et al., 1996).	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	developed Bigram Statistics Package produce ranked lists bigrams using range tests.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	software written Perl freely available www.d.umn.edu/~tpederse.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Decision trees among widely used machine learning algorithms.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	perform general speci?c search feature space, adding informative features tree structure search proceeds.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	objective select minimal set features eÆciently partitions feature space classes observations assemble tree.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	case, observations manually sense{tagged examples ambiguous word context partitions correspond di?erent possible senses.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	feature selected search process represented node learned decision tree.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	node represents choice point number di?erent possible values feature.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Learning continues training examples accounted decision tree.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	general, tree overly speci?c training data generalize well new examples.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Therefore learning followed pruning step nodes eliminated reorganized produce tree generalize new circumstances.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Test instances disambiguated ?nding path learned decision tree root leaf node corresponds observed features.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	instance ambiguous word dis- ambiguated passing series tests, test asks particular bigram occurs available window context.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	also include three benchmark learning algorithms study: majority classi?er, decision stump, Naive Bayesian classi?er.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	majority classi?er assigns common sense training data every instance test data.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	decision stump one node decision tree(Holte, 1993) created stopping decision tree learner single informative feature added tree.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Naive Bayesian classi?er (Duda Hart, 1973) based certain blanket assumptions interactions among features corpus.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	search feature space performed build representative model case decision trees.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Instead, features included classi- ?er assumed relevant task hand.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	assumption feature conditionally independent features, given sense ambiguous word.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	often used bag words feature set, every word training sample represented binary feature indicates whether occurs window context surrounding ambiguous word.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	use Weka (Witten Frank, 2000) implementations C4.5 decision tree learner (known J48), decision stump, Naive Bayesian classi?er.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Weka written Java freely available www.cs.waikato.ac.nz/~ml.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	empirical study utilizes training test data 1998 SENSEVAL evaluation word sense disambiguation systems.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Ten teams participated supervised learning portion event.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Additional details exercise, including data results referred paper, found SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) (Kilgarri?	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Palmer, 2000).	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	included 36 tasks SENSEVAL training test data provided.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	task requires occurrences particular word test data disambiguated based model learned sense{tagged instances training data.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	words used multiple tasks di?erent parts speech.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	example, two tasks associated bet, one use noun verb.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Thus, 36 tasks involving disambiguation 29 di?erent words.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	words part speech associated task shown Table 1 column 1.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Note parts speech encoded n noun, adjective, v verb, p words part speech provided.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	number test training instances task shown columns 2 4.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	instance consists sentence ambiguous word occurs well one two surrounding sentences.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	general total context available ambiguous word less 100 surrounding words.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	number distinct senses test data task shown column 3.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	following process repeated task.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Capitalization punctuation removed training test data.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Two feature sets selected training data based top 100 ranked bigrams according power divergence statistic Dice CoeÆcient.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	bigram must occurred 5 times included feature.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	step ?lters large number possible bi- grams allows decision tree learner focus small number candidate bigrams likely helpful disambiguation process.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	training test data converted feature vectors feature represents occurrence one bigrams belong feature set.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	representation training data actual input learning algorithms.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Decision tree decision stump learning performed twice, using feature set determined power divergence statistic using feature set identi?ed Dice CoeÆcient.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	majority classi?er simply determines frequent sense training data assigns instances test data.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Naive Bayesian classi?er based feature set every word occurs 5 times training data included feature.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	learned models used disambiguate test data.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	test data kept separate stage.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	employ ?ne grained scoring method, word counted correctly disambiguated assigned sense tag exactly matches true sense tag.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	partial credit assigned near misses.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	accuracy attained learning algorithms shown Table 1.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Column 5 reports accuracy majority classi?er, columns 6 7 show best average accuracy reported 10 participating SENSEVAL teams.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	evaluation SENSEVAL based precision recall, converted scores accuracy taking product.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	However, best precision recall may come di?erent teams, best accuracy shown column 6 may actually higher single participating SENSEVAL system.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	average accuracy column 7 product average precision recall reported participating SENSEVAL teams.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Column 8 shows accuracy decision tree using J48 learning algorithm features identi?ed power divergence statistic.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Column 10 shows accuracy decision tree Dice CoeÆcient selects features.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Columns 9 11 show accuracy decision stump based power divergence statistic Dice CoeÆcient respectively.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Finally, column 13 shows accuracy Naive Bayesian classi?er based bag words feature set.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	accurate method decision tree based feature set determined power divergence statistic.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	last line Table 1 shows win-tie-loss score decision tree/power divergence method relative every method.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	win shows accurate method column, loss means less accurate, tie means equally accurate.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	decision tree/power divergence method accurate best reported SENSEVAL results 19 36 tasks, accurate 30 36 tasks compared average reported accuracy.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	decision stumps also fared well, proving accurate best SENSEVAL results 14 36 tasks.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	general feature sets selected power divergence statistic result accurate decision trees selected Dice CoeÆcient.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	power divergence tests prove reliable since account possible events surrounding two words w 1 w 2 ; occur bigram w 1 w 2 , w 1 w 2 occurs bigram without other, bigram consists neither.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Dice CoeÆcient based strictly event w 1 w 2 occur together bigram.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	6 tasks decision tree / power divergence approach less accurate SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, sanction-p.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	dramatic difference occurred amaze-v, SENSE- VAL average 92.4% decision tree accuracy 58.6%.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	However, unusual task every instance test data belonged single sense minority sense training data.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	characteristics decision trees decision stumps learned word shown Table 2.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Column 1 shows word part speech.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Columns 2, 3, 4 based feature set selected power divergence statistic columns 5, 6, 7 based Dice CoeÆ- cient.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Columns 2 5 show node selected serve decision stump.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Columns 3 6 show number leaf nodes learned decision tree relative number total nodes.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Columns 4 7 show number bigram features selected Table 1: Experimental Results.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	table shows little di?erence decision stump nodes selected feature sets determined power divergence statistics versus Dice CoeÆcient.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	expected since top ranked bigrams measure consistent, decision stump node generally chosen among those.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	However, di?erences feature sets selected power divergence statistics Dice CoeÆcient.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	ected different sized trees learned based feature sets.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	number leaf nodes total number nodes learned tree shown columns 3 6.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	number internal nodes simply di?erence total nodes leaf nodes.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	leaf node represents end path decision tree makes sense distinction.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Since bigram feature appear decision tree, number inter- Table 2: Decision Tree Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n accident 8/15 101 accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n excess 13/25 104 excess 11/21 102 oat-n oat 7/13 13 oat 7/13 13 giant-n giants 16/31 103 giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n 1/1 7 1/1 7 promise-n promise 95/189 100 promising 49/97 107 sack-n sack 5/9 31 sack 5/9 31 scrap-n scrap 7/13 8 scrap 7/13 8 shirt-n shirt 38/75 101 shirt 55/109 101 amaze-v amazed 11/21 102 amazed 11/21 102 bet-v bet 4/7 10 bet 4/7 10 bother-v bothered 19/37 101 bothered 20/39 106 bury-v buried 28/55 103 buried 32/63 103 calculate-v calculated 5/9 103 calculated 5/9 103 consume-v 4/7 20 4/7 20 derive-v derived 10/19 104 derived 10/19 104 oat-v oated 24/47 80 oated 24/47 80 invade-v invade 55/109 107 invade 66/127 108 promise-v promise 3/5 100 promise 5/9 106 sack-v return 1/1 91 return 1/1 91 scrap-v 1/1 7 1/1 7 seize-v seize 26/51 104 seize 57/113 104 brilliant-a brilliant 26/51 101 brilliant 42/83 103 oating-a 7/13 10 7/13 10 generous-a generous 57/113 103 generous 56/111 102 giant-a giant 2/3 102 giant 1/1 101 modest-a modest 14/27 101 modest 10/19 105 slight-a slightest 2/3 105 slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band 14/27 100 band 21/41 117 bitter-p bitter 22/43 54 bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p head 90/179 100 head 81/161 105 nal nodes represents number bigram features selected decision tree learner.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	One original hypotheses accurate decision trees bigrams include relatively small number features.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	motivated success decision stumps performing disambiguation based single bigram feature.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	experiments, decision trees used bigram features identi?ed ?ltering step, many words decision tree learner went eliminate candidate features.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	seen comparing number internal nodes number candidate features shown columns 4 7.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	1 also noteworthy bigrams ultimately selected decision tree learner inclusion tree always include bigrams ranked highly power divergence statistic Dice CoeÆcient.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	expected, since selection bigrams raw text mea1 words 100 top ranked bigrams form set candidate features presented decision tree learner.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	ties top 100 rankings may 100 features, fewer 100 bi- grams occurred 5 times bigrams included feature set.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	suring association two words, decision tree seeks bigrams partition instances ambiguous word distinct senses.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	particular, decision tree learner makes decisions bigram include nodes tree using gain ratio, measure based overall Mutual Information bigram particular word sense.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Finally, note smallest decision trees functionally equivalent benchmark methods.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	decision tree 1 leaf node internal nodes (1/1) acts majority classi?er.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	decision tree 2 leaf nodes 1 internal node (2/3) structure decision stump.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	One long-term objectives identify core set features useful disambiguating wide class words using supervised unsupervised methodologies.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	presented ensemble approach word sense disambiguation (Pedersen, 2000) multiple Naive Bayesian classi?ers, based co{ occurrence features varying sized windows context, shown perform well widely studied nouns interest line.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	accuracy approach good previously published results, learned models complex diÆcult interpret, e?ect acting accurate black boxes.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	experience variations learning algorithms far less signi?cant contributors disambiguation accuracy variations feature set.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	words, informative feature set result accurate disambiguation used wide range learning algorithms, learning algorithm perform well given uninformative misleading set features.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Therefore, focus developing discovering feature sets make distinctions among word senses.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	learning algorithms must produce accurate models, also shed new light relationships among features allow us continue re?ning understanding feature sets.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	believe decision trees meet criteria.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	wide range implementations available, known robust accurate across range domains.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	important, structure easy interpret may provide insights relationships exist among features general rules disambiguation.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Bigrams used features word sense disambiguation, particularly form collocations ambiguous word one component bigram (e.g., (Bruce Wiebe, 1994), (Ng Lee, 1996), (Yarowsky, 1995)).	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	bigrams identify collocations include word disambiguated, requirement case.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Decision trees used supervised learning approaches word sense disambiguation, fared well number comparative studies (e.g., (Mooney, 1996), (Pedersen Bruce, 1997)).	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	former used bag word feature sets latter used mixed feature set included part-of-speech neighboring words, three collocations, morphology ambiguous word.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	believe approach paper ?rst time decision trees based strictly bigram features employed.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	decision list closely related approach also applied word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks Stevenson, 1998), (Yarowsky, 2000)).	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Rather building traversing tree perform disambiguation, list employed.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	general case decision list may suffer less fragmentation learning decision trees; practical matter means decision list less likely over{trained.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	However, believe fragmentation also ects feature set used learning.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	consists approximately 100 binary features.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	results relatively small feature space likely su?er fragmentation larger spaces.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	number immediate extensions work.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	?rst ease requirement bi- grams made two consecutive words.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Rather, search bigrams component words may separated words text.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	second eliminate ?ltering step candidate bigrams selected power divergence statistic.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Instead, decision tree learner would consider possible bigrams.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Despite increasing danger fragmentation, interesting issue since bigrams judged informative decision tree learner always ranked highly ?ltering step.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	particular, determine ?ltering process ever eliminates bi- grams could signi?cant sources disambiguation information.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	longer term, hope adapt approach unsupervised learning, disambiguation performed without bene?t sense tagged text.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	optimistic viable, since bigram features easy identify raw text.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation.	1
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	results approach compared 1998 SENSEVAL word sense disambiguation exercise show bigram based decision tree approach accurate best SENSEVAL results 19 36 words.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	Bigram Statistics Package implemented Satanjeev Banerjee, supported Grant{in{Aid Research, Artistry Scholarship OÆce Vice President Research Dean Graduate School University Minnesota.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	would like thank SENSEVAL organizers making data results 1998 event freely available.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	comments three anonymous reviewers helpful preparing ?nal version paper.	0
fact, Pedersen (2001) found bigrams alone e.ective features word sense disambiguation.	preliminary version paper appears (Pedersen, 2001).	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Decision Tree Bigrams Accurate Predictor Word Sense	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	paper presents corpus-based approach word sense disambiguation decision tree assigns sense ambiguous word based bigrams occur nearby.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	approach evaluated using sense-tagged corpora 1998 SENSEVAL word sense disambiguation exercise.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	accurate average results reported 30 36 words, accurate best results 19 36 words.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Word sense disambiguation process selecting appropriate meaning word, based context occurs.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	purposes assumed set possible meanings, i.e., sense inventory, already determined.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	example, suppose bill following set possible meanings: piece currency, pending legislation, bird jaw.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	used context Senate bill consideration, human reader immediately understands bill used legislative sense.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	However, computer program attempting perform task faces diÆcult problem since bene?t innate common{sense linguistic knowledge.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Rather attempting provide computer programs real{world knowledge comparable humans, natural language processing turned corpus{based methods.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	approaches use techniques statistics machine learning induce models language usage large samples text.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	models trained perform particular tasks, usually via supervised learning.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	paper describes approach decision tree learned number sentences instance ambiguous word manually annotated sense{tag denotes appropriate sense context.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Prior learning, sense{tagged corpus must converted regular form suitable automatic processing.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	sense{tagged occurrence ambiguous word converted feature vector, feature represents property surrounding text considered relevant disambiguation process.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Given exibility complexity human language, potentially in?nite set features could utilized.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	However, corpus{based approaches features usually consist information readily iden- ti?ed text, without relying extensive external knowledge sources.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	typically include part{of{speech surrounding words, presence certain key words within window context, various syntactic properties sentence ambiguous word.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	approach paper relies upon feature set made bigrams, two word sequences occur text.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	context ambiguous word occurs represented number binary features indicate whether particular bigram occurred within approximately 50 words left right word disambiguated.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	take approach since surface lexical features like bigrams, collocations, co{occurrences often contribute great deal disambiguation accuracy.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	clear much disambiguation accuracy improved use features identi?ed complex pre{processing part{of{speech tagging, parsing, anaphora resolution.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	One objectives establish clear upper bounds accuracy disambiguation using feature sets impose substantial pre{ processing requirements.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	paper continues discussion methods identifying bigrams included feature set learning.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	decision tree learning algorithm described, benchmark learning algorithms included purposes comparison.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	experimental data discussed, empirical results presented.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	close analysis ?ndings discussion related work.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	developed approach word sense disambiguation represents text entirely terms occurrence bigrams, de?ne two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation Bigram Counts consecutive words occur text.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	distributional characteristics bigrams fairly consistent across corpora; majority occur one time.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Given sparse skewed nature data, statistical methods used select interesting bigrams must carefully chosen.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	explore two alternatives, power divergence family goodness ?t statistics Dice CoeÆcient, information theoretic measure related point- wise Mutual Information.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Figure 1 summarizes notation word bigram counts used paper way 2 ? 2 contingency table.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	value n 11 shows many times bigram big cat occurs corpus.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	value n 12 shows often bigrams occur big ?rst word cat second.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	counts n +1 n 1+ indicate often words big cat occur ?rst second words bigram corpus.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	total number bigrams corpus represented n ++ . 2.1 Power Divergence Family.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	(Cressie Read, 1984) introduce power divergence family goodness ?t statistics.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	number well known statistics belong family, including likelihood ratio statisticG 2 Pearson'sX 2 statistic.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	measure divergence observed (n ij ) expected (m ij ) bigram counts, ij estimated based assumption component words bigram occur together strictly chance.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	(Dunning, 1993) argues favor G2 X2, especially dealing sparse skewed data distributions.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	However, (Cressie Read, 1984) suggest cases Pearson's statistic reliable likelihood ratio one test always preferred other.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	light this, (Pedersen, 1996) presents Fisher's exact test alternative since rely distributional assumptions underly Pearson's test likelihood ratio.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Unfortunately usually clear test appropriate particular sample data.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	take following approach, based observation tests assign approximately measure statistical signi?cance bi- gram counts contingency table violate distributional assumptions underly goodness ?t statistics.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	perform tests using X 2 , G 2 , Fisher's exact test bigram.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	resulting measures statistical signi?cance di?er, distribution bigram counts causing least one tests become unreliable.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	occurs rely upon value Fisher's exact test since makes fewer assumptions underlying distribution data.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	experiments paper, identi?ed top 100 ranked bigrams occur 5 times training corpus associated word.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	cases rankings produced G 2 , X 2 , Fisher's exact test disagreed, altogether surprising given low frequency bigrams excluded.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Since statistics produced rankings, hereafter make distinction among simply refer generically power divergence statistic.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	2.2 Dice CoeÆcient.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Dice CoeÆcient descriptive statistic provides measure association among two words corpus.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	similar pointwise Mutual Information, widely used measure ?rst introduced identifying lexical relationships (Church Hanks, 1990).	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Pointwise Mutual Information de?ned follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ w 1 w 2 represent two words make bigram.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Pointwise Mutual Information quanti?es often two words occur together bigram (the numerator) relative often occur overall corpus (the denominator).	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	However, curious limitation pointwise Mutual Information.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	bigram w 1 w 2 occurs n 11 times corpus, whose component words w 1 w 2 occur part bigram, result increasingly strong measures association value n 11 decreases.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Thus, maximum pointwise Mutual Information given corpus assigned bi- grams occur one time, whose component words never occur outside bigram.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	usually bigrams prove useful disambiguation, yet dominate ranked list determined pointwise Mutual Information.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Dice CoeÆcient overcomes limitation, de?ned follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ n 11 = n 1+ = n +1 value Dice(w 1 ; w 2 ) 1 values n 11 . value n. 11 less either marginal totals (the typical case) rankings produced Dice Co- eÆcient similar Mutual Information.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	relationship pointwise Mutual Information Dice CoeÆcient also discussed (Smadja et al., 1996).	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	developed Bigram Statistics Package produce ranked lists bigrams using range tests.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	software written Perl freely available www.d.umn.edu/~tpederse.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Decision trees among widely used machine learning algorithms.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	perform general speci?c search feature space, adding informative features tree structure search proceeds.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	objective select minimal set features eÆciently partitions feature space classes observations assemble tree.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	case, observations manually sense{tagged examples ambiguous word context partitions correspond di?erent possible senses.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	feature selected search process represented node learned decision tree.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	node represents choice point number di?erent possible values feature.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Learning continues training examples accounted decision tree.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	general, tree overly speci?c training data generalize well new examples.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Therefore learning followed pruning step nodes eliminated reorganized produce tree generalize new circumstances.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Test instances disambiguated ?nding path learned decision tree root leaf node corresponds observed features.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	instance ambiguous word dis- ambiguated passing series tests, test asks particular bigram occurs available window context.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	also include three benchmark learning algorithms study: majority classi?er, decision stump, Naive Bayesian classi?er.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	majority classi?er assigns common sense training data every instance test data.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	decision stump one node decision tree(Holte, 1993) created stopping decision tree learner single informative feature added tree.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Naive Bayesian classi?er (Duda Hart, 1973) based certain blanket assumptions interactions among features corpus.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	search feature space performed build representative model case decision trees.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Instead, features included classi- ?er assumed relevant task hand.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	assumption feature conditionally independent features, given sense ambiguous word.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	often used bag words feature set, every word training sample represented binary feature indicates whether occurs window context surrounding ambiguous word.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	use Weka (Witten Frank, 2000) implementations C4.5 decision tree learner (known J48), decision stump, Naive Bayesian classi?er.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Weka written Java freely available www.cs.waikato.ac.nz/~ml.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	empirical study utilizes training test data 1998 SENSEVAL evaluation word sense disambiguation systems.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Ten teams participated supervised learning portion event.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Additional details exercise, including data results referred paper, found SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) (Kilgarri?	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Palmer, 2000).	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	included 36 tasks SENSEVAL training test data provided.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	task requires occurrences particular word test data disambiguated based model learned sense{tagged instances training data.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	words used multiple tasks di?erent parts speech.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	example, two tasks associated bet, one use noun verb.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Thus, 36 tasks involving disambiguation 29 di?erent words.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	words part speech associated task shown Table 1 column 1.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Note parts speech encoded n noun, adjective, v verb, p words part speech provided.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	number test training instances task shown columns 2 4.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	instance consists sentence ambiguous word occurs well one two surrounding sentences.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	general total context available ambiguous word less 100 surrounding words.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	number distinct senses test data task shown column 3.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	following process repeated task.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Capitalization punctuation removed training test data.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Two feature sets selected training data based top 100 ranked bigrams according power divergence statistic Dice CoeÆcient.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	bigram must occurred 5 times included feature.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	step ?lters large number possible bi- grams allows decision tree learner focus small number candidate bigrams likely helpful disambiguation process.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	training test data converted feature vectors feature represents occurrence one bigrams belong feature set.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	representation training data actual input learning algorithms.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Decision tree decision stump learning performed twice, using feature set determined power divergence statistic using feature set identi?ed Dice CoeÆcient.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	majority classi?er simply determines frequent sense training data assigns instances test data.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Naive Bayesian classi?er based feature set every word occurs 5 times training data included feature.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	learned models used disambiguate test data.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	test data kept separate stage.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	employ ?ne grained scoring method, word counted correctly disambiguated assigned sense tag exactly matches true sense tag.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	partial credit assigned near misses.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	accuracy attained learning algorithms shown Table 1.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Column 5 reports accuracy majority classi?er, columns 6 7 show best average accuracy reported 10 participating SENSEVAL teams.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	evaluation SENSEVAL based precision recall, converted scores accuracy taking product.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	However, best precision recall may come di?erent teams, best accuracy shown column 6 may actually higher single participating SENSEVAL system.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	average accuracy column 7 product average precision recall reported participating SENSEVAL teams.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Column 8 shows accuracy decision tree using J48 learning algorithm features identi?ed power divergence statistic.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Column 10 shows accuracy decision tree Dice CoeÆcient selects features.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Columns 9 11 show accuracy decision stump based power divergence statistic Dice CoeÆcient respectively.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Finally, column 13 shows accuracy Naive Bayesian classi?er based bag words feature set.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	accurate method decision tree based feature set determined power divergence statistic.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	last line Table 1 shows win-tie-loss score decision tree/power divergence method relative every method.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	win shows accurate method column, loss means less accurate, tie means equally accurate.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	decision tree/power divergence method accurate best reported SENSEVAL results 19 36 tasks, accurate 30 36 tasks compared average reported accuracy.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	decision stumps also fared well, proving accurate best SENSEVAL results 14 36 tasks.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	general feature sets selected power divergence statistic result accurate decision trees selected Dice CoeÆcient.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	power divergence tests prove reliable since account possible events surrounding two words w 1 w 2 ; occur bigram w 1 w 2 , w 1 w 2 occurs bigram without other, bigram consists neither.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Dice CoeÆcient based strictly event w 1 w 2 occur together bigram.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	6 tasks decision tree / power divergence approach less accurate SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, sanction-p.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	dramatic difference occurred amaze-v, SENSE- VAL average 92.4% decision tree accuracy 58.6%.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	However, unusual task every instance test data belonged single sense minority sense training data.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	characteristics decision trees decision stumps learned word shown Table 2.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Column 1 shows word part speech.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Columns 2, 3, 4 based feature set selected power divergence statistic columns 5, 6, 7 based Dice CoeÆ- cient.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Columns 2 5 show node selected serve decision stump.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Columns 3 6 show number leaf nodes learned decision tree relative number total nodes.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Columns 4 7 show number bigram features selected Table 1: Experimental Results.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	table shows little di?erence decision stump nodes selected feature sets determined power divergence statistics versus Dice CoeÆcient.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	expected since top ranked bigrams measure consistent, decision stump node generally chosen among those.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	However, di?erences feature sets selected power divergence statistics Dice CoeÆcient.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	ected different sized trees learned based feature sets.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	number leaf nodes total number nodes learned tree shown columns 3 6.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	number internal nodes simply di?erence total nodes leaf nodes.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	leaf node represents end path decision tree makes sense distinction.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Since bigram feature appear decision tree, number inter- Table 2: Decision Tree Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n accident 8/15 101 accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n excess 13/25 104 excess 11/21 102 oat-n oat 7/13 13 oat 7/13 13 giant-n giants 16/31 103 giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n 1/1 7 1/1 7 promise-n promise 95/189 100 promising 49/97 107 sack-n sack 5/9 31 sack 5/9 31 scrap-n scrap 7/13 8 scrap 7/13 8 shirt-n shirt 38/75 101 shirt 55/109 101 amaze-v amazed 11/21 102 amazed 11/21 102 bet-v bet 4/7 10 bet 4/7 10 bother-v bothered 19/37 101 bothered 20/39 106 bury-v buried 28/55 103 buried 32/63 103 calculate-v calculated 5/9 103 calculated 5/9 103 consume-v 4/7 20 4/7 20 derive-v derived 10/19 104 derived 10/19 104 oat-v oated 24/47 80 oated 24/47 80 invade-v invade 55/109 107 invade 66/127 108 promise-v promise 3/5 100 promise 5/9 106 sack-v return 1/1 91 return 1/1 91 scrap-v 1/1 7 1/1 7 seize-v seize 26/51 104 seize 57/113 104 brilliant-a brilliant 26/51 101 brilliant 42/83 103 oating-a 7/13 10 7/13 10 generous-a generous 57/113 103 generous 56/111 102 giant-a giant 2/3 102 giant 1/1 101 modest-a modest 14/27 101 modest 10/19 105 slight-a slightest 2/3 105 slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band 14/27 100 band 21/41 117 bitter-p bitter 22/43 54 bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p head 90/179 100 head 81/161 105 nal nodes represents number bigram features selected decision tree learner.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	One original hypotheses accurate decision trees bigrams include relatively small number features.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	motivated success decision stumps performing disambiguation based single bigram feature.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	experiments, decision trees used bigram features identi?ed ?ltering step, many words decision tree learner went eliminate candidate features.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	seen comparing number internal nodes number candidate features shown columns 4 7.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	1 also noteworthy bigrams ultimately selected decision tree learner inclusion tree always include bigrams ranked highly power divergence statistic Dice CoeÆcient.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	expected, since selection bigrams raw text mea1 words 100 top ranked bigrams form set candidate features presented decision tree learner.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	ties top 100 rankings may 100 features, fewer 100 bi- grams occurred 5 times bigrams included feature set.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	suring association two words, decision tree seeks bigrams partition instances ambiguous word distinct senses.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	particular, decision tree learner makes decisions bigram include nodes tree using gain ratio, measure based overall Mutual Information bigram particular word sense.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Finally, note smallest decision trees functionally equivalent benchmark methods.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	decision tree 1 leaf node internal nodes (1/1) acts majority classi?er.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	decision tree 2 leaf nodes 1 internal node (2/3) structure decision stump.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	One long-term objectives identify core set features useful disambiguating wide class words using supervised unsupervised methodologies.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	presented ensemble approach word sense disambiguation (Pedersen, 2000) multiple Naive Bayesian classi?ers, based co{ occurrence features varying sized windows context, shown perform well widely studied nouns interest line.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	accuracy approach good previously published results, learned models complex diÆcult interpret, e?ect acting accurate black boxes.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	experience variations learning algorithms far less signi?cant contributors disambiguation accuracy variations feature set.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	words, informative feature set result accurate disambiguation used wide range learning algorithms, learning algorithm perform well given uninformative misleading set features.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Therefore, focus developing discovering feature sets make distinctions among word senses.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	learning algorithms must produce accurate models, also shed new light relationships among features allow us continue re?ning understanding feature sets.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	believe decision trees meet criteria.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	wide range implementations available, known robust accurate across range domains.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	important, structure easy interpret may provide insights relationships exist among features general rules disambiguation.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Bigrams used features word sense disambiguation, particularly form collocations ambiguous word one component bigram (e.g., (Bruce Wiebe, 1994), (Ng Lee, 1996), (Yarowsky, 1995)).	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	bigrams identify collocations include word disambiguated, requirement case.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Decision trees used supervised learning approaches word sense disambiguation, fared well number comparative studies (e.g., (Mooney, 1996), (Pedersen Bruce, 1997)).	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	former used bag word feature sets latter used mixed feature set included part-of-speech neighboring words, three collocations, morphology ambiguous word.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	believe approach paper ?rst time decision trees based strictly bigram features employed.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	decision list closely related approach also applied word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks Stevenson, 1998), (Yarowsky, 2000)).	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Rather building traversing tree perform disambiguation, list employed.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	general case decision list may suffer less fragmentation learning decision trees; practical matter means decision list less likely over{trained.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	However, believe fragmentation also ects feature set used learning.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	consists approximately 100 binary features.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	results relatively small feature space likely su?er fragmentation larger spaces.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	number immediate extensions work.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	?rst ease requirement bi- grams made two consecutive words.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Rather, search bigrams component words may separated words text.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	second eliminate ?ltering step candidate bigrams selected power divergence statistic.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Instead, decision tree learner would consider possible bigrams.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Despite increasing danger fragmentation, interesting issue since bigrams judged informative decision tree learner always ranked highly ?ltering step.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	particular, determine ?ltering process ever eliminates bi- grams could signi?cant sources disambiguation information.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	longer term, hope adapt approach unsupervised learning, disambiguation performed without bene?t sense tagged text.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	optimistic viable, since bigram features easy identify raw text.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation.	1
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	results approach compared 1998 SENSEVAL word sense disambiguation exercise show bigram based decision tree approach accurate best SENSEVAL results 19 36 words.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	Bigram Statistics Package implemented Satanjeev Banerjee, supported Grant{in{Aid Research, Artistry Scholarship OÆce Vice President Research Dean Graduate School University Minnesota.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	would like thank SENSEVAL organizers making data results 1998 event freely available.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	comments three anonymous reviewers helpful preparing ?nal version paper.	0
Bigrams recently shown successful features supervised word sense disambiguation (Peder­sen, 2001).	preliminary version paper appears (Pedersen, 2001).	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Decision Tree Bigrams Accurate Predictor Word Sense	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	paper presents corpus-based approach word sense disambiguation decision tree assigns sense ambiguous word based bigrams occur nearby.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	approach evaluated using sense-tagged corpora 1998 SENSEVAL word sense disambiguation exercise.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	accurate average results reported 30 36 words, accurate best results 19 36 words.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Word sense disambiguation process selecting appropriate meaning word, based context occurs.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	purposes assumed set possible meanings, i.e., sense inventory, already determined.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	example, suppose bill following set possible meanings: piece currency, pending legislation, bird jaw.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	used context Senate bill consideration, human reader immediately understands bill used legislative sense.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	However, computer program attempting perform task faces diÆcult problem since bene?t innate common{sense linguistic knowledge.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Rather attempting provide computer programs real{world knowledge comparable humans, natural language processing turned corpus{based methods.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	approaches use techniques statistics machine learning induce models language usage large samples text.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	models trained perform particular tasks, usually via supervised learning.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	paper describes approach decision tree learned number sentences instance ambiguous word manually annotated sense{tag denotes appropriate sense context.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Prior learning, sense{tagged corpus must converted regular form suitable automatic processing.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	sense{tagged occurrence ambiguous word converted feature vector, feature represents property surrounding text considered relevant disambiguation process.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Given exibility complexity human language, potentially in?nite set features could utilized.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	However, corpus{based approaches features usually consist information readily iden- ti?ed text, without relying extensive external knowledge sources.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	typically include part{of{speech surrounding words, presence certain key words within window context, various syntactic properties sentence ambiguous word.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	approach paper relies upon feature set made bigrams, two word sequences occur text.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	context ambiguous word occurs represented number binary features indicate whether particular bigram occurred within approximately 50 words left right word disambiguated.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	take approach since surface lexical features like bigrams, collocations, co{occurrences often contribute great deal disambiguation accuracy.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	clear much disambiguation accuracy improved use features identi?ed complex pre{processing part{of{speech tagging, parsing, anaphora resolution.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	One objectives establish clear upper bounds accuracy disambiguation using feature sets impose substantial pre{ processing requirements.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	paper continues discussion methods identifying bigrams included feature set learning.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	decision tree learning algorithm described, benchmark learning algorithms included purposes comparison.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	experimental data discussed, empirical results presented.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	close analysis ?ndings discussion related work.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	developed approach word sense disambiguation represents text entirely terms occurrence bigrams, de?ne two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation Bigram Counts consecutive words occur text.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	distributional characteristics bigrams fairly consistent across corpora; majority occur one time.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Given sparse skewed nature data, statistical methods used select interesting bigrams must carefully chosen.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	explore two alternatives, power divergence family goodness ?t statistics Dice CoeÆcient, information theoretic measure related point- wise Mutual Information.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Figure 1 summarizes notation word bigram counts used paper way 2 ? 2 contingency table.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	value n 11 shows many times bigram big cat occurs corpus.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	value n 12 shows often bigrams occur big ?rst word cat second.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	counts n +1 n 1+ indicate often words big cat occur ?rst second words bigram corpus.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	total number bigrams corpus represented n ++ . 2.1 Power Divergence Family.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	(Cressie Read, 1984) introduce power divergence family goodness ?t statistics.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	number well known statistics belong family, including likelihood ratio statisticG 2 Pearson'sX 2 statistic.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	measure divergence observed (n ij ) expected (m ij ) bigram counts, ij estimated based assumption component words bigram occur together strictly chance.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	(Dunning, 1993) argues favor G2 X2, especially dealing sparse skewed data distributions.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	However, (Cressie Read, 1984) suggest cases Pearson's statistic reliable likelihood ratio one test always preferred other.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	light this, (Pedersen, 1996) presents Fisher's exact test alternative since rely distributional assumptions underly Pearson's test likelihood ratio.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Unfortunately usually clear test appropriate particular sample data.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	take following approach, based observation tests assign approximately measure statistical signi?cance bi- gram counts contingency table violate distributional assumptions underly goodness ?t statistics.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	perform tests using X 2 , G 2 , Fisher's exact test bigram.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	resulting measures statistical signi?cance di?er, distribution bigram counts causing least one tests become unreliable.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	occurs rely upon value Fisher's exact test since makes fewer assumptions underlying distribution data.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	experiments paper, identi?ed top 100 ranked bigrams occur 5 times training corpus associated word.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	cases rankings produced G 2 , X 2 , Fisher's exact test disagreed, altogether surprising given low frequency bigrams excluded.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Since statistics produced rankings, hereafter make distinction among simply refer generically power divergence statistic.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	2.2 Dice CoeÆcient.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Dice CoeÆcient descriptive statistic provides measure association among two words corpus.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	similar pointwise Mutual Information, widely used measure ?rst introduced identifying lexical relationships (Church Hanks, 1990).	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Pointwise Mutual Information de?ned follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ w 1 w 2 represent two words make bigram.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Pointwise Mutual Information quanti?es often two words occur together bigram (the numerator) relative often occur overall corpus (the denominator).	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	However, curious limitation pointwise Mutual Information.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	bigram w 1 w 2 occurs n 11 times corpus, whose component words w 1 w 2 occur part bigram, result increasingly strong measures association value n 11 decreases.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Thus, maximum pointwise Mutual Information given corpus assigned bi- grams occur one time, whose component words never occur outside bigram.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	usually bigrams prove useful disambiguation, yet dominate ranked list determined pointwise Mutual Information.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Dice CoeÆcient overcomes limitation, de?ned follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ n 11 = n 1+ = n +1 value Dice(w 1 ; w 2 ) 1 values n 11 . value n. 11 less either marginal totals (the typical case) rankings produced Dice Co- eÆcient similar Mutual Information.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	relationship pointwise Mutual Information Dice CoeÆcient also discussed (Smadja et al., 1996).	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	developed Bigram Statistics Package produce ranked lists bigrams using range tests.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	software written Perl freely available www.d.umn.edu/~tpederse.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Decision trees among widely used machine learning algorithms.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	perform general speci?c search feature space, adding informative features tree structure search proceeds.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	objective select minimal set features eÆciently partitions feature space classes observations assemble tree.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	case, observations manually sense{tagged examples ambiguous word context partitions correspond di?erent possible senses.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	feature selected search process represented node learned decision tree.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	node represents choice point number di?erent possible values feature.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Learning continues training examples accounted decision tree.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	general, tree overly speci?c training data generalize well new examples.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Therefore learning followed pruning step nodes eliminated reorganized produce tree generalize new circumstances.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Test instances disambiguated ?nding path learned decision tree root leaf node corresponds observed features.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	instance ambiguous word dis- ambiguated passing series tests, test asks particular bigram occurs available window context.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	also include three benchmark learning algorithms study: majority classi?er, decision stump, Naive Bayesian classi?er.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	majority classi?er assigns common sense training data every instance test data.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	decision stump one node decision tree(Holte, 1993) created stopping decision tree learner single informative feature added tree.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Naive Bayesian classi?er (Duda Hart, 1973) based certain blanket assumptions interactions among features corpus.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	search feature space performed build representative model case decision trees.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Instead, features included classi- ?er assumed relevant task hand.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	assumption feature conditionally independent features, given sense ambiguous word.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	often used bag words feature set, every word training sample represented binary feature indicates whether occurs window context surrounding ambiguous word.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	use Weka (Witten Frank, 2000) implementations C4.5 decision tree learner (known J48), decision stump, Naive Bayesian classi?er.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Weka written Java freely available www.cs.waikato.ac.nz/~ml.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	empirical study utilizes training test data 1998 SENSEVAL evaluation word sense disambiguation systems.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Ten teams participated supervised learning portion event.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Additional details exercise, including data results referred paper, found SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) (Kilgarri?	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Palmer, 2000).	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	included 36 tasks SENSEVAL training test data provided.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	task requires occurrences particular word test data disambiguated based model learned sense{tagged instances training data.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	words used multiple tasks di?erent parts speech.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	example, two tasks associated bet, one use noun verb.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Thus, 36 tasks involving disambiguation 29 di?erent words.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	words part speech associated task shown Table 1 column 1.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Note parts speech encoded n noun, adjective, v verb, p words part speech provided.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	number test training instances task shown columns 2 4.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	instance consists sentence ambiguous word occurs well one two surrounding sentences.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	general total context available ambiguous word less 100 surrounding words.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	number distinct senses test data task shown column 3.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	following process repeated task.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Capitalization punctuation removed training test data.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Two feature sets selected training data based top 100 ranked bigrams according power divergence statistic Dice CoeÆcient.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	bigram must occurred 5 times included feature.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	step ?lters large number possible bi- grams allows decision tree learner focus small number candidate bigrams likely helpful disambiguation process.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	training test data converted feature vectors feature represents occurrence one bigrams belong feature set.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	representation training data actual input learning algorithms.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Decision tree decision stump learning performed twice, using feature set determined power divergence statistic using feature set identi?ed Dice CoeÆcient.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	majority classi?er simply determines frequent sense training data assigns instances test data.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Naive Bayesian classi?er based feature set every word occurs 5 times training data included feature.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	learned models used disambiguate test data.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	test data kept separate stage.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	employ ?ne grained scoring method, word counted correctly disambiguated assigned sense tag exactly matches true sense tag.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	partial credit assigned near misses.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	accuracy attained learning algorithms shown Table 1.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Column 5 reports accuracy majority classi?er, columns 6 7 show best average accuracy reported 10 participating SENSEVAL teams.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	evaluation SENSEVAL based precision recall, converted scores accuracy taking product.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	However, best precision recall may come di?erent teams, best accuracy shown column 6 may actually higher single participating SENSEVAL system.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	average accuracy column 7 product average precision recall reported participating SENSEVAL teams.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Column 8 shows accuracy decision tree using J48 learning algorithm features identi?ed power divergence statistic.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Column 10 shows accuracy decision tree Dice CoeÆcient selects features.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Columns 9 11 show accuracy decision stump based power divergence statistic Dice CoeÆcient respectively.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Finally, column 13 shows accuracy Naive Bayesian classi?er based bag words feature set.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	accurate method decision tree based feature set determined power divergence statistic.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	last line Table 1 shows win-tie-loss score decision tree/power divergence method relative every method.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	win shows accurate method column, loss means less accurate, tie means equally accurate.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	decision tree/power divergence method accurate best reported SENSEVAL results 19 36 tasks, accurate 30 36 tasks compared average reported accuracy.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	decision stumps also fared well, proving accurate best SENSEVAL results 14 36 tasks.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	general feature sets selected power divergence statistic result accurate decision trees selected Dice CoeÆcient.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	power divergence tests prove reliable since account possible events surrounding two words w 1 w 2 ; occur bigram w 1 w 2 , w 1 w 2 occurs bigram without other, bigram consists neither.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Dice CoeÆcient based strictly event w 1 w 2 occur together bigram.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	6 tasks decision tree / power divergence approach less accurate SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, sanction-p.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	dramatic difference occurred amaze-v, SENSE- VAL average 92.4% decision tree accuracy 58.6%.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	However, unusual task every instance test data belonged single sense minority sense training data.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	characteristics decision trees decision stumps learned word shown Table 2.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Column 1 shows word part speech.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Columns 2, 3, 4 based feature set selected power divergence statistic columns 5, 6, 7 based Dice CoeÆ- cient.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Columns 2 5 show node selected serve decision stump.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Columns 3 6 show number leaf nodes learned decision tree relative number total nodes.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Columns 4 7 show number bigram features selected Table 1: Experimental Results.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	table shows little di?erence decision stump nodes selected feature sets determined power divergence statistics versus Dice CoeÆcient.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	expected since top ranked bigrams measure consistent, decision stump node generally chosen among those.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	However, di?erences feature sets selected power divergence statistics Dice CoeÆcient.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	ected different sized trees learned based feature sets.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	number leaf nodes total number nodes learned tree shown columns 3 6.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	number internal nodes simply di?erence total nodes leaf nodes.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	leaf node represents end path decision tree makes sense distinction.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Since bigram feature appear decision tree, number inter- Table 2: Decision Tree Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n accident 8/15 101 accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n excess 13/25 104 excess 11/21 102 oat-n oat 7/13 13 oat 7/13 13 giant-n giants 16/31 103 giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n 1/1 7 1/1 7 promise-n promise 95/189 100 promising 49/97 107 sack-n sack 5/9 31 sack 5/9 31 scrap-n scrap 7/13 8 scrap 7/13 8 shirt-n shirt 38/75 101 shirt 55/109 101 amaze-v amazed 11/21 102 amazed 11/21 102 bet-v bet 4/7 10 bet 4/7 10 bother-v bothered 19/37 101 bothered 20/39 106 bury-v buried 28/55 103 buried 32/63 103 calculate-v calculated 5/9 103 calculated 5/9 103 consume-v 4/7 20 4/7 20 derive-v derived 10/19 104 derived 10/19 104 oat-v oated 24/47 80 oated 24/47 80 invade-v invade 55/109 107 invade 66/127 108 promise-v promise 3/5 100 promise 5/9 106 sack-v return 1/1 91 return 1/1 91 scrap-v 1/1 7 1/1 7 seize-v seize 26/51 104 seize 57/113 104 brilliant-a brilliant 26/51 101 brilliant 42/83 103 oating-a 7/13 10 7/13 10 generous-a generous 57/113 103 generous 56/111 102 giant-a giant 2/3 102 giant 1/1 101 modest-a modest 14/27 101 modest 10/19 105 slight-a slightest 2/3 105 slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band 14/27 100 band 21/41 117 bitter-p bitter 22/43 54 bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p head 90/179 100 head 81/161 105 nal nodes represents number bigram features selected decision tree learner.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	One original hypotheses accurate decision trees bigrams include relatively small number features.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	motivated success decision stumps performing disambiguation based single bigram feature.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	experiments, decision trees used bigram features identi?ed ?ltering step, many words decision tree learner went eliminate candidate features.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	seen comparing number internal nodes number candidate features shown columns 4 7.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	1 also noteworthy bigrams ultimately selected decision tree learner inclusion tree always include bigrams ranked highly power divergence statistic Dice CoeÆcient.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	expected, since selection bigrams raw text mea1 words 100 top ranked bigrams form set candidate features presented decision tree learner.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	ties top 100 rankings may 100 features, fewer 100 bi- grams occurred 5 times bigrams included feature set.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	suring association two words, decision tree seeks bigrams partition instances ambiguous word distinct senses.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	particular, decision tree learner makes decisions bigram include nodes tree using gain ratio, measure based overall Mutual Information bigram particular word sense.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Finally, note smallest decision trees functionally equivalent benchmark methods.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	decision tree 1 leaf node internal nodes (1/1) acts majority classi?er.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	decision tree 2 leaf nodes 1 internal node (2/3) structure decision stump.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	One long-term objectives identify core set features useful disambiguating wide class words using supervised unsupervised methodologies.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	presented ensemble approach word sense disambiguation (Pedersen, 2000) multiple Naive Bayesian classi?ers, based co{ occurrence features varying sized windows context, shown perform well widely studied nouns interest line.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	accuracy approach good previously published results, learned models complex diÆcult interpret, e?ect acting accurate black boxes.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	experience variations learning algorithms far less signi?cant contributors disambiguation accuracy variations feature set.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	words, informative feature set result accurate disambiguation used wide range learning algorithms, learning algorithm perform well given uninformative misleading set features.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Therefore, focus developing discovering feature sets make distinctions among word senses.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	learning algorithms must produce accurate models, also shed new light relationships among features allow us continue re?ning understanding feature sets.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	believe decision trees meet criteria.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	wide range implementations available, known robust accurate across range domains.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	important, structure easy interpret may provide insights relationships exist among features general rules disambiguation.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Bigrams used features word sense disambiguation, particularly form collocations ambiguous word one component bigram (e.g., (Bruce Wiebe, 1994), (Ng Lee, 1996), (Yarowsky, 1995)).	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	bigrams identify collocations include word disambiguated, requirement case.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Decision trees used supervised learning approaches word sense disambiguation, fared well number comparative studies (e.g., (Mooney, 1996), (Pedersen Bruce, 1997)).	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	former used bag word feature sets latter used mixed feature set included part-of-speech neighboring words, three collocations, morphology ambiguous word.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	believe approach paper ?rst time decision trees based strictly bigram features employed.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	decision list closely related approach also applied word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks Stevenson, 1998), (Yarowsky, 2000)).	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Rather building traversing tree perform disambiguation, list employed.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	general case decision list may suffer less fragmentation learning decision trees; practical matter means decision list less likely over{trained.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	However, believe fragmentation also ects feature set used learning.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	consists approximately 100 binary features.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	results relatively small feature space likely su?er fragmentation larger spaces.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	number immediate extensions work.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	?rst ease requirement bi- grams made two consecutive words.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Rather, search bigrams component words may separated words text.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	second eliminate ?ltering step candidate bigrams selected power divergence statistic.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Instead, decision tree learner would consider possible bigrams.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Despite increasing danger fragmentation, interesting issue since bigrams judged informative decision tree learner always ranked highly ?ltering step.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	particular, determine ?ltering process ever eliminates bi- grams could signi?cant sources disambiguation information.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	longer term, hope adapt approach unsupervised learning, disambiguation performed without bene?t sense tagged text.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	optimistic viable, since bigram features easy identify raw text.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	results approach compared 1998 SENSEVAL word sense disambiguation exercise show bigram based decision tree approach accurate best SENSEVAL results 19 36 words.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	Bigram Statistics Package implemented Satanjeev Banerjee, supported Grant{in{Aid Research, Artistry Scholarship OÆce Vice President Research Dean Graduate School University Minnesota.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	would like thank SENSEVAL organizers making data results 1998 event freely available.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	comments three anonymous reviewers helpful preparing ?nal version paper.	0
Commonly used features include surrounding words part speech(Bruce Wiebe, 1999),context keywords (Ng Lee, 1996) context bigrams (Pedersen, 2001)	preliminary version paper appears (Pedersen, 2001).	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Decision Tree Bigrams Accurate Predictor Word Sense	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	paper presents corpus-based approach word sense disambiguation decision tree assigns sense ambiguous word based bigrams occur nearby.	1
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	approach evaluated using sense-tagged corpora 1998 SENSEVAL word sense disambiguation exercise.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	accurate average results reported 30 36 words, accurate best results 19 36 words.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Word sense disambiguation process selecting appropriate meaning word, based context occurs.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	purposes assumed set possible meanings, i.e., sense inventory, already determined.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	example, suppose bill following set possible meanings: piece currency, pending legislation, bird jaw.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	used context Senate bill consideration, human reader immediately understands bill used legislative sense.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	However, computer program attempting perform task faces diÆcult problem since bene?t innate common{sense linguistic knowledge.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Rather attempting provide computer programs real{world knowledge comparable humans, natural language processing turned corpus{based methods.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	approaches use techniques statistics machine learning induce models language usage large samples text.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	models trained perform particular tasks, usually via supervised learning.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	paper describes approach decision tree learned number sentences instance ambiguous word manually annotated sense{tag denotes appropriate sense context.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Prior learning, sense{tagged corpus must converted regular form suitable automatic processing.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	sense{tagged occurrence ambiguous word converted feature vector, feature represents property surrounding text considered relevant disambiguation process.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Given exibility complexity human language, potentially in?nite set features could utilized.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	However, corpus{based approaches features usually consist information readily iden- ti?ed text, without relying extensive external knowledge sources.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	typically include part{of{speech surrounding words, presence certain key words within window context, various syntactic properties sentence ambiguous word.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	approach paper relies upon feature set made bigrams, two word sequences occur text.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	context ambiguous word occurs represented number binary features indicate whether particular bigram occurred within approximately 50 words left right word disambiguated.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	take approach since surface lexical features like bigrams, collocations, co{occurrences often contribute great deal disambiguation accuracy.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	clear much disambiguation accuracy improved use features identi?ed complex pre{processing part{of{speech tagging, parsing, anaphora resolution.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	One objectives establish clear upper bounds accuracy disambiguation using feature sets impose substantial pre{ processing requirements.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	paper continues discussion methods identifying bigrams included feature set learning.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	decision tree learning algorithm described, benchmark learning algorithms included purposes comparison.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	experimental data discussed, empirical results presented.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	close analysis ?ndings discussion related work.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	developed approach word sense disambiguation represents text entirely terms occurrence bigrams, de?ne two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation Bigram Counts consecutive words occur text.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	distributional characteristics bigrams fairly consistent across corpora; majority occur one time.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Given sparse skewed nature data, statistical methods used select interesting bigrams must carefully chosen.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	explore two alternatives, power divergence family goodness ?t statistics Dice CoeÆcient, information theoretic measure related point- wise Mutual Information.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Figure 1 summarizes notation word bigram counts used paper way 2 ? 2 contingency table.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	value n 11 shows many times bigram big cat occurs corpus.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	value n 12 shows often bigrams occur big ?rst word cat second.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	counts n +1 n 1+ indicate often words big cat occur ?rst second words bigram corpus.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	total number bigrams corpus represented n ++ . 2.1 Power Divergence Family.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	(Cressie Read, 1984) introduce power divergence family goodness ?t statistics.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	number well known statistics belong family, including likelihood ratio statisticG 2 Pearson'sX 2 statistic.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	measure divergence observed (n ij ) expected (m ij ) bigram counts, ij estimated based assumption component words bigram occur together strictly chance.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	(Dunning, 1993) argues favor G2 X2, especially dealing sparse skewed data distributions.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	However, (Cressie Read, 1984) suggest cases Pearson's statistic reliable likelihood ratio one test always preferred other.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	light this, (Pedersen, 1996) presents Fisher's exact test alternative since rely distributional assumptions underly Pearson's test likelihood ratio.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Unfortunately usually clear test appropriate particular sample data.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	take following approach, based observation tests assign approximately measure statistical signi?cance bi- gram counts contingency table violate distributional assumptions underly goodness ?t statistics.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	perform tests using X 2 , G 2 , Fisher's exact test bigram.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	resulting measures statistical signi?cance di?er, distribution bigram counts causing least one tests become unreliable.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	occurs rely upon value Fisher's exact test since makes fewer assumptions underlying distribution data.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	experiments paper, identi?ed top 100 ranked bigrams occur 5 times training corpus associated word.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	cases rankings produced G 2 , X 2 , Fisher's exact test disagreed, altogether surprising given low frequency bigrams excluded.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Since statistics produced rankings, hereafter make distinction among simply refer generically power divergence statistic.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	2.2 Dice CoeÆcient.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Dice CoeÆcient descriptive statistic provides measure association among two words corpus.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	similar pointwise Mutual Information, widely used measure ?rst introduced identifying lexical relationships (Church Hanks, 1990).	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Pointwise Mutual Information de?ned follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ w 1 w 2 represent two words make bigram.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Pointwise Mutual Information quanti?es often two words occur together bigram (the numerator) relative often occur overall corpus (the denominator).	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	However, curious limitation pointwise Mutual Information.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	bigram w 1 w 2 occurs n 11 times corpus, whose component words w 1 w 2 occur part bigram, result increasingly strong measures association value n 11 decreases.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Thus, maximum pointwise Mutual Information given corpus assigned bi- grams occur one time, whose component words never occur outside bigram.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	usually bigrams prove useful disambiguation, yet dominate ranked list determined pointwise Mutual Information.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Dice CoeÆcient overcomes limitation, de?ned follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ n 11 = n 1+ = n +1 value Dice(w 1 ; w 2 ) 1 values n 11 . value n. 11 less either marginal totals (the typical case) rankings produced Dice Co- eÆcient similar Mutual Information.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	relationship pointwise Mutual Information Dice CoeÆcient also discussed (Smadja et al., 1996).	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	developed Bigram Statistics Package produce ranked lists bigrams using range tests.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	software written Perl freely available www.d.umn.edu/~tpederse.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Decision trees among widely used machine learning algorithms.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	perform general speci?c search feature space, adding informative features tree structure search proceeds.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	objective select minimal set features eÆciently partitions feature space classes observations assemble tree.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	case, observations manually sense{tagged examples ambiguous word context partitions correspond di?erent possible senses.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	feature selected search process represented node learned decision tree.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	node represents choice point number di?erent possible values feature.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Learning continues training examples accounted decision tree.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	general, tree overly speci?c training data generalize well new examples.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Therefore learning followed pruning step nodes eliminated reorganized produce tree generalize new circumstances.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Test instances disambiguated ?nding path learned decision tree root leaf node corresponds observed features.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	instance ambiguous word dis- ambiguated passing series tests, test asks particular bigram occurs available window context.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	also include three benchmark learning algorithms study: majority classi?er, decision stump, Naive Bayesian classi?er.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	majority classi?er assigns common sense training data every instance test data.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	decision stump one node decision tree(Holte, 1993) created stopping decision tree learner single informative feature added tree.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Naive Bayesian classi?er (Duda Hart, 1973) based certain blanket assumptions interactions among features corpus.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	search feature space performed build representative model case decision trees.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Instead, features included classi- ?er assumed relevant task hand.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	assumption feature conditionally independent features, given sense ambiguous word.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	often used bag words feature set, every word training sample represented binary feature indicates whether occurs window context surrounding ambiguous word.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	use Weka (Witten Frank, 2000) implementations C4.5 decision tree learner (known J48), decision stump, Naive Bayesian classi?er.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Weka written Java freely available www.cs.waikato.ac.nz/~ml.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	empirical study utilizes training test data 1998 SENSEVAL evaluation word sense disambiguation systems.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Ten teams participated supervised learning portion event.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Additional details exercise, including data results referred paper, found SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) (Kilgarri?	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Palmer, 2000).	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	included 36 tasks SENSEVAL training test data provided.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	task requires occurrences particular word test data disambiguated based model learned sense{tagged instances training data.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	words used multiple tasks di?erent parts speech.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	example, two tasks associated bet, one use noun verb.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Thus, 36 tasks involving disambiguation 29 di?erent words.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	words part speech associated task shown Table 1 column 1.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Note parts speech encoded n noun, adjective, v verb, p words part speech provided.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	number test training instances task shown columns 2 4.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	instance consists sentence ambiguous word occurs well one two surrounding sentences.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	general total context available ambiguous word less 100 surrounding words.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	number distinct senses test data task shown column 3.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	following process repeated task.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Capitalization punctuation removed training test data.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Two feature sets selected training data based top 100 ranked bigrams according power divergence statistic Dice CoeÆcient.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	bigram must occurred 5 times included feature.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	step ?lters large number possible bi- grams allows decision tree learner focus small number candidate bigrams likely helpful disambiguation process.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	training test data converted feature vectors feature represents occurrence one bigrams belong feature set.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	representation training data actual input learning algorithms.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Decision tree decision stump learning performed twice, using feature set determined power divergence statistic using feature set identi?ed Dice CoeÆcient.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	majority classi?er simply determines frequent sense training data assigns instances test data.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Naive Bayesian classi?er based feature set every word occurs 5 times training data included feature.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	learned models used disambiguate test data.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	test data kept separate stage.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	employ ?ne grained scoring method, word counted correctly disambiguated assigned sense tag exactly matches true sense tag.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	partial credit assigned near misses.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	accuracy attained learning algorithms shown Table 1.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Column 5 reports accuracy majority classi?er, columns 6 7 show best average accuracy reported 10 participating SENSEVAL teams.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	evaluation SENSEVAL based precision recall, converted scores accuracy taking product.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	However, best precision recall may come di?erent teams, best accuracy shown column 6 may actually higher single participating SENSEVAL system.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	average accuracy column 7 product average precision recall reported participating SENSEVAL teams.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Column 8 shows accuracy decision tree using J48 learning algorithm features identi?ed power divergence statistic.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Column 10 shows accuracy decision tree Dice CoeÆcient selects features.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Columns 9 11 show accuracy decision stump based power divergence statistic Dice CoeÆcient respectively.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Finally, column 13 shows accuracy Naive Bayesian classi?er based bag words feature set.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	accurate method decision tree based feature set determined power divergence statistic.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	last line Table 1 shows win-tie-loss score decision tree/power divergence method relative every method.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	win shows accurate method column, loss means less accurate, tie means equally accurate.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	decision tree/power divergence method accurate best reported SENSEVAL results 19 36 tasks, accurate 30 36 tasks compared average reported accuracy.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	decision stumps also fared well, proving accurate best SENSEVAL results 14 36 tasks.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	general feature sets selected power divergence statistic result accurate decision trees selected Dice CoeÆcient.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	power divergence tests prove reliable since account possible events surrounding two words w 1 w 2 ; occur bigram w 1 w 2 , w 1 w 2 occurs bigram without other, bigram consists neither.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Dice CoeÆcient based strictly event w 1 w 2 occur together bigram.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	6 tasks decision tree / power divergence approach less accurate SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, sanction-p.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	dramatic difference occurred amaze-v, SENSE- VAL average 92.4% decision tree accuracy 58.6%.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	However, unusual task every instance test data belonged single sense minority sense training data.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	characteristics decision trees decision stumps learned word shown Table 2.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Column 1 shows word part speech.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Columns 2, 3, 4 based feature set selected power divergence statistic columns 5, 6, 7 based Dice CoeÆ- cient.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Columns 2 5 show node selected serve decision stump.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Columns 3 6 show number leaf nodes learned decision tree relative number total nodes.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Columns 4 7 show number bigram features selected Table 1: Experimental Results.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	table shows little di?erence decision stump nodes selected feature sets determined power divergence statistics versus Dice CoeÆcient.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	expected since top ranked bigrams measure consistent, decision stump node generally chosen among those.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	However, di?erences feature sets selected power divergence statistics Dice CoeÆcient.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	ected different sized trees learned based feature sets.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	number leaf nodes total number nodes learned tree shown columns 3 6.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	number internal nodes simply di?erence total nodes leaf nodes.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	leaf node represents end path decision tree makes sense distinction.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Since bigram feature appear decision tree, number inter- Table 2: Decision Tree Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n accident 8/15 101 accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n excess 13/25 104 excess 11/21 102 oat-n oat 7/13 13 oat 7/13 13 giant-n giants 16/31 103 giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n 1/1 7 1/1 7 promise-n promise 95/189 100 promising 49/97 107 sack-n sack 5/9 31 sack 5/9 31 scrap-n scrap 7/13 8 scrap 7/13 8 shirt-n shirt 38/75 101 shirt 55/109 101 amaze-v amazed 11/21 102 amazed 11/21 102 bet-v bet 4/7 10 bet 4/7 10 bother-v bothered 19/37 101 bothered 20/39 106 bury-v buried 28/55 103 buried 32/63 103 calculate-v calculated 5/9 103 calculated 5/9 103 consume-v 4/7 20 4/7 20 derive-v derived 10/19 104 derived 10/19 104 oat-v oated 24/47 80 oated 24/47 80 invade-v invade 55/109 107 invade 66/127 108 promise-v promise 3/5 100 promise 5/9 106 sack-v return 1/1 91 return 1/1 91 scrap-v 1/1 7 1/1 7 seize-v seize 26/51 104 seize 57/113 104 brilliant-a brilliant 26/51 101 brilliant 42/83 103 oating-a 7/13 10 7/13 10 generous-a generous 57/113 103 generous 56/111 102 giant-a giant 2/3 102 giant 1/1 101 modest-a modest 14/27 101 modest 10/19 105 slight-a slightest 2/3 105 slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band 14/27 100 band 21/41 117 bitter-p bitter 22/43 54 bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p head 90/179 100 head 81/161 105 nal nodes represents number bigram features selected decision tree learner.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	One original hypotheses accurate decision trees bigrams include relatively small number features.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	motivated success decision stumps performing disambiguation based single bigram feature.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	experiments, decision trees used bigram features identi?ed ?ltering step, many words decision tree learner went eliminate candidate features.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	seen comparing number internal nodes number candidate features shown columns 4 7.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	1 also noteworthy bigrams ultimately selected decision tree learner inclusion tree always include bigrams ranked highly power divergence statistic Dice CoeÆcient.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	expected, since selection bigrams raw text mea1 words 100 top ranked bigrams form set candidate features presented decision tree learner.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	ties top 100 rankings may 100 features, fewer 100 bi- grams occurred 5 times bigrams included feature set.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	suring association two words, decision tree seeks bigrams partition instances ambiguous word distinct senses.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	particular, decision tree learner makes decisions bigram include nodes tree using gain ratio, measure based overall Mutual Information bigram particular word sense.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Finally, note smallest decision trees functionally equivalent benchmark methods.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	decision tree 1 leaf node internal nodes (1/1) acts majority classi?er.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	decision tree 2 leaf nodes 1 internal node (2/3) structure decision stump.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	One long-term objectives identify core set features useful disambiguating wide class words using supervised unsupervised methodologies.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	presented ensemble approach word sense disambiguation (Pedersen, 2000) multiple Naive Bayesian classi?ers, based co{ occurrence features varying sized windows context, shown perform well widely studied nouns interest line.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	accuracy approach good previously published results, learned models complex diÆcult interpret, e?ect acting accurate black boxes.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	experience variations learning algorithms far less signi?cant contributors disambiguation accuracy variations feature set.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	words, informative feature set result accurate disambiguation used wide range learning algorithms, learning algorithm perform well given uninformative misleading set features.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Therefore, focus developing discovering feature sets make distinctions among word senses.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	learning algorithms must produce accurate models, also shed new light relationships among features allow us continue re?ning understanding feature sets.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	believe decision trees meet criteria.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	wide range implementations available, known robust accurate across range domains.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	important, structure easy interpret may provide insights relationships exist among features general rules disambiguation.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Bigrams used features word sense disambiguation, particularly form collocations ambiguous word one component bigram (e.g., (Bruce Wiebe, 1994), (Ng Lee, 1996), (Yarowsky, 1995)).	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	bigrams identify collocations include word disambiguated, requirement case.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Decision trees used supervised learning approaches word sense disambiguation, fared well number comparative studies (e.g., (Mooney, 1996), (Pedersen Bruce, 1997)).	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	former used bag word feature sets latter used mixed feature set included part-of-speech neighboring words, three collocations, morphology ambiguous word.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	believe approach paper ?rst time decision trees based strictly bigram features employed.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	decision list closely related approach also applied word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks Stevenson, 1998), (Yarowsky, 2000)).	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Rather building traversing tree perform disambiguation, list employed.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	general case decision list may suffer less fragmentation learning decision trees; practical matter means decision list less likely over{trained.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	However, believe fragmentation also ects feature set used learning.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	consists approximately 100 binary features.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	results relatively small feature space likely su?er fragmentation larger spaces.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	number immediate extensions work.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	?rst ease requirement bi- grams made two consecutive words.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Rather, search bigrams component words may separated words text.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	second eliminate ?ltering step candidate bigrams selected power divergence statistic.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Instead, decision tree learner would consider possible bigrams.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Despite increasing danger fragmentation, interesting issue since bigrams judged informative decision tree learner always ranked highly ?ltering step.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	particular, determine ?ltering process ever eliminates bi- grams could signi?cant sources disambiguation information.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	longer term, hope adapt approach unsupervised learning, disambiguation performed without bene?t sense tagged text.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	optimistic viable, since bigram features easy identify raw text.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	results approach compared 1998 SENSEVAL word sense disambiguation exercise show bigram based decision tree approach accurate best SENSEVAL results 19 36 words.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	Bigram Statistics Package implemented Satanjeev Banerjee, supported Grant{in{Aid Research, Artistry Scholarship OÆce Vice President Research Dean Graduate School University Minnesota.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	would like thank SENSEVAL organizers making data results 1998 event freely available.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	comments three anonymous reviewers helpful preparing ?nal version paper.	0
learning methodology, large range ofalgorithms employed, including neu ral networks (Leacock et al., 1998), decision trees (Pedersen, 2001)	preliminary version paper appears (Pedersen, 2001).	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Decision Tree Bigrams Accurate Predictor Word Sense	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	paper presents corpus-based approach word sense disambiguation decision tree assigns sense ambiguous word based bigrams occur nearby.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	approach evaluated using sense-tagged corpora 1998 SENSEVAL word sense disambiguation exercise.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	accurate average results reported 30 36 words, accurate best results 19 36 words.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Word sense disambiguation process selecting appropriate meaning word, based context occurs.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	purposes assumed set possible meanings, i.e., sense inventory, already determined.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	example, suppose bill following set possible meanings: piece currency, pending legislation, bird jaw.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	used context Senate bill consideration, human reader immediately understands bill used legislative sense.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	However, computer program attempting perform task faces diÆcult problem since bene?t innate common{sense linguistic knowledge.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Rather attempting provide computer programs real{world knowledge comparable humans, natural language processing turned corpus{based methods.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	approaches use techniques statistics machine learning induce models language usage large samples text.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	models trained perform particular tasks, usually via supervised learning.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	paper describes approach decision tree learned number sentences instance ambiguous word manually annotated sense{tag denotes appropriate sense context.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Prior learning, sense{tagged corpus must converted regular form suitable automatic processing.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	sense{tagged occurrence ambiguous word converted feature vector, feature represents property surrounding text considered relevant disambiguation process.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Given exibility complexity human language, potentially in?nite set features could utilized.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	However, corpus{based approaches features usually consist information readily iden- ti?ed text, without relying extensive external knowledge sources.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	typically include part{of{speech surrounding words, presence certain key words within window context, various syntactic properties sentence ambiguous word.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	approach paper relies upon feature set made bigrams, two word sequences occur text.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	context ambiguous word occurs represented number binary features indicate whether particular bigram occurred within approximately 50 words left right word disambiguated.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	take approach since surface lexical features like bigrams, collocations, co{occurrences often contribute great deal disambiguation accuracy.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	clear much disambiguation accuracy improved use features identi?ed complex pre{processing part{of{speech tagging, parsing, anaphora resolution.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	One objectives establish clear upper bounds accuracy disambiguation using feature sets impose substantial pre{ processing requirements.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	paper continues discussion methods identifying bigrams included feature set learning.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	decision tree learning algorithm described, benchmark learning algorithms included purposes comparison.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	experimental data discussed, empirical results presented.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	close analysis ?ndings discussion related work.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	developed approach word sense disambiguation represents text entirely terms occurrence bigrams, de?ne two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation Bigram Counts consecutive words occur text.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	distributional characteristics bigrams fairly consistent across corpora; majority occur one time.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Given sparse skewed nature data, statistical methods used select interesting bigrams must carefully chosen.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	explore two alternatives, power divergence family goodness ?t statistics Dice CoeÆcient, information theoretic measure related point- wise Mutual Information.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Figure 1 summarizes notation word bigram counts used paper way 2 ? 2 contingency table.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	value n 11 shows many times bigram big cat occurs corpus.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	value n 12 shows often bigrams occur big ?rst word cat second.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	counts n +1 n 1+ indicate often words big cat occur ?rst second words bigram corpus.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	total number bigrams corpus represented n ++ . 2.1 Power Divergence Family.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	(Cressie Read, 1984) introduce power divergence family goodness ?t statistics.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	number well known statistics belong family, including likelihood ratio statisticG 2 Pearson'sX 2 statistic.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	measure divergence observed (n ij ) expected (m ij ) bigram counts, ij estimated based assumption component words bigram occur together strictly chance.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	(Dunning, 1993) argues favor G2 X2, especially dealing sparse skewed data distributions.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	However, (Cressie Read, 1984) suggest cases Pearson's statistic reliable likelihood ratio one test always preferred other.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	light this, (Pedersen, 1996) presents Fisher's exact test alternative since rely distributional assumptions underly Pearson's test likelihood ratio.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Unfortunately usually clear test appropriate particular sample data.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	take following approach, based observation tests assign approximately measure statistical signi?cance bi- gram counts contingency table violate distributional assumptions underly goodness ?t statistics.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	perform tests using X 2 , G 2 , Fisher's exact test bigram.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	resulting measures statistical signi?cance di?er, distribution bigram counts causing least one tests become unreliable.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	occurs rely upon value Fisher's exact test since makes fewer assumptions underlying distribution data.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	experiments paper, identi?ed top 100 ranked bigrams occur 5 times training corpus associated word.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	cases rankings produced G 2 , X 2 , Fisher's exact test disagreed, altogether surprising given low frequency bigrams excluded.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Since statistics produced rankings, hereafter make distinction among simply refer generically power divergence statistic.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	2.2 Dice CoeÆcient.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Dice CoeÆcient descriptive statistic provides measure association among two words corpus.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	similar pointwise Mutual Information, widely used measure ?rst introduced identifying lexical relationships (Church Hanks, 1990).	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Pointwise Mutual Information de?ned follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ w 1 w 2 represent two words make bigram.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Pointwise Mutual Information quanti?es often two words occur together bigram (the numerator) relative often occur overall corpus (the denominator).	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	However, curious limitation pointwise Mutual Information.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	bigram w 1 w 2 occurs n 11 times corpus, whose component words w 1 w 2 occur part bigram, result increasingly strong measures association value n 11 decreases.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Thus, maximum pointwise Mutual Information given corpus assigned bi- grams occur one time, whose component words never occur outside bigram.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	usually bigrams prove useful disambiguation, yet dominate ranked list determined pointwise Mutual Information.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Dice CoeÆcient overcomes limitation, de?ned follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ n 11 = n 1+ = n +1 value Dice(w 1 ; w 2 ) 1 values n 11 . value n. 11 less either marginal totals (the typical case) rankings produced Dice Co- eÆcient similar Mutual Information.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	relationship pointwise Mutual Information Dice CoeÆcient also discussed (Smadja et al., 1996).	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	developed Bigram Statistics Package produce ranked lists bigrams using range tests.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	software written Perl freely available www.d.umn.edu/~tpederse.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Decision trees among widely used machine learning algorithms.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	perform general speci?c search feature space, adding informative features tree structure search proceeds.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	objective select minimal set features eÆciently partitions feature space classes observations assemble tree.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	case, observations manually sense{tagged examples ambiguous word context partitions correspond di?erent possible senses.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	feature selected search process represented node learned decision tree.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	node represents choice point number di?erent possible values feature.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Learning continues training examples accounted decision tree.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	general, tree overly speci?c training data generalize well new examples.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Therefore learning followed pruning step nodes eliminated reorganized produce tree generalize new circumstances.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Test instances disambiguated ?nding path learned decision tree root leaf node corresponds observed features.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	instance ambiguous word dis- ambiguated passing series tests, test asks particular bigram occurs available window context.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	also include three benchmark learning algorithms study: majority classi?er, decision stump, Naive Bayesian classi?er.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	majority classi?er assigns common sense training data every instance test data.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	decision stump one node decision tree(Holte, 1993) created stopping decision tree learner single informative feature added tree.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Naive Bayesian classi?er (Duda Hart, 1973) based certain blanket assumptions interactions among features corpus.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	search feature space performed build representative model case decision trees.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Instead, features included classi- ?er assumed relevant task hand.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	assumption feature conditionally independent features, given sense ambiguous word.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	often used bag words feature set, every word training sample represented binary feature indicates whether occurs window context surrounding ambiguous word.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	use Weka (Witten Frank, 2000) implementations C4.5 decision tree learner (known J48), decision stump, Naive Bayesian classi?er.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Weka written Java freely available www.cs.waikato.ac.nz/~ml.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	empirical study utilizes training test data 1998 SENSEVAL evaluation word sense disambiguation systems.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Ten teams participated supervised learning portion event.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Additional details exercise, including data results referred paper, found SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) (Kilgarri?	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Palmer, 2000).	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	included 36 tasks SENSEVAL training test data provided.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	task requires occurrences particular word test data disambiguated based model learned sense{tagged instances training data.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	words used multiple tasks di?erent parts speech.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	example, two tasks associated bet, one use noun verb.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Thus, 36 tasks involving disambiguation 29 di?erent words.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	words part speech associated task shown Table 1 column 1.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Note parts speech encoded n noun, adjective, v verb, p words part speech provided.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	number test training instances task shown columns 2 4.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	instance consists sentence ambiguous word occurs well one two surrounding sentences.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	general total context available ambiguous word less 100 surrounding words.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	number distinct senses test data task shown column 3.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	following process repeated task.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Capitalization punctuation removed training test data.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Two feature sets selected training data based top 100 ranked bigrams according power divergence statistic Dice CoeÆcient.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	bigram must occurred 5 times included feature.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	step ?lters large number possible bi- grams allows decision tree learner focus small number candidate bigrams likely helpful disambiguation process.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	training test data converted feature vectors feature represents occurrence one bigrams belong feature set.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	representation training data actual input learning algorithms.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Decision tree decision stump learning performed twice, using feature set determined power divergence statistic using feature set identi?ed Dice CoeÆcient.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	majority classi?er simply determines frequent sense training data assigns instances test data.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Naive Bayesian classi?er based feature set every word occurs 5 times training data included feature.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	learned models used disambiguate test data.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	test data kept separate stage.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	employ ?ne grained scoring method, word counted correctly disambiguated assigned sense tag exactly matches true sense tag.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	partial credit assigned near misses.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	accuracy attained learning algorithms shown Table 1.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Column 5 reports accuracy majority classi?er, columns 6 7 show best average accuracy reported 10 participating SENSEVAL teams.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	evaluation SENSEVAL based precision recall, converted scores accuracy taking product.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	However, best precision recall may come di?erent teams, best accuracy shown column 6 may actually higher single participating SENSEVAL system.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	average accuracy column 7 product average precision recall reported participating SENSEVAL teams.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Column 8 shows accuracy decision tree using J48 learning algorithm features identi?ed power divergence statistic.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Column 10 shows accuracy decision tree Dice CoeÆcient selects features.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Columns 9 11 show accuracy decision stump based power divergence statistic Dice CoeÆcient respectively.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Finally, column 13 shows accuracy Naive Bayesian classi?er based bag words feature set.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	accurate method decision tree based feature set determined power divergence statistic.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	last line Table 1 shows win-tie-loss score decision tree/power divergence method relative every method.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	win shows accurate method column, loss means less accurate, tie means equally accurate.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	decision tree/power divergence method accurate best reported SENSEVAL results 19 36 tasks, accurate 30 36 tasks compared average reported accuracy.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	decision stumps also fared well, proving accurate best SENSEVAL results 14 36 tasks.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	general feature sets selected power divergence statistic result accurate decision trees selected Dice CoeÆcient.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	power divergence tests prove reliable since account possible events surrounding two words w 1 w 2 ; occur bigram w 1 w 2 , w 1 w 2 occurs bigram without other, bigram consists neither.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Dice CoeÆcient based strictly event w 1 w 2 occur together bigram.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	6 tasks decision tree / power divergence approach less accurate SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, sanction-p.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	dramatic difference occurred amaze-v, SENSE- VAL average 92.4% decision tree accuracy 58.6%.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	However, unusual task every instance test data belonged single sense minority sense training data.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	characteristics decision trees decision stumps learned word shown Table 2.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Column 1 shows word part speech.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Columns 2, 3, 4 based feature set selected power divergence statistic columns 5, 6, 7 based Dice CoeÆ- cient.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Columns 2 5 show node selected serve decision stump.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Columns 3 6 show number leaf nodes learned decision tree relative number total nodes.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Columns 4 7 show number bigram features selected Table 1: Experimental Results.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	table shows little di?erence decision stump nodes selected feature sets determined power divergence statistics versus Dice CoeÆcient.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	expected since top ranked bigrams measure consistent, decision stump node generally chosen among those.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	However, di?erences feature sets selected power divergence statistics Dice CoeÆcient.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	ected different sized trees learned based feature sets.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	number leaf nodes total number nodes learned tree shown columns 3 6.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	number internal nodes simply di?erence total nodes leaf nodes.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	leaf node represents end path decision tree makes sense distinction.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Since bigram feature appear decision tree, number inter- Table 2: Decision Tree Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n accident 8/15 101 accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n excess 13/25 104 excess 11/21 102 oat-n oat 7/13 13 oat 7/13 13 giant-n giants 16/31 103 giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n 1/1 7 1/1 7 promise-n promise 95/189 100 promising 49/97 107 sack-n sack 5/9 31 sack 5/9 31 scrap-n scrap 7/13 8 scrap 7/13 8 shirt-n shirt 38/75 101 shirt 55/109 101 amaze-v amazed 11/21 102 amazed 11/21 102 bet-v bet 4/7 10 bet 4/7 10 bother-v bothered 19/37 101 bothered 20/39 106 bury-v buried 28/55 103 buried 32/63 103 calculate-v calculated 5/9 103 calculated 5/9 103 consume-v 4/7 20 4/7 20 derive-v derived 10/19 104 derived 10/19 104 oat-v oated 24/47 80 oated 24/47 80 invade-v invade 55/109 107 invade 66/127 108 promise-v promise 3/5 100 promise 5/9 106 sack-v return 1/1 91 return 1/1 91 scrap-v 1/1 7 1/1 7 seize-v seize 26/51 104 seize 57/113 104 brilliant-a brilliant 26/51 101 brilliant 42/83 103 oating-a 7/13 10 7/13 10 generous-a generous 57/113 103 generous 56/111 102 giant-a giant 2/3 102 giant 1/1 101 modest-a modest 14/27 101 modest 10/19 105 slight-a slightest 2/3 105 slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band 14/27 100 band 21/41 117 bitter-p bitter 22/43 54 bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p head 90/179 100 head 81/161 105 nal nodes represents number bigram features selected decision tree learner.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	One original hypotheses accurate decision trees bigrams include relatively small number features.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	motivated success decision stumps performing disambiguation based single bigram feature.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	experiments, decision trees used bigram features identi?ed ?ltering step, many words decision tree learner went eliminate candidate features.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	seen comparing number internal nodes number candidate features shown columns 4 7.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	1 also noteworthy bigrams ultimately selected decision tree learner inclusion tree always include bigrams ranked highly power divergence statistic Dice CoeÆcient.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	expected, since selection bigrams raw text mea1 words 100 top ranked bigrams form set candidate features presented decision tree learner.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	ties top 100 rankings may 100 features, fewer 100 bi- grams occurred 5 times bigrams included feature set.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	suring association two words, decision tree seeks bigrams partition instances ambiguous word distinct senses.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	particular, decision tree learner makes decisions bigram include nodes tree using gain ratio, measure based overall Mutual Information bigram particular word sense.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Finally, note smallest decision trees functionally equivalent benchmark methods.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	decision tree 1 leaf node internal nodes (1/1) acts majority classi?er.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	decision tree 2 leaf nodes 1 internal node (2/3) structure decision stump.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	One long-term objectives identify core set features useful disambiguating wide class words using supervised unsupervised methodologies.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	presented ensemble approach word sense disambiguation (Pedersen, 2000) multiple Naive Bayesian classi?ers, based co{ occurrence features varying sized windows context, shown perform well widely studied nouns interest line.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	accuracy approach good previously published results, learned models complex diÆcult interpret, e?ect acting accurate black boxes.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	experience variations learning algorithms far less signi?cant contributors disambiguation accuracy variations feature set.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	words, informative feature set result accurate disambiguation used wide range learning algorithms, learning algorithm perform well given uninformative misleading set features.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Therefore, focus developing discovering feature sets make distinctions among word senses.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	learning algorithms must produce accurate models, also shed new light relationships among features allow us continue re?ning understanding feature sets.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	believe decision trees meet criteria.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	wide range implementations available, known robust accurate across range domains.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	important, structure easy interpret may provide insights relationships exist among features general rules disambiguation.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Bigrams used features word sense disambiguation, particularly form collocations ambiguous word one component bigram (e.g., (Bruce Wiebe, 1994), (Ng Lee, 1996), (Yarowsky, 1995)).	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	bigrams identify collocations include word disambiguated, requirement case.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Decision trees used supervised learning approaches word sense disambiguation, fared well number comparative studies (e.g., (Mooney, 1996), (Pedersen Bruce, 1997)).	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	former used bag word feature sets latter used mixed feature set included part-of-speech neighboring words, three collocations, morphology ambiguous word.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	believe approach paper ?rst time decision trees based strictly bigram features employed.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	decision list closely related approach also applied word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks Stevenson, 1998), (Yarowsky, 2000)).	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Rather building traversing tree perform disambiguation, list employed.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	general case decision list may suffer less fragmentation learning decision trees; practical matter means decision list less likely over{trained.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	However, believe fragmentation also ects feature set used learning.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	consists approximately 100 binary features.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	results relatively small feature space likely su?er fragmentation larger spaces.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	number immediate extensions work.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	?rst ease requirement bi- grams made two consecutive words.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Rather, search bigrams component words may separated words text.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	second eliminate ?ltering step candidate bigrams selected power divergence statistic.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Instead, decision tree learner would consider possible bigrams.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Despite increasing danger fragmentation, interesting issue since bigrams judged informative decision tree learner always ranked highly ?ltering step.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	particular, determine ?ltering process ever eliminates bi- grams could signi?cant sources disambiguation information.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	longer term, hope adapt approach unsupervised learning, disambiguation performed without bene?t sense tagged text.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	optimistic viable, since bigram features easy identify raw text.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	results approach compared 1998 SENSEVAL word sense disambiguation exercise show bigram based decision tree approach accurate best SENSEVAL results 19 36 words.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	Bigram Statistics Package implemented Satanjeev Banerjee, supported Grant{in{Aid Research, Artistry Scholarship OÆce Vice President Research Dean Graduate School University Minnesota.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	would like thank SENSEVAL organizers making data results 1998 event freely available.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	comments three anonymous reviewers helpful preparing ?nal version paper.	0
addition, Pedersen (2001) questions whether one statistic preferred bigram acquisition task cites Cressie Read (1984), argue cases Pearson statistic reliable log-likelihood statistic.	preliminary version paper appears (Pedersen, 2001).	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Decision Tree Bigrams Accurate Predictor Word Sense	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	paper presents corpus-based approach word sense disambiguation decision tree assigns sense ambiguous word based bigrams occur nearby.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	approach evaluated using sense-tagged corpora 1998 SENSEVAL word sense disambiguation exercise.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	accurate average results reported 30 36 words, accurate best results 19 36 words.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Word sense disambiguation process selecting appropriate meaning word, based context occurs.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	purposes assumed set possible meanings, i.e., sense inventory, already determined.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	example, suppose bill following set possible meanings: piece currency, pending legislation, bird jaw.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	used context Senate bill consideration, human reader immediately understands bill used legislative sense.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	However, computer program attempting perform task faces diÆcult problem since bene?t innate common{sense linguistic knowledge.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Rather attempting provide computer programs real{world knowledge comparable humans, natural language processing turned corpus{based methods.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	approaches use techniques statistics machine learning induce models language usage large samples text.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	models trained perform particular tasks, usually via supervised learning.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	paper describes approach decision tree learned number sentences instance ambiguous word manually annotated sense{tag denotes appropriate sense context.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Prior learning, sense{tagged corpus must converted regular form suitable automatic processing.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	sense{tagged occurrence ambiguous word converted feature vector, feature represents property surrounding text considered relevant disambiguation process.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Given exibility complexity human language, potentially in?nite set features could utilized.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	However, corpus{based approaches features usually consist information readily iden- ti?ed text, without relying extensive external knowledge sources.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	typically include part{of{speech surrounding words, presence certain key words within window context, various syntactic properties sentence ambiguous word.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	approach paper relies upon feature set made bigrams, two word sequences occur text.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	context ambiguous word occurs represented number binary features indicate whether particular bigram occurred within approximately 50 words left right word disambiguated.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	take approach since surface lexical features like bigrams, collocations, co{occurrences often contribute great deal disambiguation accuracy.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	clear much disambiguation accuracy improved use features identi?ed complex pre{processing part{of{speech tagging, parsing, anaphora resolution.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	One objectives establish clear upper bounds accuracy disambiguation using feature sets impose substantial pre{ processing requirements.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	paper continues discussion methods identifying bigrams included feature set learning.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	decision tree learning algorithm described, benchmark learning algorithms included purposes comparison.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	experimental data discussed, empirical results presented.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	close analysis ?ndings discussion related work.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	developed approach word sense disambiguation represents text entirely terms occurrence bigrams, de?ne two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation Bigram Counts consecutive words occur text.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	distributional characteristics bigrams fairly consistent across corpora; majority occur one time.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Given sparse skewed nature data, statistical methods used select interesting bigrams must carefully chosen.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	explore two alternatives, power divergence family goodness ?t statistics Dice CoeÆcient, information theoretic measure related point- wise Mutual Information.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Figure 1 summarizes notation word bigram counts used paper way 2 ? 2 contingency table.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	value n 11 shows many times bigram big cat occurs corpus.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	value n 12 shows often bigrams occur big ?rst word cat second.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	counts n +1 n 1+ indicate often words big cat occur ?rst second words bigram corpus.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	total number bigrams corpus represented n ++ . 2.1 Power Divergence Family.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	(Cressie Read, 1984) introduce power divergence family goodness ?t statistics.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	number well known statistics belong family, including likelihood ratio statisticG 2 Pearson'sX 2 statistic.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	measure divergence observed (n ij ) expected (m ij ) bigram counts, ij estimated based assumption component words bigram occur together strictly chance.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	(Dunning, 1993) argues favor G2 X2, especially dealing sparse skewed data distributions.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	However, (Cressie Read, 1984) suggest cases Pearson's statistic reliable likelihood ratio one test always preferred other.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	light this, (Pedersen, 1996) presents Fisher's exact test alternative since rely distributional assumptions underly Pearson's test likelihood ratio.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Unfortunately usually clear test appropriate particular sample data.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	take following approach, based observation tests assign approximately measure statistical signi?cance bi- gram counts contingency table violate distributional assumptions underly goodness ?t statistics.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	perform tests using X 2 , G 2 , Fisher's exact test bigram.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	resulting measures statistical signi?cance di?er, distribution bigram counts causing least one tests become unreliable.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	occurs rely upon value Fisher's exact test since makes fewer assumptions underlying distribution data.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	experiments paper, identi?ed top 100 ranked bigrams occur 5 times training corpus associated word.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	cases rankings produced G 2 , X 2 , Fisher's exact test disagreed, altogether surprising given low frequency bigrams excluded.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Since statistics produced rankings, hereafter make distinction among simply refer generically power divergence statistic.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	2.2 Dice CoeÆcient.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Dice CoeÆcient descriptive statistic provides measure association among two words corpus.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	similar pointwise Mutual Information, widely used measure ?rst introduced identifying lexical relationships (Church Hanks, 1990).	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Pointwise Mutual Information de?ned follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ w 1 w 2 represent two words make bigram.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Pointwise Mutual Information quanti?es often two words occur together bigram (the numerator) relative often occur overall corpus (the denominator).	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	However, curious limitation pointwise Mutual Information.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	bigram w 1 w 2 occurs n 11 times corpus, whose component words w 1 w 2 occur part bigram, result increasingly strong measures association value n 11 decreases.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Thus, maximum pointwise Mutual Information given corpus assigned bi- grams occur one time, whose component words never occur outside bigram.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	usually bigrams prove useful disambiguation, yet dominate ranked list determined pointwise Mutual Information.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Dice CoeÆcient overcomes limitation, de?ned follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ n 11 = n 1+ = n +1 value Dice(w 1 ; w 2 ) 1 values n 11 . value n. 11 less either marginal totals (the typical case) rankings produced Dice Co- eÆcient similar Mutual Information.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	relationship pointwise Mutual Information Dice CoeÆcient also discussed (Smadja et al., 1996).	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	developed Bigram Statistics Package produce ranked lists bigrams using range tests.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	software written Perl freely available www.d.umn.edu/~tpederse.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Decision trees among widely used machine learning algorithms.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	perform general speci?c search feature space, adding informative features tree structure search proceeds.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	objective select minimal set features eÆciently partitions feature space classes observations assemble tree.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	case, observations manually sense{tagged examples ambiguous word context partitions correspond di?erent possible senses.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	feature selected search process represented node learned decision tree.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	node represents choice point number di?erent possible values feature.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Learning continues training examples accounted decision tree.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	general, tree overly speci?c training data generalize well new examples.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Therefore learning followed pruning step nodes eliminated reorganized produce tree generalize new circumstances.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Test instances disambiguated ?nding path learned decision tree root leaf node corresponds observed features.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	instance ambiguous word dis- ambiguated passing series tests, test asks particular bigram occurs available window context.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	also include three benchmark learning algorithms study: majority classi?er, decision stump, Naive Bayesian classi?er.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	majority classi?er assigns common sense training data every instance test data.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	decision stump one node decision tree(Holte, 1993) created stopping decision tree learner single informative feature added tree.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Naive Bayesian classi?er (Duda Hart, 1973) based certain blanket assumptions interactions among features corpus.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	search feature space performed build representative model case decision trees.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Instead, features included classi- ?er assumed relevant task hand.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	assumption feature conditionally independent features, given sense ambiguous word.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	often used bag words feature set, every word training sample represented binary feature indicates whether occurs window context surrounding ambiguous word.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	use Weka (Witten Frank, 2000) implementations C4.5 decision tree learner (known J48), decision stump, Naive Bayesian classi?er.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Weka written Java freely available www.cs.waikato.ac.nz/~ml.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	empirical study utilizes training test data 1998 SENSEVAL evaluation word sense disambiguation systems.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Ten teams participated supervised learning portion event.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Additional details exercise, including data results referred paper, found SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) (Kilgarri?	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Palmer, 2000).	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	included 36 tasks SENSEVAL training test data provided.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	task requires occurrences particular word test data disambiguated based model learned sense{tagged instances training data.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	words used multiple tasks di?erent parts speech.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	example, two tasks associated bet, one use noun verb.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Thus, 36 tasks involving disambiguation 29 di?erent words.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	words part speech associated task shown Table 1 column 1.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Note parts speech encoded n noun, adjective, v verb, p words part speech provided.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	number test training instances task shown columns 2 4.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	instance consists sentence ambiguous word occurs well one two surrounding sentences.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	general total context available ambiguous word less 100 surrounding words.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	number distinct senses test data task shown column 3.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	following process repeated task.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Capitalization punctuation removed training test data.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Two feature sets selected training data based top 100 ranked bigrams according power divergence statistic Dice CoeÆcient.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	bigram must occurred 5 times included feature.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	step ?lters large number possible bi- grams allows decision tree learner focus small number candidate bigrams likely helpful disambiguation process.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	training test data converted feature vectors feature represents occurrence one bigrams belong feature set.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	representation training data actual input learning algorithms.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Decision tree decision stump learning performed twice, using feature set determined power divergence statistic using feature set identi?ed Dice CoeÆcient.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	majority classi?er simply determines frequent sense training data assigns instances test data.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Naive Bayesian classi?er based feature set every word occurs 5 times training data included feature.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	learned models used disambiguate test data.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	test data kept separate stage.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	employ ?ne grained scoring method, word counted correctly disambiguated assigned sense tag exactly matches true sense tag.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	partial credit assigned near misses.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	accuracy attained learning algorithms shown Table 1.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Column 5 reports accuracy majority classi?er, columns 6 7 show best average accuracy reported 10 participating SENSEVAL teams.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	evaluation SENSEVAL based precision recall, converted scores accuracy taking product.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	However, best precision recall may come di?erent teams, best accuracy shown column 6 may actually higher single participating SENSEVAL system.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	average accuracy column 7 product average precision recall reported participating SENSEVAL teams.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Column 8 shows accuracy decision tree using J48 learning algorithm features identi?ed power divergence statistic.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Column 10 shows accuracy decision tree Dice CoeÆcient selects features.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Columns 9 11 show accuracy decision stump based power divergence statistic Dice CoeÆcient respectively.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Finally, column 13 shows accuracy Naive Bayesian classi?er based bag words feature set.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	accurate method decision tree based feature set determined power divergence statistic.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	last line Table 1 shows win-tie-loss score decision tree/power divergence method relative every method.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	win shows accurate method column, loss means less accurate, tie means equally accurate.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	decision tree/power divergence method accurate best reported SENSEVAL results 19 36 tasks, accurate 30 36 tasks compared average reported accuracy.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	decision stumps also fared well, proving accurate best SENSEVAL results 14 36 tasks.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	general feature sets selected power divergence statistic result accurate decision trees selected Dice CoeÆcient.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	power divergence tests prove reliable since account possible events surrounding two words w 1 w 2 ; occur bigram w 1 w 2 , w 1 w 2 occurs bigram without other, bigram consists neither.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Dice CoeÆcient based strictly event w 1 w 2 occur together bigram.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	6 tasks decision tree / power divergence approach less accurate SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, sanction-p.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	dramatic difference occurred amaze-v, SENSE- VAL average 92.4% decision tree accuracy 58.6%.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	However, unusual task every instance test data belonged single sense minority sense training data.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	characteristics decision trees decision stumps learned word shown Table 2.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Column 1 shows word part speech.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Columns 2, 3, 4 based feature set selected power divergence statistic columns 5, 6, 7 based Dice CoeÆ- cient.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Columns 2 5 show node selected serve decision stump.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Columns 3 6 show number leaf nodes learned decision tree relative number total nodes.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Columns 4 7 show number bigram features selected Table 1: Experimental Results.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	table shows little di?erence decision stump nodes selected feature sets determined power divergence statistics versus Dice CoeÆcient.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	expected since top ranked bigrams measure consistent, decision stump node generally chosen among those.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	However, di?erences feature sets selected power divergence statistics Dice CoeÆcient.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	ected different sized trees learned based feature sets.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	number leaf nodes total number nodes learned tree shown columns 3 6.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	number internal nodes simply di?erence total nodes leaf nodes.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	leaf node represents end path decision tree makes sense distinction.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Since bigram feature appear decision tree, number inter- Table 2: Decision Tree Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n accident 8/15 101 accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n excess 13/25 104 excess 11/21 102 oat-n oat 7/13 13 oat 7/13 13 giant-n giants 16/31 103 giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n 1/1 7 1/1 7 promise-n promise 95/189 100 promising 49/97 107 sack-n sack 5/9 31 sack 5/9 31 scrap-n scrap 7/13 8 scrap 7/13 8 shirt-n shirt 38/75 101 shirt 55/109 101 amaze-v amazed 11/21 102 amazed 11/21 102 bet-v bet 4/7 10 bet 4/7 10 bother-v bothered 19/37 101 bothered 20/39 106 bury-v buried 28/55 103 buried 32/63 103 calculate-v calculated 5/9 103 calculated 5/9 103 consume-v 4/7 20 4/7 20 derive-v derived 10/19 104 derived 10/19 104 oat-v oated 24/47 80 oated 24/47 80 invade-v invade 55/109 107 invade 66/127 108 promise-v promise 3/5 100 promise 5/9 106 sack-v return 1/1 91 return 1/1 91 scrap-v 1/1 7 1/1 7 seize-v seize 26/51 104 seize 57/113 104 brilliant-a brilliant 26/51 101 brilliant 42/83 103 oating-a 7/13 10 7/13 10 generous-a generous 57/113 103 generous 56/111 102 giant-a giant 2/3 102 giant 1/1 101 modest-a modest 14/27 101 modest 10/19 105 slight-a slightest 2/3 105 slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band 14/27 100 band 21/41 117 bitter-p bitter 22/43 54 bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p head 90/179 100 head 81/161 105 nal nodes represents number bigram features selected decision tree learner.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	One original hypotheses accurate decision trees bigrams include relatively small number features.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	motivated success decision stumps performing disambiguation based single bigram feature.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	experiments, decision trees used bigram features identi?ed ?ltering step, many words decision tree learner went eliminate candidate features.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	seen comparing number internal nodes number candidate features shown columns 4 7.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	1 also noteworthy bigrams ultimately selected decision tree learner inclusion tree always include bigrams ranked highly power divergence statistic Dice CoeÆcient.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	expected, since selection bigrams raw text mea1 words 100 top ranked bigrams form set candidate features presented decision tree learner.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	ties top 100 rankings may 100 features, fewer 100 bi- grams occurred 5 times bigrams included feature set.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	suring association two words, decision tree seeks bigrams partition instances ambiguous word distinct senses.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	particular, decision tree learner makes decisions bigram include nodes tree using gain ratio, measure based overall Mutual Information bigram particular word sense.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Finally, note smallest decision trees functionally equivalent benchmark methods.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	decision tree 1 leaf node internal nodes (1/1) acts majority classi?er.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	decision tree 2 leaf nodes 1 internal node (2/3) structure decision stump.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	One long-term objectives identify core set features useful disambiguating wide class words using supervised unsupervised methodologies.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	presented ensemble approach word sense disambiguation (Pedersen, 2000) multiple Naive Bayesian classi?ers, based co{ occurrence features varying sized windows context, shown perform well widely studied nouns interest line.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	accuracy approach good previously published results, learned models complex diÆcult interpret, e?ect acting accurate black boxes.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	experience variations learning algorithms far less signi?cant contributors disambiguation accuracy variations feature set.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	words, informative feature set result accurate disambiguation used wide range learning algorithms, learning algorithm perform well given uninformative misleading set features.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Therefore, focus developing discovering feature sets make distinctions among word senses.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	learning algorithms must produce accurate models, also shed new light relationships among features allow us continue re?ning understanding feature sets.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	believe decision trees meet criteria.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	wide range implementations available, known robust accurate across range domains.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	important, structure easy interpret may provide insights relationships exist among features general rules disambiguation.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Bigrams used features word sense disambiguation, particularly form collocations ambiguous word one component bigram (e.g., (Bruce Wiebe, 1994), (Ng Lee, 1996), (Yarowsky, 1995)).	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	bigrams identify collocations include word disambiguated, requirement case.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Decision trees used supervised learning approaches word sense disambiguation, fared well number comparative studies (e.g., (Mooney, 1996), (Pedersen Bruce, 1997)).	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	former used bag word feature sets latter used mixed feature set included part-of-speech neighboring words, three collocations, morphology ambiguous word.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	believe approach paper ?rst time decision trees based strictly bigram features employed.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	decision list closely related approach also applied word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks Stevenson, 1998), (Yarowsky, 2000)).	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Rather building traversing tree perform disambiguation, list employed.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	general case decision list may suffer less fragmentation learning decision trees; practical matter means decision list less likely over{trained.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	However, believe fragmentation also ects feature set used learning.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	consists approximately 100 binary features.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	results relatively small feature space likely su?er fragmentation larger spaces.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	number immediate extensions work.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	?rst ease requirement bi- grams made two consecutive words.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Rather, search bigrams component words may separated words text.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	second eliminate ?ltering step candidate bigrams selected power divergence statistic.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Instead, decision tree learner would consider possible bigrams.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Despite increasing danger fragmentation, interesting issue since bigrams judged informative decision tree learner always ranked highly ?ltering step.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	particular, determine ?ltering process ever eliminates bi- grams could signi?cant sources disambiguation information.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	longer term, hope adapt approach unsupervised learning, disambiguation performed without bene?t sense tagged text.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	optimistic viable, since bigram features easy identify raw text.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	paper shows combination simple feature set made bigrams standard decision tree learning algorithm results accurate word sense disambiguation.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	results approach compared 1998 SENSEVAL word sense disambiguation exercise show bigram based decision tree approach accurate best SENSEVAL results 19 36 words.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	Bigram Statistics Package implemented Satanjeev Banerjee, supported Grant{in{Aid Research, Artistry Scholarship OÆce Vice President Research Dean Graduate School University Minnesota.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	would like thank SENSEVAL organizers making data results 1998 event freely available.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	comments three anonymous reviewers helpful preparing ?nal version paper.	0
• Salient bigrams: Salient bigrams within abstract high log-likelihood scores, described Pedersen (2001).	preliminary version paper appears (Pedersen, 2001).	0
