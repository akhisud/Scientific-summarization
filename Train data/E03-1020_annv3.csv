Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Discovering Corpus-Specific Word Senses	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	paper presents unsupervised algorithm automatically discovers word senses text.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	algorithm based graph model representing words relationships them.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Discrimination previously extracted sense clusters enables us discover new senses.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	use data recognising resolving ambiguity.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Automatic word sense discovery applications many kinds.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	paper organised follows.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	section 2, present graph model discover word senses.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Section 5 describes experiment presents sample results.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Finally, section 6 sketches applications algorithm discusses future work.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Instead link word top n neighbors n determined user (cf.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	section 4)..	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	41=0 441=P .4161.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	are, course, many types polysemy (cf.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	e.g.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	(Kilgarriff, 1992)).	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	"happens wing ""part building"" wing ""political group"" linked via policy."	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	1
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Flow within dense regions graph concentrated expansion inflation.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	case homonymy, small inflation parameter r would appropriate.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Usually, one sense ambiguous word w much frequent senses present corpus.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	local graph handed MCL process small, might miss w's meanings corpus.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	hand, local graph big, get lot noise.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Below, outline algorithm circumvents problem choosing right parameters.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	recompute local graph Gw discriminating c's features.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	process stopped similarity w best neighbour reduced set features fixed threshold.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	algorithm consists following steps: 1.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	2. Recursively remove nodes degree one.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	remove node corresponding w G. 3.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Apply MCL Gw fairly big inflation parameter r fixed.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	4.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Go back 1 reduced/devalued set features F. 6.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	7.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Output list class-labels best represent different senses w corpus.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	family algorithms described (Widdows, 2003).	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	section, describe initial evaluation experiment present results.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	soon carry report thorough analysis algorithm.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	used simple graph model based co-occurrences nouns lists (cf.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	section 2) experiment.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	determined WordNet synsets adequately characterized sense clusters.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	extract results listed table 1.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	mention direct results work.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	often contain many rare senses, ones relevant specific domains corpora.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	prepare evaluation algorithm applied collocation relationships (cf.	0
Current approaches used clustering (Dorow Widdows, 2003; Klapaftis Manandhar, 2008) statistical graph models (Klapaftis Manandhar, 2010) identify sense-specific subgraphs.	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Discovering Corpus-Specific Word Senses	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	paper presents unsupervised algorithm automatically discovers word senses text.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	algorithm based graph model representing words relationships them.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Discrimination previously extracted sense clusters enables us discover new senses.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	use data recognising resolving ambiguity.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Automatic word sense discovery applications many kinds.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	paper organised follows.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	section 2, present graph model discover word senses.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Section 5 describes experiment presents sample results.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Finally, section 6 sketches applications algorithm discusses future work.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Instead link word top n neighbors n determined user (cf.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	section 4)..	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	41=0 441=P .4161.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	are, course, many types polysemy (cf.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	e.g.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	(Kilgarriff, 1992)).	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	"happens wing ""part building"" wing ""political group"" linked via policy."	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Flow within dense regions graph concentrated expansion inflation.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	case homonymy, small inflation parameter r would appropriate.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Usually, one sense ambiguous word w much frequent senses present corpus.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	local graph handed MCL process small, might miss w's meanings corpus.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	hand, local graph big, get lot noise.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Below, outline algorithm circumvents problem choosing right parameters.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	recompute local graph Gw discriminating c's features.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	process stopped similarity w best neighbour reduced set features fixed threshold.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	algorithm consists following steps: 1.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	2. Recursively remove nodes degree one.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	remove node corresponding w G. 3.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Apply MCL Gw fairly big inflation parameter r fixed.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	4.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Go back 1 reduced/devalued set features F. 6.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	7.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Output list class-labels best represent different senses w corpus.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	family algorithms described (Widdows, 2003).	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	section, describe initial evaluation experiment present results.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	soon carry report thorough analysis algorithm.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	used simple graph model based co-occurrences nouns lists (cf.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	section 2) experiment.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	determined WordNet synsets adequately characterized sense clusters.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	extract results listed table 1.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	mention direct results work.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	often contain many rare senses, ones relevant specific domains corpora.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	prepare evaluation algorithm applied collocation relationships (cf.	0
"Dorow Widdows (2003) use BNC build cooccurrencegraph nouns, based co-occurrence frequency threshold.</S><S sid =""19"" ssid = ""3"">They perform Markov clustering graph."	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Discovering Corpus-Specific Word Senses	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	paper presents unsupervised algorithm automatically discovers word senses text.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	algorithm based graph model representing words relationships them.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Discrimination previously extracted sense clusters enables us discover new senses.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	use data recognising resolving ambiguity.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Automatic word sense discovery applications many kinds.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	paper organised follows.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	section 2, present graph model discover word senses.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Section 5 describes experiment presents sample results.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Finally, section 6 sketches applications algorithm discusses future work.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Instead link word top n neighbors n determined user (cf.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	section 4)..	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	41=0 441=P .4161.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	are, course, many types polysemy (cf.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	e.g.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	(Kilgarriff, 1992)).	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	"happens wing ""part building"" wing ""political group"" linked via policy."	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Flow within dense regions graph concentrated expansion inflation.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	case homonymy, small inflation parameter r would appropriate.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Usually, one sense ambiguous word w much frequent senses present corpus.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	local graph handed MCL process small, might miss w's meanings corpus.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	hand, local graph big, get lot noise.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Below, outline algorithm circumvents problem choosing right parameters.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	recompute local graph Gw discriminating c's features.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	process stopped similarity w best neighbour reduced set features fixed threshold.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	algorithm consists following steps: 1.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	2. Recursively remove nodes degree one.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	remove node corresponding w G. 3.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Apply MCL Gw fairly big inflation parameter r fixed.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	4.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Go back 1 reduced/devalued set features F. 6.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	7.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Output list class-labels best represent different senses w corpus.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	family algorithms described (Widdows, 2003).	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	section, describe initial evaluation experiment present results.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	soon carry report thorough analysis algorithm.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	used simple graph model based co-occurrences nouns lists (cf.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	section 2) experiment.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	determined WordNet synsets adequately characterized sense clusters.	1
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	extract results listed table 1.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	mention direct results work.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	often contain many rare senses, ones relevant specific domains corpora.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	prepare evaluation algorithm applied collocation relationships (cf.	0
highly accurateTopicSignaturesfor monosemous wordsin WordNet(forinstance,usingInfoMap(DorowandWiddows,2003)).	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Discovering Corpus-Specific Word Senses	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	paper presents unsupervised algorithm automatically discovers word senses text.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	algorithm based graph model representing words relationships them.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Discrimination previously extracted sense clusters enables us discover new senses.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	use data recognising resolving ambiguity.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	1
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Automatic word sense discovery applications many kinds.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	paper organised follows.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	section 2, present graph model discover word senses.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Section 5 describes experiment presents sample results.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Finally, section 6 sketches applications algorithm discusses future work.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Instead link word top n neighbors n determined user (cf.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	section 4)..	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	41=0 441=P .4161.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	are, course, many types polysemy (cf.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	e.g.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	(Kilgarriff, 1992)).	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	"happens wing ""part building"" wing ""political group"" linked via policy."	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Flow within dense regions graph concentrated expansion inflation.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	case homonymy, small inflation parameter r would appropriate.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Usually, one sense ambiguous word w much frequent senses present corpus.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	local graph handed MCL process small, might miss w's meanings corpus.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	hand, local graph big, get lot noise.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Below, outline algorithm circumvents problem choosing right parameters.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	recompute local graph Gw discriminating c's features.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	process stopped similarity w best neighbour reduced set features fixed threshold.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	algorithm consists following steps: 1.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	2. Recursively remove nodes degree one.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	remove node corresponding w G. 3.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Apply MCL Gw fairly big inflation parameter r fixed.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	4.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Go back 1 reduced/devalued set features F. 6.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	7.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Output list class-labels best represent different senses w corpus.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	family algorithms described (Widdows, 2003).	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	section, describe initial evaluation experiment present results.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	soon carry report thorough analysis algorithm.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	used simple graph model based co-occurrences nouns lists (cf.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	section 2) experiment.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	determined WordNet synsets adequately characterized sense clusters.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	extract results listed table 1.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	mention direct results work.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	often contain many rare senses, ones relevant specific domains corpora.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	prepare evaluation algorithm applied collocation relationships (cf.	0
related efforts word sense discrimination (Dorow Widdows, 2003; Fukumoto Suzuki, 1999; Pedersen Bruce, 1997).	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Discovering Corpus-Specific Word Senses	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	paper presents unsupervised algorithm automatically discovers word senses text.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	algorithm based graph model representing words relationships them.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Discrimination previously extracted sense clusters enables us discover new senses.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	use data recognising resolving ambiguity.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Automatic word sense discovery applications many kinds.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	paper organised follows.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	section 2, present graph model discover word senses.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Section 5 describes experiment presents sample results.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Finally, section 6 sketches applications algorithm discusses future work.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	1
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Instead link word top n neighbors n determined user (cf.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	section 4)..	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	41=0 441=P .4161.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	are, course, many types polysemy (cf.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	e.g.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	(Kilgarriff, 1992)).	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	"happens wing ""part building"" wing ""political group"" linked via policy."	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Flow within dense regions graph concentrated expansion inflation.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	case homonymy, small inflation parameter r would appropriate.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Usually, one sense ambiguous word w much frequent senses present corpus.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	local graph handed MCL process small, might miss w's meanings corpus.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	hand, local graph big, get lot noise.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Below, outline algorithm circumvents problem choosing right parameters.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	recompute local graph Gw discriminating c's features.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	process stopped similarity w best neighbour reduced set features fixed threshold.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	algorithm consists following steps: 1.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	2. Recursively remove nodes degree one.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	remove node corresponding w G. 3.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Apply MCL Gw fairly big inflation parameter r fixed.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	4.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Go back 1 reduced/devalued set features F. 6.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	7.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Output list class-labels best represent different senses w corpus.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	family algorithms described (Widdows, 2003).	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	section, describe initial evaluation experiment present results.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	soon carry report thorough analysis algorithm.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	used simple graph model based co-occurrences nouns lists (cf.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	section 2) experiment.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	determined WordNet synsets adequately characterized sense clusters.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	extract results listed table 1.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	mention direct results work.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	often contain many rare senses, ones relevant specific domains corpora.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	prepare evaluation algorithm applied collocation relationships (cf.	0
"algorithm (Dorow Widdows, 2003) represented target noun word, neighbors relationships using graph node denoted noun two nodes edge co-occurred given number times.</S> <S sid =""200"" ssid = ""10"">Then senses target word iteratively learned clustering local graph similar words around target word.</S><S sid =""201"" ssid = ""11"">Their algorithm required threshold input, controlled number senses."	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Discovering Corpus-Specific Word Senses	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	paper presents unsupervised algorithm automatically discovers word senses text.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	algorithm based graph model representing words relationships them.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Discrimination previously extracted sense clusters enables us discover new senses.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	use data recognising resolving ambiguity.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Automatic word sense discovery applications many kinds.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	paper organised follows.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	section 2, present graph model discover word senses.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Section 5 describes experiment presents sample results.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Finally, section 6 sketches applications algorithm discusses future work.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	1
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Instead link word top n neighbors n determined user (cf.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	section 4)..	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	41=0 441=P .4161.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	are, course, many types polysemy (cf.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	e.g.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	(Kilgarriff, 1992)).	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	"happens wing ""part building"" wing ""political group"" linked via policy."	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Flow within dense regions graph concentrated expansion inflation.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	case homonymy, small inflation parameter r would appropriate.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Usually, one sense ambiguous word w much frequent senses present corpus.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	local graph handed MCL process small, might miss w's meanings corpus.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	hand, local graph big, get lot noise.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Below, outline algorithm circumvents problem choosing right parameters.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	recompute local graph Gw discriminating c's features.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	process stopped similarity w best neighbour reduced set features fixed threshold.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	algorithm consists following steps: 1.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	2. Recursively remove nodes degree one.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	remove node corresponding w G. 3.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Apply MCL Gw fairly big inflation parameter r fixed.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	4.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Go back 1 reduced/devalued set features F. 6.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	7.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Output list class-labels best represent different senses w corpus.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	family algorithms described (Widdows, 2003).	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	section, describe initial evaluation experiment present results.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	soon carry report thorough analysis algorithm.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	used simple graph model based co-occurrences nouns lists (cf.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	section 2) experiment.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	determined WordNet synsets adequately characterized sense clusters.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	extract results listed table 1.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	mention direct results work.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	often contain many rare senses, ones relevant specific domains corpora.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	prepare evaluation algorithm applied collocation relationships (cf.	0
"Another graph-based method presented in(Dorow Widdows, 2003).</S><S sid =""36"" ssid = ""17"">They extract onlynoun neighbours appear conjunctions dis-junctions target word.</S><S sid =""37"" ssid = ""18"">Additionally, theyextract second-order co-occurrences."	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Discovering Corpus-Specific Word Senses	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	paper presents unsupervised algorithm automatically discovers word senses text.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	algorithm based graph model representing words relationships them.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Discrimination previously extracted sense clusters enables us discover new senses.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	use data recognising resolving ambiguity.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Automatic word sense discovery applications many kinds.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	paper organised follows.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	section 2, present graph model discover word senses.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Section 5 describes experiment presents sample results.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Finally, section 6 sketches applications algorithm discusses future work.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Instead link word top n neighbors n determined user (cf.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	section 4)..	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	41=0 441=P .4161.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	are, course, many types polysemy (cf.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	e.g.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	(Kilgarriff, 1992)).	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	"happens wing ""part building"" wing ""political group"" linked via policy."	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	1
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Flow within dense regions graph concentrated expansion inflation.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	case homonymy, small inflation parameter r would appropriate.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Usually, one sense ambiguous word w much frequent senses present corpus.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	local graph handed MCL process small, might miss w's meanings corpus.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	hand, local graph big, get lot noise.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Below, outline algorithm circumvents problem choosing right parameters.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	recompute local graph Gw discriminating c's features.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	process stopped similarity w best neighbour reduced set features fixed threshold.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	algorithm consists following steps: 1.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	2. Recursively remove nodes degree one.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	remove node corresponding w G. 3.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Apply MCL Gw fairly big inflation parameter r fixed.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	4.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Go back 1 reduced/devalued set features F. 6.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	7.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Output list class-labels best represent different senses w corpus.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	family algorithms described (Widdows, 2003).	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	section, describe initial evaluation experiment present results.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	soon carry report thorough analysis algorithm.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	used simple graph model based co-occurrences nouns lists (cf.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	section 2) experiment.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	determined WordNet synsets adequately characterized sense clusters.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	extract results listed table 1.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	mention direct results work.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	often contain many rare senses, ones relevant specific domains corpora.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	prepare evaluation algorithm applied collocation relationships (cf.	0
last trend, explored (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), starts cooccurrents word recorded corpus builds senses gathering cooccurrents according similarity dissimilarity.	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Discovering Corpus-Specific Word Senses	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	paper presents unsupervised algorithm automatically discovers word senses text.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	algorithm based graph model representing words relationships them.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Discrimination previously extracted sense clusters enables us discover new senses.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	use data recognising resolving ambiguity.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Automatic word sense discovery applications many kinds.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	paper organised follows.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	section 2, present graph model discover word senses.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Section 5 describes experiment presents sample results.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Finally, section 6 sketches applications algorithm discusses future work.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Instead link word top n neighbors n determined user (cf.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	section 4)..	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	41=0 441=P .4161.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	are, course, many types polysemy (cf.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	e.g.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	(Kilgarriff, 1992)).	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	"happens wing ""part building"" wing ""political group"" linked via policy."	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Flow within dense regions graph concentrated expansion inflation.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	case homonymy, small inflation parameter r would appropriate.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Usually, one sense ambiguous word w much frequent senses present corpus.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	local graph handed MCL process small, might miss w's meanings corpus.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	hand, local graph big, get lot noise.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Below, outline algorithm circumvents problem choosing right parameters.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	recompute local graph Gw discriminating c's features.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	process stopped similarity w best neighbour reduced set features fixed threshold.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	algorithm consists following steps: 1.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	2. Recursively remove nodes degree one.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	remove node corresponding w G. 3.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Apply MCL Gw fairly big inflation parameter r fixed.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	4.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Go back 1 reduced/devalued set features F. 6.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	7.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Output list class-labels best represent different senses w corpus.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	family algorithms described (Widdows, 2003).	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	section, describe initial evaluation experiment present results.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	soon carry report thorough analysis algorithm.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	used simple graph model based co-occurrences nouns lists (cf.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	section 2) experiment.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	determined WordNet synsets adequately characterized sense clusters.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	extract results listed table 1.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	mention direct results work.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	often contain many rare senses, ones relevant specific domains corpora.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	prepare evaluation algorithm applied collocation relationships (cf.	0
method, ones presented (Véronis, 2003), (Dorow Widdows, 2003) (Rapp, 2003), relies following hypothesis: subgraph gathering cooccurrents word, number relations cooccurrents defining sense higher number relations cooccurrents defining senses considered word.	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Discovering Corpus-Specific Word Senses	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	paper presents unsupervised algorithm automatically discovers word senses text.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	algorithm based graph model representing words relationships them.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Discrimination previously extracted sense clusters enables us discover new senses.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	use data recognising resolving ambiguity.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Automatic word sense discovery applications many kinds.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	paper organised follows.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	section 2, present graph model discover word senses.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Section 5 describes experiment presents sample results.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Finally, section 6 sketches applications algorithm discusses future work.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Instead link word top n neighbors n determined user (cf.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	section 4)..	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	41=0 441=P .4161.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	are, course, many types polysemy (cf.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	e.g.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	(Kilgarriff, 1992)).	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	"happens wing ""part building"" wing ""political group"" linked via policy."	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Flow within dense regions graph concentrated expansion inflation.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	case homonymy, small inflation parameter r would appropriate.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Usually, one sense ambiguous word w much frequent senses present corpus.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	local graph handed MCL process small, might miss w's meanings corpus.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	hand, local graph big, get lot noise.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Below, outline algorithm circumvents problem choosing right parameters.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	recompute local graph Gw discriminating c's features.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	process stopped similarity w best neighbour reduced set features fixed threshold.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	algorithm consists following steps: 1.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	2. Recursively remove nodes degree one.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	remove node corresponding w G. 3.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Apply MCL Gw fairly big inflation parameter r fixed.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	4.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Go back 1 reduced/devalued set features F. 6.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	7.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Output list class-labels best represent different senses w corpus.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	family algorithms described (Widdows, 2003).	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	section, describe initial evaluation experiment present results.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	soon carry report thorough analysis algorithm.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	used simple graph model based co-occurrences nouns lists (cf.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	section 2) experiment.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	determined WordNet synsets adequately characterized sense clusters.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	extract results listed table 1.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	mention direct results work.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	1
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	often contain many rare senses, ones relevant specific domains corpora.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	prepare evaluation algorithm applied collocation relationships (cf.	0
rely detection high-density areas network cooccurrences, (Véronis, 2003) (Dorow Widdows, 2003) closest methods ours.	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Discovering Corpus-Specific Word Senses	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	paper presents unsupervised algorithm automatically discovers word senses text.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	algorithm based graph model representing words relationships them.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Discrimination previously extracted sense clusters enables us discover new senses.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	use data recognising resolving ambiguity.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Automatic word sense discovery applications many kinds.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	paper organised follows.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	section 2, present graph model discover word senses.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Section 5 describes experiment presents sample results.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Finally, section 6 sketches applications algorithm discusses future work.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	1
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Instead link word top n neighbors n determined user (cf.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	section 4)..	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	41=0 441=P .4161.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	are, course, many types polysemy (cf.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	e.g.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	(Kilgarriff, 1992)).	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	"happens wing ""part building"" wing ""political group"" linked via policy."	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Flow within dense regions graph concentrated expansion inflation.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	case homonymy, small inflation parameter r would appropriate.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Usually, one sense ambiguous word w much frequent senses present corpus.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	local graph handed MCL process small, might miss w's meanings corpus.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	hand, local graph big, get lot noise.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Below, outline algorithm circumvents problem choosing right parameters.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	recompute local graph Gw discriminating c's features.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	process stopped similarity w best neighbour reduced set features fixed threshold.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	algorithm consists following steps: 1.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	2. Recursively remove nodes degree one.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	remove node corresponding w G. 3.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Apply MCL Gw fairly big inflation parameter r fixed.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	4.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Go back 1 reduced/devalued set features F. 6.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	7.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Output list class-labels best represent different senses w corpus.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	family algorithms described (Widdows, 2003).	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	section, describe initial evaluation experiment present results.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	soon carry report thorough analysis algorithm.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	used simple graph model based co-occurrences nouns lists (cf.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	section 2) experiment.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	determined WordNet synsets adequately characterized sense clusters.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	extract results listed table 1.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	mention direct results work.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	often contain many rare senses, ones relevant specific domains corpora.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	prepare evaluation algorithm applied collocation relationships (cf.	0
case, chose general approach working level simi­larity graph: similarity two words given relation cooccurrence, situa­tion comparable one (Véronis, 2003) (Dorow Widdows, 2003)	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Discovering Corpus-Specific Word Senses	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	paper presents unsupervised algorithm automatically discovers word senses text.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	algorithm based graph model representing words relationships them.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Discrimination previously extracted sense clusters enables us discover new senses.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	use data recognising resolving ambiguity.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Automatic word sense discovery applications many kinds.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	paper organised follows.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	section 2, present graph model discover word senses.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Section 5 describes experiment presents sample results.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Finally, section 6 sketches applications algorithm discusses future work.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Instead link word top n neighbors n determined user (cf.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	section 4)..	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	41=0 441=P .4161.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	are, course, many types polysemy (cf.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	e.g.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	(Kilgarriff, 1992)).	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	"happens wing ""part building"" wing ""political group"" linked via policy."	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Flow within dense regions graph concentrated expansion inflation.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	case homonymy, small inflation parameter r would appropriate.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Usually, one sense ambiguous word w much frequent senses present corpus.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	local graph handed MCL process small, might miss w's meanings corpus.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	hand, local graph big, get lot noise.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Below, outline algorithm circumvents problem choosing right parameters.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	recompute local graph Gw discriminating c's features.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	process stopped similarity w best neighbour reduced set features fixed threshold.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	algorithm consists following steps: 1.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	2. Recursively remove nodes degree one.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	remove node corresponding w G. 3.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Apply MCL Gw fairly big inflation parameter r fixed.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	4.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Go back 1 reduced/devalued set features F. 6.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	7.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Output list class-labels best represent different senses w corpus.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	family algorithms described (Widdows, 2003).	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	section, describe initial evaluation experiment present results.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	soon carry report thorough analysis algorithm.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	used simple graph model based co-occurrences nouns lists (cf.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	section 2) experiment.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	determined WordNet synsets adequately characterized sense clusters.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	extract results listed table 1.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	mention direct results work.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	often contain many rare senses, ones relevant specific domains corpora.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	prepare evaluation algorithm applied collocation relationships (cf.	0
global viewpoint, two differences lead (Véronis, 2003) (Dorow Widdows, 2003) build finer senses ours.	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Discovering Corpus-Specific Word Senses	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	paper presents unsupervised algorithm automatically discovers word senses text.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	algorithm based graph model representing words relationships them.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Discrimination previously extracted sense clusters enables us discover new senses.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	use data recognising resolving ambiguity.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Automatic word sense discovery applications many kinds.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	paper organised follows.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	section 2, present graph model discover word senses.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Section 5 describes experiment presents sample results.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Finally, section 6 sketches applications algorithm discusses future work.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Instead link word top n neighbors n determined user (cf.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	section 4)..	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	41=0 441=P .4161.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	are, course, many types polysemy (cf.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	e.g.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	(Kilgarriff, 1992)).	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	"happens wing ""part building"" wing ""political group"" linked via policy."	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	1
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Flow within dense regions graph concentrated expansion inflation.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	case homonymy, small inflation parameter r would appropriate.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Usually, one sense ambiguous word w much frequent senses present corpus.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	local graph handed MCL process small, might miss w's meanings corpus.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	hand, local graph big, get lot noise.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Below, outline algorithm circumvents problem choosing right parameters.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	recompute local graph Gw discriminating c's features.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	process stopped similarity w best neighbour reduced set features fixed threshold.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	algorithm consists following steps: 1.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	2. Recursively remove nodes degree one.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	remove node corresponding w G. 3.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Apply MCL Gw fairly big inflation parameter r fixed.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	4.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Go back 1 reduced/devalued set features F. 6.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	7.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Output list class-labels best represent different senses w corpus.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	family algorithms described (Widdows, 2003).	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	section, describe initial evaluation experiment present results.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	soon carry report thorough analysis algorithm.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	used simple graph model based co-occurrences nouns lists (cf.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	section 2) experiment.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	determined WordNet synsets adequately characterized sense clusters.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	extract results listed table 1.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	mention direct results work.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	often contain many rare senses, ones relevant specific domains corpora.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	prepare evaluation algorithm applied collocation relationships (cf.	0
methodology Dorow Widdows (2003) adopted: focus word, obtain graph neighborhood (all vertices connected via edges focus word vertex edges these).	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Discovering Corpus-Specific Word Senses	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	paper presents unsupervised algorithm automatically discovers word senses text.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	algorithm based graph model representing words relationships them.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Discrimination previously extracted sense clusters enables us discover new senses.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	use data recognising resolving ambiguity.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Automatic word sense discovery applications many kinds.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	paper organised follows.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	section 2, present graph model discover word senses.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Section 5 describes experiment presents sample results.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Finally, section 6 sketches applications algorithm discusses future work.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Instead link word top n neighbors n determined user (cf.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	section 4)..	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	41=0 441=P .4161.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	are, course, many types polysemy (cf.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	e.g.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	(Kilgarriff, 1992)).	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	"happens wing ""part building"" wing ""political group"" linked via policy."	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Flow within dense regions graph concentrated expansion inflation.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	case homonymy, small inflation parameter r would appropriate.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Usually, one sense ambiguous word w much frequent senses present corpus.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	local graph handed MCL process small, might miss w's meanings corpus.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	hand, local graph big, get lot noise.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Below, outline algorithm circumvents problem choosing right parameters.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	recompute local graph Gw discriminating c's features.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	process stopped similarity w best neighbour reduced set features fixed threshold.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	algorithm consists following steps: 1.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	2. Recursively remove nodes degree one.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	remove node corresponding w G. 3.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Apply MCL Gw fairly big inflation parameter r fixed.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	4.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Go back 1 reduced/devalued set features F. 6.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	7.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Output list class-labels best represent different senses w corpus.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	family algorithms described (Widdows, 2003).	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	section, describe initial evaluation experiment present results.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	soon carry report thorough analysis algorithm.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	used simple graph model based co-occurrences nouns lists (cf.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	section 2) experiment.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	determined WordNet synsets adequately characterized sense clusters.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	extract results listed table 1.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	mention direct results work.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	often contain many rare senses, ones relevant specific domains corpora.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	prepare evaluation algorithm applied collocation relationships (cf.	0
unsupervised discovery process produces sense inventory number senses corpus-driven senses may reflect additional usages present predefined sense inventory, medicine law (Dorow Widdows, 2003).	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Discovering Corpus-Specific Word Senses	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	paper presents unsupervised algorithm automatically discovers word senses text.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	algorithm based graph model representing words relationships them.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Discrimination previously extracted sense clusters enables us discover new senses.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	use data recognising resolving ambiguity.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Automatic word sense discovery applications many kinds.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	paper organised follows.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	section 2, present graph model discover word senses.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Section 5 describes experiment presents sample results.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Finally, section 6 sketches applications algorithm discusses future work.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Instead link word top n neighbors n determined user (cf.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	section 4)..	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	41=0 441=P .4161.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	are, course, many types polysemy (cf.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	e.g.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	(Kilgarriff, 1992)).	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	"happens wing ""part building"" wing ""political group"" linked via policy."	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Flow within dense regions graph concentrated expansion inflation.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	case homonymy, small inflation parameter r would appropriate.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Usually, one sense ambiguous word w much frequent senses present corpus.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	local graph handed MCL process small, might miss w's meanings corpus.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	hand, local graph big, get lot noise.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Below, outline algorithm circumvents problem choosing right parameters.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	recompute local graph Gw discriminating c's features.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	process stopped similarity w best neighbour reduced set features fixed threshold.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	algorithm consists following steps: 1.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	2. Recursively remove nodes degree one.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	remove node corresponding w G. 3.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Apply MCL Gw fairly big inflation parameter r fixed.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	4.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Go back 1 reduced/devalued set features F. 6.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	7.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Output list class-labels best represent different senses w corpus.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	family algorithms described (Widdows, 2003).	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	section, describe initial evaluation experiment present results.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	soon carry report thorough analysis algorithm.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	used simple graph model based co-occurrences nouns lists (cf.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	section 2) experiment.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	determined WordNet synsets adequately characterized sense clusters.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	extract results listed table 1.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	mention direct results work.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	1
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	often contain many rare senses, ones relevant specific domains corpora.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	prepare evaluation algorithm applied collocation relationships (cf.	0
follow Pantel Lin (2002) Dorow Widdows (2003) using sentence contexts words dependency path length 3 less, last word relation feature.	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Discovering Corpus-Specific Word Senses	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	paper presents unsupervised algorithm automatically discovers word senses text.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	algorithm based graph model representing words relationships them.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Sense clusters iteratively computed clustering local graph similar words around ambiguous word.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Discrimination previously extracted sense clusters enables us discover new senses.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	use data recognising resolving ambiguity.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	paper describes algorithm automatically discovers word senses free text maps appropriate entries existing dictionaries taxonomies.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Automatic word sense discovery applications many kinds.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	greatly facilitate lexicographer's work used automatically construct corpus-based taxonomies tune existing ones.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	corpus evidence supports clustering ambiguous word distinct senses used decide sense referred given context (Schiitze, 1998).	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	paper organised follows.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	section 2, present graph model discover word senses.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Section 3 describes way divide graphs surrounding ambiguous words different areas corresponding different senses, using Markov clustering (van Dongen, 2000).	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	quality Markov clustering depends strongly several parameters granularity factor size local graph.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	section 4, outline word sense discovery algorithm bypasses problem parameter tuning.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	conducted pilot experiment examine performance algorithm set words varying degree ambiguity.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Section 5 describes experiment presents sample results.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Finally, section 6 sketches applications algorithm discusses future work.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	model discover distinct word senses built automatically British National corpus, tagged parts speech.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	"Based intuition nouns co-occur list often semantically related, extract contexts form Noun, Noun,... and/or Noun, e.g. ""genomic DNA rat, mouse dog""."	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Following method (Widdows Dorow, 2002), build graph node represents noun two nodes edge co-occur lists given number times 1.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Following Lin's work (1998), currently investigating graph verb-object, verb-subject modifier-noun-collocations possible infer senses systematically polysemous words.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	word sense clustering algorithm outlined applied kind similarity measure based set features.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	1 Si mple cutoff functions proved unsatisfactory bias give frequent words.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Instead link word top n neighbors n determined user (cf.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	section 4)..	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	41=0 441=P .4161.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	sz44, CD miltrA, litrepate inovio. h,) Cik Figure 1: Local graph word mouse	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Ambiguous words link otherwise unrelated areas meaning E.g. rat printer different meaning, closely related different meanings mouse.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	However, remove mouse-node local graph illustrated figure 1, graph decomposes two parts, one representing electronic device meaning mouse one representing animal sense.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	are, course, many types polysemy (cf.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	e.g.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	(Kilgarriff, 1992)).	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	"seen figure 2, wing ""part bird"" closely related tail, wing ""part plane""."	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Therefore, even removal wing-node, two areas meaning still linked via tail.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	"happens wing ""part building"" wing ""political group"" linked via policy."	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	However, whereas many edges within area meaning, small number (weak) links different areas meaning.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	detect different areas meaning local graphs, use cluster algorithm graphs (Markov clustering, MCL) developed van Dongen (2000).	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	idea underlying MCL-algorithm random walks within graph tend stay cluster rather jump clusters.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	following notation description MCL algorithm borrows heavily van Dongen (2000).	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Let G, denote local graph around ambiguous word w. adjacency matrix MG 4111) 11 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph word wing graph G, defined setting (111G) pq equal weight edge nodes v v q . Normalizing columns A/G results Markov Matrix Taw whose entries (Thi,)pq interpreted transition probability v q vv . easily shown k-th power TG lists probabilities (TL )pq path length k starting node vq ending node V. MCL-algorithm simulates flow Gw iteratively recomputing set transition probabilities via two steps, expansion inflation.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	expansion step corresponds taking k-th power TG outlined allows nodes see new neighbours.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	inflation step takes matrix entry r-th power rescales column entries sum 1.Vi inflation, popular neighbours supported expense less popular ones.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Flow within dense regions graph concentrated expansion inflation.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Eventually, flow dense regions disappear, matrix transition probabilities TG converge limiting matrix interpreted clustering graph.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	output MCL-algorithm strongly depends inflation expansion parameters r k well size local graph serves input MCL.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	appropriate choice inflation param 80 eter r depend ambiguous word w clustered.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	case homonymy, small inflation parameter r would appropriate.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	However, ambiguous words closely related senses metaphorical metonymic variations one another.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	case, different regions meaning strongly interlinked small power coefficient r would lump different meanings together.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Usually, one sense ambiguous word w much frequent senses present corpus.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	local graph handed MCL process small, might miss w's meanings corpus.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	hand, local graph big, get lot noise.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Below, outline algorithm circumvents problem choosing right parameters.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	contrast pure Markov clustering, don't try find complete clustering G senses once.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Instead, step iterative process, try find disctinctive cluster c G w (i.e. distinctive meaning w) only.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	recompute local graph Gw discriminating c's features.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	achieved, manner similar Pantel Lin's (2002) sense clustering approach, removing c's features set features used finding similar words.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	process stopped similarity w best neighbour reduced set features fixed threshold.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Let F set w's features, let L output algorithm, i.e. list sense clusters initially empty.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	algorithm consists following steps: 1.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Compute small local graph Gw around w using set features F. similarity w closest neighbour fixed threshold go 6.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	2. Recursively remove nodes degree one.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	remove node corresponding w G. 3.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Apply MCL Gw fairly big inflation parameter r fixed.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	4.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	"Take ""best"" cluster (the one strongly connected w Gw removal w), add final list clusters L remove/devalue features F. 5."	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Go back 1 reduced/devalued set features F. 6.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Go final list clusters L assign name cluster using broad-coverage taxonomy (see below).	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Merge semantically close clusters using taxonomy-based semantic distance measure (Budanitsky Hirst, 2001) assign class-label newly formed cluster.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	7.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Output list class-labels best represent different senses w corpus.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	"local graph step 1 consists w, ni neighbours w n9 neighbours neighbours w. Since iteration attempt find ""best"" cluster, suffices build relatively small graph 1."	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Step 2 removes noisy strings nodes pointing away G. removal w G w might already separate different areas meaning, least significantly loosen ties them.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	simple model based noun co-occurrences lists, step 5 corresponds rebuilding graph restriction nodes new graph co-occur (or least often) cluster members already extracted.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	class-labelling (step 6) accomplished using taxonomic structure WordNet, using robust algorithm developed specially purpose.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	hypemym subsumes many cluster members possible closely possible taxonomic tree chosen class-label.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	family algorithms described (Widdows, 2003).	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	section, describe initial evaluation experiment present results.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	soon carry report thorough analysis algorithm.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	used simple graph model based co-occurrences nouns lists (cf.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	section 2) experiment.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	gathered list nouns varying degree ambiguity, homonymy (e.g. arms) systematic polysemy (e.g. cherry).	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	algorithm applied word list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) order extract top two sense clusters only.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	determined WordNet synsets adequately characterized sense clusters.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	extract results listed table 1.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output word sense clustering.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	benefits automatic, data-driven word sense discovery natural language processing lexicography would great.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	mention direct results work.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	algorithm recognise ambiguity, also used resolve it, features shared members sense cluster provide strong indication reading ambiguous word appropriate given certain context.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	gives rise automatic, unsupervised word sense disambiguation algorithm trained data disambiguated.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	ability map senses taxonomy using class-labelling algorithm used ensure sense-distinctions discovered correspond recognised differences meaning.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	approach disambiguation combines benefits Yarowsky's (1995) Schtitze's (1998) approaches.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Preliminary observations show different neighbours Table 1 used indicate great accuracy senses used.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	Off-the-shelf lexical resources rarely adequate NLP tasks without adapted.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	often contain many rare senses, ones relevant specific domains corpora.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	problem addressed using word sense clustering attune existing resource accurately describe meanings used particular corpus.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	prepare evaluation algorithm applied collocation relationships (cf.	0
"Similar approach presented (Dorow Widdows, 2003) construct word graph.</S><S sid =""178"" ssid = ""52"">Dorow Widdows construct graph target word w taking sub-graph induced neighborhood w (without w) clustering MCL."	section 2), plan evaluate uses clustering algorithm unsupervised disambiguation thoroughly.	0
